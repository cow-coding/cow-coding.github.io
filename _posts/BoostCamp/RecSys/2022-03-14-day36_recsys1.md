---
layout: post
date: 2022-03-14 03:00:00 PM
title: "[BoostCamp AI Tech / RecSys] Day36 - RecSys with GNN"
categories: [NAVER BoostCamp AI Tech, 추천 시스템 이론]
tags: [Deep Learning, NAVER, BoostCamp, AI Tech, Recommender System, GNN]
math: true
---
# RecSys : Recommender System with Graph Neural Net

**Neural Graph Collaborative Filtering의 자세한 내용은 Paper Review 카테고리에 있습니다.**  

---

## Graph Neural Network

### Graph

![](/image/boostcamp/recsys/deep/undirectedgraph.png)

- 점(node)과 간선(edge)으로 구성된 자료구조
- 일반적으로 $G = (V, E)$로 정의

### Naive Graph Neural Net

![](/image/boostcamp/recsys/deep/naive1.png)

- Neural Network에 그래프가 도입된 이유는 크게 2가지
  1. 정보간의 관계, **상호작용**같은 추상적 개념 표현에 적합
     - 상호작용으로 table로 표현하면 너무 복잡하다는 단점이 있음
     - 추천 시스템에서 유저-아이템 관계를 표현하기에 적합함
  2. Non-Euclidean Space 표현이 가능
     - 이미지, 텍스트, tabular는 대부분이 격자 공간에 투영 가능한 euclidean space 데이터
     - SNS데이터, 상호작용, 분자구조는 격자 공간에 투영할 수 없는 non-euclidean space
- 그래프를 **인접행렬** 방식으로 표현하여 하나의 노드정보를 MLP에 학습하는 naive approach 방법으로 시작함
- **문제점**
  - 노드의 수가 많아지면 인접행렬 그래프 표현의 한계인 연산량 증가문제가 발생함
  - 노드의 수만큼 MLP 노드가 추가되므로 모델의 차원 수가 증가하고 결국 모델의 복잡도로 연결됨
  - 인접행렬 특성상 표현하는 순서에 따라 의미가 달라지는 경우가 발생

### Graph Convolutional Network

![](/image/boostcamp/recsys/deep/gcn1.png){: w="500"}

[Paper : [ICLR 2017] Semi-Supervised Classification with Graph Convolutional Networks](https://arxiv.org/abs/1609.02907)

- Naive 방식의 문제점을 극복하고자 convolution 연산을 적용한 Graph Convolution Network가 등장
- Local Connectivity : 모든 노드를 확인하는 것이 아닌 **특정 노드의 주변부**를 대상으로 convolution 연산을 진행
- Shared Weight : convolution 연산 효과인 주변 노드들과 weight를 공유
- multi-layer : convolution을 여러층 쌓으면 떨어져 있는 정보를 확보할 수 있음
  - 연산량을 효과적으로 줄이면서 간접적인 관계 파악이 가능

---

## Neural Graph Collaborative Filtering

- 대부분의 강의 내용은 [논문 리뷰 포스트](https://cow-coding.github.io/posts/ngcf/)에 있습니다. 참고 바랍니다.

---

## LightGCN

![](/image/boostcamp/recsys/deep/lgcn.png)

- GCN의 핵심부분을 가져오고 일부 수정하여 더 가벼운 형태를 제시

![](/image/boostcamp/recsys/deep/lgcn2.png){: w="500"}*LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation*

- 기존 NGCF에서 모든 embedding에 대해 weight parameter를 적용했으나 LightGCN에서는 **이웃 노드의 임베딩을 가중합 처리**
- layer가 깊을수록 멀리있는 정보이므로 강도가 약할 것이라는 아이디어를 사용함

$$
\mathbf{e}^{(k+1)}_u = \sum_{i\in\mathcal{N}_u}\frac{1}{\sqrt{\lvert \mathcal{N}_u \rvert}\sqrt{\lvert \mathcal{N}_i \rvert}}\mathbf{e}^{(k)}_i
$$

- 주변에 연결되어 있는 노드만 사용하므로 self-connection 항이 제거됨
- 실제 학습 파라미터는 0번째 임베딩 레이어에만 존재함
- one-hot encoding이 embedding으로 표현되는 부분에서만 weight가 존재함

$$
\mathbf{e}_u = \sum_{k=0}^{K}\alpha_k\mathbf{e}^{(k)}_u
$$

- 최종 예측에서도 k층 레이어의 임베딩에 가중치 $\alpha$를 곱해서 가중합 처리를 함
- 이때, $\alpha_k = (k+1)^{-1}$로 설정해서 레이어가 깊을수록 가중치가 0에 가까워져서 영향력을 낮추게함