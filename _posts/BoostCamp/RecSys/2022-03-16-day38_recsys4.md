---
layout: post
date: 2022-03-16 05:00:00 PM
title: "[BoostCamp AI Tech / RecSys] Day38 - Context-aware Recommendation(GBM)"
categories: [NAVER BoostCamp AI Tech, Level 2 - 추천 시스템 이론]
tags: [Deep Learning, NAVER, BoostCamp, AI Tech, Recommender System, GBM, Ensemble]
math: true
---
# RecSys : Context-aware Recommendation(GBM)

---

## Gradient Boosting Machine (GBM)

![](/image/boostcamp/recsys/deep/gbm.png)*Greedy Function Approximation: A Gradient Boosting Machine*

- [Paper: Greedy Function Approximation: A Gradient Boosting Machine](https://jerryfriedman.su.domains/ftp/trebst.pdf)

### 문제 제기

- CTR 예측으로 개인화 추천 시스템을 위한 모델
- 특별한 문제를 제기했다기보단 **기존의 앙상블**을 진행했다는 것이 의미가 있음
- 실시간 서비스는 feature가 자주 변화하므로 하이퍼파라미터에 robust한 모델이 필요

### Boosting

- 앙상블 기법의 일종
- 핵심 아이디어는 의사결정 나무 (decision tree)로 된 **weak learner들을 연속적으로 학습**하고 **결합**하는 방식
  - weak learner는 정확도와 복잡도는 낮지만 그만큼 간단한 learner
  - **이전의 weak learner가 취약한 부분 위주로** 샘플링, 가중치 부여를 통해 부족한 부분 위주로 **다음 learner가 학습**
- AdaBoost, Gradient Boost Machine(GBM), XGBoost, LightGBM, CatBoost, ...

### GBM 학습

![](/image/boostcamp/recsys/deep/gbm2.png)*Greedy Function Approximation: A Gradient Boosting Machine*

- gradient descent로 loss function 학습
- 기존의 gradient descent와 다르게 파라미터가 아닌 **weak learner 자체를 gradient**항으로 연산

$$
\arg\min_{\mathbf{a}, \beta}\sum_{i=1}^{N}\left[ \hat{y}_i - \beta h(\mathbf{x}_i ; \mathbf{a}) \right]^2
$$

- 통계학적으로 보면 Gradient Boosting은 **잔차(residual)** 를 활용해서 학습을 진행함
- 이전 단계의 weak learner까지 residual을 활용해서 다음 learner를 학습
- 회귀 문제는 예측과정에서 residual을 사용, 분류에서는 $\log(odds)$를 사용
- 각 decision tree는 분기점의 기준이 다름
- loss 값이 threshold 이하로 내려가거나, leaf node에 속하는 데이터 수가 적어지면 학습 종료

### GBM 특징

- 장점
  - Bagging 원리를 쓰는 random forest보다 좋은 성능을 보임
- 단점
  - 학습 속도가 느림
  - overfitting 발생 가능