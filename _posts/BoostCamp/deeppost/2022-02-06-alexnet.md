---
layout: post
date: 2022-02-06
title: "[BoostCamp AI Tech] AlexNet 논문 리뷰"
categories: [NAVER BoostCamp AI Tech, 심화 포스팅]
tags: [NAVER, BoostCamp, AI Tech, Python, Basic, Jekyll, MathJax, LaTex]
math: true
---
# AlexNet 논문 리뷰

---

## 논문 소개

![](/image/boostcamp/alexnet/alexnet1.png){: w="500"}*출처 : NIPS2012 ImageNet Classification with Deep Convolutional Neural Networks (a.k.a AlexNet)*

<center>
<a href="https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf"><bold>[NIPS2012] ImageNet Classification with Deep Convolutional Neural Networks</bold></a>
</center><br>

AlexNet은 본격적으로 딥러닝이 실전적인 효과를 나타낸 대표적인 신경망 모델입니다. 논문에 따르면 AlexNet은 224 X 224의 이미지를 분류하는 대회인 *ImageNet LSVRC(ILSVRC)* 에 2010년과 2012년 총 2번 참가를 하였고 2012년 대회에서 우승을 차지하며 딥러닝의 시대를 열었습니다.  
기존에도 CNN이 존재하였지만 이를 직접적으로 활용하기에 어려움이 있었습니다. 하지만 GPU를 활용한 CNN구조의 구현과 Dropout을 적용시킨 AlexNet이 등장하며 이미지 분류에서 CNN의 입지가 굉장히 뛰어올랐습니다. 이번 글에서는 AlexNet의 논문을 리뷰하며 자세한 내용을 공부해보도록 하겠습니다.  
개인 공부의 목적도 포함하고 있어서 각 파트별로 나눠서 작성하였으므로 필요한 부분에 맞춰 우측의 목차를 클릭하셔서 이동해주시길 바랍니다.

**이 글에는 오타 및 오역이 존재할 수 있습니다. 참고하여 읽어주시고 번역이 부자연스러운 경우 원문 부분을 함께 작성하였습니다.**

## Abstract

- AlexNet은 120만개의 고해상도(224 X 224)이미지를 분류하는 ILSVRC 2012에서 error rate 15.3%를 기록하며 우승
- 총 6천만개의 parameter와 65만개의 뉴런으로 구성된 모델
- 5개의 convolution layer + 3 fully-connected layer로 구성
  - 5개의 convolution layer 중 일부는 max-pooling layer가 연결되어있음
  - 3개의 fully-connected layer는 최종 1000-way softmax로 구성되어있음
- 훈련 속도의 향상을 위해 **비포화 뉴런**과 **GPU구현의 convolution 연산**을 사용
- 과적합 방지를 위해 **Dropout**을 활용

## 1. Introduction

### Chapter 요약

- 기존의 이미지 분류 문제들의 데이터 셋은 수만개 정도로 작은 크기의 데이터 셋이었고 학습을 통해 인식을 진행하는 데에 큰 무리가 없었음
  - 기존에도 이미지를 변형하는 방식은 image augmentation이 존재해서 지정된 label을 지키는 선에서 데이터를 증폭하는 방식으로 학습을 했기 때문
- 현실의 데이터는 상당히 가변적인 환경(*considerable variability*)을 갖고 있기 때문에 이를 적용하기 위해서는 큰 규모의 데이터 셋이 필요
- 작은 데이터 셋의 문제점은 이미 알려졌지만 어쩔 수 없는 한계로 최근에서야 큰 규모의 데이터 셋을 만들 수 있었음
  - 대표적인 예시가 ILSVRC의 ImageNet 데이터 셋
- 큰 규모의 데이터를 학습하기 위한 모델은 규모가 커져야 하는데, object recognition 작업은 매우 큰 복잡도를 갖는 작업이고 따라서 ImageNet에 적용하기 어렵다는 문제가 발생함
  - 이를 해결하고자 모델은 갖고 있지 않은 데이터에 대해 많은 사전지식이 있어야 함  
  (원문: so our model should also have lots of prior knowledge to compensate for all the data we don’t have.)
- CNN의 가정에 따르면 이런 복잡도와 데이터의 사전지식 문제를 해결하기에 좋음
  - CNN은 넓이와 폭에 의해 capacity가 결정되고 이미지의 특성에 대해 올바른 추정을 함
- CNN은 parameter와 connection의 수가 fully-connected layer에 비해 훨씬 적기 때문에 빠른 훈력속도와 성능의 보존이 가능
- 하지만 기존의 CNN은 적용에 있어서 대규모의 비용이 든다는 단점이 존재
  - 2D convolution과 결합된 GPU로 인해 학습에 적용할 수 있게 되었음
- 이 논문에서는 **2D convolution에 최적화된 GPU 연산구현**, **성능 향상과 동시에 학습 시간을 줄이기 위한 새로운 feature와 자주 사용하지 않는 feature제시**, **과적합 방지를 위한 Dropout의 적용**에 대해 설명
- 최종 신경망의 구조는 5개의 convolution layer와 3개의 fully-connected layer이고 이 깊이는 성능자체제 중요한 영향을 미침

### Chapter 분석

1. CNN 가정에 나오는 이미지의 특성(nature of images)은 총 2가지가 언급됩니다. [내용 참고 : CNN과 이미지가 찰떡궁합인 이유](https://seoilgun.medium.com/cnn의-stationarity와-locality-610166700979) 
   1. Stationary of statistics
        - stationary란 확률론에서 시계열의 통계적인 속성이 시간의 영향을 받지 않는 것을 의미합니다. 대체적으로 일정한 패턴을 갖는 경우를 말합니다.
        - 이미지는 특정 패턴을 갖고 있습니다. convolution 연산의 핵심 아이디어는 증폭 또는 축소를 통한 특징 및 패턴 추출입니다. 이런 관점에서 바라보면 stationary는 convolution 연산과 매우 좋은 시너지를 발휘합니다.
   2. Locality of pixel dependencies
       - locality of pixel dependencies는 이미지는 작은 특징들의 구성이기 때문에 픽셀들은 주변의 일부 픽셀에 한해서만 영향을 받는다는 것입니다.
       - 간단히 말하면 이미지는 여러개의 픽셀로 구성되는 데 특정 지역(locality)에 있는 픽셀은 이미지 전체에 영향을 받는 것이 아닌 해당 부분의 근처 픽셀들의 영향을 받는 것입니다. 즉 코 근처의 픽셀은 눈이나 입 부근의 픽셀의 영향을 받는 것이 아닌 코를 구성하는 콧볼, 콧대같은 픽셀의 영향을 받는다는 것입니다.
       - 이러한 점은 stride에 따라 모든 이미지를 convolution하는게 아닌 부분적인 이미지에 convolution을 적용하는 CNN과 잘 맞는다는 것을 알 수 있습니다.


### 궁금증

1. Introduction 파트를 읽으면서 큰 규모의 데이터 학습에 있어서 매우 큰 복잡도를 갖는 모델이 필요하여 ImageNet에 적용하기 어렵다는 부분이 있었습니다.  
   (원문 : However, the immense complexity of the object recognition task means that this prob- lem cannot be specified even by a dataset as large as ImageNet, so our model should also have lots of prior knowledge to compensate for all the data we don’t have.)  
   CNN의 가장 큰 장점은 이미지 해상도가 높아져도 kernel의 크기는 고정적으로 유지되므로 weight parameter의 크기에 대한 부담이 적다라는 것이라서 이 부분을 **"기존의 Fully-connected만으로는 224 X 224의 고해상도 이미지는 각 픽셀 개수에 비례해서 layer의 weight가 필요해서 무리가 있다는 것"** 으로 이해했는데 이게 맞는 이해인지가 확실치 않습니다.

## 2. The Dataset