---
layout: post
date: 2022-04-11 11:00:00 PM
title: "[BoostCamp AI Tech / P Stage 2] Day56 - Project Day 16"
categories: [NAVER BoostCamp AI Tech, P Stage]
tags: [Deep Learning, NAVER, BoostCamp, AI Tech, Movie Recommendataion, Project]
math: true
---

# P Stage 2 : Project Day 16

---

## Day 16 list

1. Multi-VAE 논문 공부
2. RecVAE loss 변경 제시
3. VAE의 loss와 sampling 분포 고려

---

## Multi-VAE 논문 공부

- 대회 전반적으로 VAE 계열의 representation learning이 좋은 효과를 보인다는 것은 사실상 확정된 것으로 보임
- 기본적으로 VAE와 Multi-VAE에 대한 이해가 부족한 것이라 생각해서 Multi-VAE 논문인 'Variational Autoencoders for Collaborative Filtering`을 꼼꼼하게 읽어보는 중
- 자세한 리뷰는 곧 올라올 예정

### Multi-VAE 논문 리딩에서 깨달은 점

- 이 논문 읽으면서 얼마나 무지했는지를 깨달았음
- 가장 처음 깨달은 것은 이름의 **Multi**의 의미
  - 처음에는 Multi-head attention처럼 여러 개를 의미하는 multi인 줄 알았음
  - 논문을 읽어보니 sampling 분포를 Gaussian, Logistic, Multinomial에서 하는데 이 중 Multinomial 분포를 활용한게 Multi-VAE...
- $\log p_\theta(\mathbf{x}_u \mid \mathbf{z}_u)$ 가 VAE 계열 모델의 loss인 ELBO에서 활용되는데 이 부분을 어떻게 수식적으로 처리하는 지를 몰라서 되게 답답했었음
  - Multi-VAE 논문에서 smapling 분포에 맞춰서 log likelihood 식이 변경되는 것을 알았음
  - 이를 역으로 생각하면 **현재 대회에서 사용하는 Multi-VAE 모델의 smapling 분포나 loss의 형태를 변경해 볼 가치가 있다고 생각함**
- 아직 읽는 중이라 더 읽으면 뭔가 얻을 수 있을듯....

---

## RecVAE loss 변경 아이디어 제시

- 현재 우리 팀의 제일 좋은 성능을 내는 모델인 RecVAE에도 동일하게 ELBO를 활용한 loss function이 존재
- 충분히 loss를 변경해볼 필요가 있다고 생각해서 일반적으로 BCE (Binary Cross Entorpy)를 쓰는 것을 MSE나 Focal로 변경해보는 것을 제시
- MSE로 변경하니까 0.1540에서 0.1601까지 올라감 (약간의 Hyperparametr tuning 진행해서, 평균적으론 0.159 정도로 나옴)

---

## VAE의 loss와 sampling 분포 고려

- Sampling 분포나 loss를 변경해 볼 필요가 있을 것이라 생각됨
- 실제로 RecVAE나 Multi-VAE의 sampling 근간이 되는 함수인 `reparameterize`가 Gaussian 가정으로 적혀 있는 것으로 보임

```python
    def reparameterize(self, mu, logvar):
        if self.training:
            std = torch.exp(0.5 * logvar)
            eps = torch.randn_like(std)
            return eps.mul(std).add_(mu)
        else:
            return mu
```

- Gaussian 가정이라 MSE loss가 더 좋은 효과를 보이고 있는 것으로 보임
- 이에 맞춰 sampling distribution을 변경해보는 것이 좋아보임