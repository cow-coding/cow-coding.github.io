---
layout: post
date: 2022-01-24 03:00:00 PM
title: "[BoostCamp AI Tech] Day6 - Introduction to PyTorch and Basics"
categories: [NAVER BoostCamp AI Tech, PyTorch]
tags: [NAVER, BoostCamp, AI Tech, Python, Basic, PyTorch]
math: true
---
# PyTorch : Introduction to PyTorch ans Basics

---

## 딥 러닝 프레임워크

### 1. Tensorflow

![](/image/boostcamp/pytorch/tf_logo.png){: w="450"}

- 딥러닝 프레임워크의 선두주자'였'던 TensorFlow다.
- '였'던 이라는건 지금은 아마 PyTorch로 넘어간 걸로 알고있다.
- TF의 아버지는 구글이다보니 초기부터 굉장한 밀어주기가 있어서 지금도 많은 소스들이 존재한다.
- 실제로 2020년에도 점유율은 35.1%정도를 차지하고 있다.
- 이런 텐서플로우를 보좌해주는 친구가 존재하는데 그게 바로 Keras다.
    - 텐서플로우에 코드를 하나하나 다 적는거보다는 layer나 optimizer같은 것들을 쉽게 조작할 수 있게 도와주는 역할을 케라스가 한다.
- 소스자체도 많고 관련 오픈소스들이 많다보니 실제 서비스 구축에서 많이 사용된다.
- Define and Run
    - Tensorflow의 model graph 형성방법은 static graph이다.  
    즉 모델을 우선 구성하고 학습을 진행하는 것이다.
- 텐서플로우는 production, cloud, multip GPU에서 강점을 보인다.

### 2. PyTorch

![](/image/boostcamp/pytorch/pytorch.jpeg){: w="450}  

- 딥러닝 프레임워크계의 신흥강자인 파이토치이다.
- 파이토치의 아버지도 텐서플로우 못지 않게 빠방한데, 바로 페이스북이다.
- 텐서플로우와 다르게 내부 코드를 하나하나 동작하면서 학습을 돌릴 수 있기 때문에 연구목적으로 많이 사용된다.
- Define by Run (Dynamic Computational Graph, DCG)
    - PyTorch는 텐서플로우와 다르게 실행을 하면서 모델 그래프를 그린다.
    - 이런 방식은 디버깅의 편의성을 가져오기 때문에 연구 목적으로 쓰기 가장 훌륭하다.

## PyTorch

```python
import torch
```

### 1. Numpy

- PyTorch에는 Numpy의 `ndarray`와 유사한 역할을 하는 `Tensor`가 존재한다.
- `Tensor`를 생성하는 방식도 `Numpy`와 동일하다.
    ```python
    import torch
    import numpy as np

    # numpy array
    n_array = np.arange(10).reshape(2, 5)

    # tensor
    t_array = torch.FloatTensor(n_array)

    # array to tensor
    data = [[3, 5], [10, 5]]
    x_data = torch.tensor(data)

    # ndarray to tensor
    nd_array_ex = np.array(data)
    tensor_array = torch.from_numpy(nd_array_ex)
    ```
- `GPU tensor`라는 특수한 자료형이 존재한다.
    - 이는 GPU환경 즉, cuda에서 돌리기 위한 특수 자료형이다.
- 기본적인 numpy의 연산이 대부분 활용된다.
    ```python
    data = [[3, 5, 20], [10, 5, 50], [1, 5, 10]]
    x_data = torch.tensor(data)

    x_data.flatten()
    # tensor([ 3,  5, 20, 10,  5, 50,  1,  5, 10])

    torch.ones_like(x_data)
    # tensor([[1, 1, 1],
    #         [1, 1, 1],
    #         [1, 1, 1]])

    x_data.shape
    # torch.Size([3, 3])

    x_data.dtype
    # torch.int64
    ```
- pytorch는 GPU에 올려서 사용이 가능하다.
    ```python
    if torch.cuda.is_available():
        x_data_cuda = x_data.to('cuda')
    else:
        x_data_cuda = x_data.to('cpu')
    ```

- Tensor를 다루는 방법도 numpy와 유사한데 주의해야할 것이 있다.
    - `view`
        - tensor에는 `reshape`와 `view`가 모두 존재하는데 둘의 역할은 둘다 차원을 변환해주는 것은 같다.  
        하지만 약간의 차이가 존재하는데 `reshape`는 메모리의 구조와 간련이 있어 `reshape` 동작과 함께 copy가 발생하지만 `view`는 copy가 아닌 기존의 데이터를 그대로 사용한다.  
        
            ```python
            import torch

            a = torch.zeros(3, 2)
            b = a.view(2, 3)
            a.fill_(1)

            print(b)
            # tensor([[1., 1., 1.],
            #         [1., 1., 1.]])

            a = torch.zeros(3, 2)
            b = a.t().reshape(6)
            a.fill_(1)

            print(b) # tensor([0., 0., 0., 0., 0., 0.])
            ```  
    - `squeeze`, `unsqueezq`
        - `squeeze`는 tensor의 차원을 한단계 낮추는 함수이다.
        - `unsqueeze`는 반대로 tensor의 차원을 axis에 맞춰 높여주는 역할을 한다.
        
        ```python
        import torch

        tensor_ex = torch.rand(size=(2, 1, 2))
        tensor_ex.squeeze() 
        # tensor([[0.5461, 0.9128],
        #         [0.4809, 0.8499]])

        ```
    - `torch.mm`, `torch.matmul`
        - pytorch에도 행렬연산이 가능하다. 이때 행렬곱은 `torch.mm`을 사용하는 것을 권장한다.
        - `torch.dot`은 벡터의 내적 연산만을 진행하고 행렬 연산은 지원하지 않는다.
        - `torch.matmul`도 행렬곱으로 동작하지만 이는 broadcasting이 발생하기 때문에 계산결과의 이해가 어렵다.
        - `torch.mm`은 broadcasting을 진행하지 않으므로 충분히 이해가되는 계산 연산이 이뤄진다.

### 2. AutoGrad

- ML/DL formula
    - `nn.functional` 모듈을 활용하여 다양한 수식 변환을 지원한다.
    - 간단한 예로는 softmax와 one_hot encoding이 있다.
        ```python
        import torch
        import torch.nn.functional as F

        tensor = torch.FloatTensor([0.5, 0.7, 0.1])
        h_tensor = F.softmax(tensor, dim=0)

        y = torch.randint(5, (7, 5))
        y_label = y.argmax(dim=1)

        torch.nn.functional.one_hot(y_label)
        """
        tensor([[1, 0, 0, 0, 0],
                [1, 0, 0, 0, 0],
                [0, 0, 0, 0, 1],
                [0, 0, 1, 0, 0],
                [1, 0, 0, 0, 0],
                [0, 1, 0, 0, 0],
                [1, 0, 0, 0, 0]])
        """
        ```
- AutoGrad
    - PyTorch의 핵심은 자동민부을 지원한다. 이를 통해 backward 함수에 사용된다.
        ```python
        w = torch.tensor(2.0, requires_grad=True) # 미분 대상에는 require_grad=True 설정
        y = w**3
        z = 12 * y + 2
        z.backward()
        w.grad
        # tensor(144.)
        ```
    - AutoGrad는 편미분도 가능하다.