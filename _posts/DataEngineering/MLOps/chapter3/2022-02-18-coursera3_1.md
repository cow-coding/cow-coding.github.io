---
layout: post
date: 2022-02-18 07:40:00 AM
title: "[MLOps Specialization / Step3] Hyperparameter tuning"
categories: [Date Engineering, MLOps]
tags: [Data Engineering, MLOps, Coursera]
math: true
---

# Hyperparameter tuning: searching for the best architecture

**이 포스트는 Coursera의 MLOps Specialization 강의 내용을 기반으로 작성되었습니다.**

---

## Hyperparameter Tuning

### Neural Architecture Search (NAS)

- NAS는 neural network 디자인을 자동화해주는 기술
- **적절한 구조를 찾는 것**을 도와줌
- 넓은 공간을 탐색
- AutoML은 자동 탐색 알고리즘

### Types of parameters

- 학습 가능 파라미터
  - 학습 프로세스 동안 알고리즘에 의해 학습가능
  - 자동적으로 업데이트를 진행
  - ex) NN의 weight나 bias

- 하이퍼 파라미터
  - 모델이 **어떻게 학습하는가**에 영향을 미침
  - 학습 프로세스 이전에 세팅
  - 학습 과정동안 업데이트되지 않음
  - 자동적인 업데이트가 이뤄지지 않음
  - ex) learning rate, # of units

- 수동으로 하이퍼파라미터를 튜닝하는 것은 확장가능하지 않음
- 또한 수동 튜닝은 수수께끼 퍼즐같은 것
- Keras Tuner를 통해 하이퍼파라미터 튜닝의 자동화가 가능

---

## Keras Autotuner Demo

### Setting up libraries and dataset

```python
import tensorflow as tf
from tensorflow import keras

mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train/255.0, x_test/255.0
```

### Deep learning "Hello world!"

```python
model = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(10, activation='softmax)
])

model.compile(optimizer='adam',
             loss='sparse_categorical_crossentropy',
             metrics=['accuracy'])

model.fit(x_train, y_train, epochs=5)
model.evaluate(x_test, y_test)
```

### Parameters rational: if any

- model 코드에서 모델의 구조 중 `tf.keras.layers.Dense`와 `Dropout`에 들어가는 파라미터들은 **많은 시도를 통해 알아내거나** 단순히 **복사**해서 쓰는 경우임  
  $\rightarrow$ 부정확하거나 근거가 부족

### Automated search with Keras tuner

```bash
pip install -q -U keras-tuner
```

```python
import kerastuner as kt

def model_builder(hp):
    model = keras.Sequential()
    model.add(keras.layer.Flatten(input_shape=(28, 28))

    hp_units = hp.Int('units', min_value=16, max_value=512, step=16)
    model.add(keras.layers.Dense(units=hp_units, activation='relu'))
    model.add(tf.keras.layers.Dropout(0.2))
    model.add(keras.layers.Dense(10))

    model.compile(optimizre='adam', 
                 loss='sparse_categorical_crossentropy',
                 metrics=['accuracy'])

    return model
```

### Search strategy

```python
tuner = kt.Hyperband(model_builder,
                    objective='val_accuracy',
                    max_epochs=10,
                    factor=3,
                    directory='my_dir',
                    project_name='intro_to_kt')
```

- 여기서 `Hyperband`는 `RandomSearch`, `BayesianOptimization`, `Sklearn` 등 다양한 전략을 사용할 수 있음

### Callback configuration

```python
stop_early = tf.keras.callbacks.EarlyStopping(moniotr='val_loss',
                                              patience=5)

tuner.search(x_train, y_train,
             epochs=50, validation_split=0.2,
             callbacks=[stop_early])
```
