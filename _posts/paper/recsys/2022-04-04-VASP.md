---
layout: post
date: 2022-04-04 01:00:00 PM
title: "[Paper Review] Deep Variational Autoencoder with Shallow Parallel Path for Top-N Recommendation (VASP)"
categories: [Paper Review, Recommender System]
tags: [NAVER, BoostCamp, AI Tech, VAE, VASP, EASE, paper review, recommender system]
math: true
---

# Deep Variational Autoencoder with Shallow Parallel Path for Top-N Recommendation (VASP) (2021)

---

## 논문 소개

![](/image/paper/vasp/vasp1.png)*출처 : Deep Variational Autoencoder with Shallow Parallel Path for Top-N Recommendation (VASP)*

<center>
<a href="https://arxiv.org/abs/2102.05774"><bold>Deep Variational Autoencoder with Shallow Parallel Path for Top-N Recommendation (VASP)</bold></a>
</center><br>

VASP는 2021년 2월에 나온 따끈따끈한 논문입니다. 추천 시스템에서 Benchmark 상위를 차지하고 있는 모델이며 추천 시스템에서 크게 효과를 보인 EASE와 VAE를 활용한 모델입니다. 실제 논문 상에서 엄청 특별한 수식적 접근이나 뛰어난 연구적 접근이 있지는 않다고 생각하지만 2개의 모델을 앙상블해서 좋은 효과를 보였다고 생각됩니다.

---

## Abstract

- EASE 알고리즘은 Top-N 추천 task에서 좋은 성능을 보였고 VAE는 최근 추천 시스템에서 관심도가 높음
- 이 논문에서는 EASE를 현대의 신경망 기술로 학습하여 성능을 향상시킨 **Neural EASE**를 
- identity(여기서는 Input으로 보임)에 과적합되지 않으면서 다중 비선형 레이어의 이점을 보여주는 deep autoencoder인 **FLAVE**를 제시함
- FLAVE와 Neural EASE를 병렬적으로 학습하는 방식을 활용

---

## Introduction

- RS에서는 수백만의 유저와 아이템을 다루는 큰 규모의 데이터를 다루는 기술
- 이런 상황에서 content는 dynamic하게 변화하고 실시간으로 처리되는 데이터에 대해 충분히 **빠르게 학습**해야하고 **높은 재현률**을 보여줄 필요가 있음
- EASE는 단순한 선형모델임에도 identity에 과적합하는 문제를 잘 해결하고 explainable한 결과를 제공함
- 이를 활용해서 **복잡한 비선형 패턴을 처리하는 deep autoencoder**의 잠재력을 활용하며 **EASE처럼 설명가능한** 모델을 구축할 것
- 전통적인 dropout을 통한 과적합 방지는 매우 deep한 구조에서는 효과가 좋지 않음
- 3가지 주된 문제 해결방안
  - Top-N 추천 : focal loss를 활용한 long-tail 문제 방지
  - 과적합 방지 : 간단한 data augmentation 기술 활용
  - 앙상블 : 아다마르 곱(**element-wise**)을 활용한 여러모델 학습
- **VASP는 deep VAE와 Neural EASE를 결합하여 함께 학습함**
  - 이 과정에서 선형적 특징과 비선형적 특징을 모두 모델링함

---

## Related Work

- 다양한 관련된 연구들에 대해 장단점을 나열하고 있음
- 핵심적인 모델들 위주로 정리

### AutoRec

- 2016년에 나온 AutoRec은 autoencoder 기반의 CF 모델을 활용하여 좋은 성능을 보였음
- 이 모델에서는 **explicit rating**을 활용했음

### Multi-VAE

- 2018년에 나온 Variational AutoEncoder (VAE)를 활용한 CF 모델. 
- Multi-VAE라고도 함
- **Multinomial log-likelihood 데이터 분포**를 활용한 모델

### EASE

- *Embarassingly Shallow Autoencoder for Sparse Data*에서 공개한 모델
- deep architecture에 반대되는 hidden layer가 없는 Autoencoder 모델
- SOTA 모델을 달성한 모델로 유명함

### Wide & Deep

- 단순하고 shallow하거나 복잡한 architecture를 갖는 다양한 모델들이 나타남
- Wide & Deep이라 불리는 위의 2가지 특징을 조합한 모델이 등장
- 두 특징을 조합하는 과정에서 **joint training**이라 불리는 기술을 사용함
- Wide & Deep에서 사용한 아이디어를 활용해서 **item attribute가 아닌 interaction을 사용**해서 비선형 패턴을 찾는 방식을 설계함

### Other fields

- CV나 NLP에서 사용하는 딥러닝 기술이 추천 시스템에 적용되어 좋은 성능을 보임
- 대표적인 예는 Residual Network와 RecVAE가 있음
- 이런 점에서 **Focal Loss (FL)** 를 추천 시스템에 적용함
  - Focal Loss는 object detection에서 나타나는 불균형한 class에 적용하는 loss function
  - **Class imbalance는 추천 시스템의 niche item interaction**과 동일하게 생각할 수 있음
  - 이런 점에서 CF에서 나타는 cold start problem을 해결할 수 있을 것임

### Data augmentation

![](/image/paper/vasp/vasp2.png)*Split-Brain Autoencoders: Unsupervised Learning by Cross-Channel Prediction*

- 논문 참조 : [Split-Brain Autoencoders: Unsupervised Learning by Cross-Channel Prediction](https://arxiv.org/pdf/1611.09842.pdf)
- Autoencoder에서 핵심은 입출력의 과적합을 방지하는 것
- *Split-Brain Autoencoders*에 따르면 gray-scale representation을 학습하는 영역과 color-channel을 학습하는 영역으로 분할되어 훈련을 진행함
- 두 영역은 각각 교차로 예측을 수행하여 원본 사진으로 복구를 함
- 아래는 논문에서 제시한 예시 사진

![](/image/paper/vasp/vasp3.png)*Split-Brain Autoencoders: Unsupervised Learning by Cross-Channel Prediction*

- VASP에서는 전처리 단계로 자동화된 data augmentation을 사용하는 1개의 신경망을 활용할 것
- 전체 데이터를 절반으로 나눠서 각자 학습하는 형태로 훈련

---

## Model Architecture
