[ { "title": "[Review] Infcon 2022 참가 후기", "url": "/posts/Infcon2022/", "categories": "Life, Career", "tags": "인프런, 인프콘, inflearn, infcon", "date": "2022-08-26 17:30:00 +0900", "snippet": "Infcon 2022처음 참가한 컨퍼런스인프런의 첫번째 개발자 컨퍼런스인 인프콘 2022(INFCON 2022) 참가자에 선정되어 다녀왔습니다. 인프런은 개발자라면 많은 분들이 아실 것이라 생각합니다. 저도 데이터 분석이나 머신러닝, 캐글관련 강의들을 배우는 과정에 많이 사용하기도 했습니다. 많은 개발자들이 사용하는 국내 커뮤니티를 지향하는 인프런의 첫번째 개발자 컨퍼런스는 어떤 내용들이 있을지 기대도 많이 되었고 미리 공개된 세션구성에서부터 듣고 싶은 내용들이 많았습니다.인프콘 홈페이지에 들어가면 세션 구성에 대해 안내가 되어 있습니다. 기대한 세션은 인프랩의 CTO인 이동욱님의 발표인 인프런 아키텍처의 과거와 현재, 그리고 미래와 인프런의 아이돌 김영한님의 어느 날 고민 많은 주니어 개발자가 찾아왔다 - 성장과 취업, 이직 이야기였습니다.인프콘에 도착!인프콘은 코엑스 그랜드볼룸에서 진행됐습니다. 코엑스는 가~끔 가봤는데 그랜드볼룸은 어딘지 몰라서 한참을 헤매다가 정문이 아니라 옆문…?으로 들어갔습니다. 다행히 스태프분께서 정문을 안내해주셔서 제대로 스타트를 끊을 수 있었습니다.참가 목걸이와 함께 리플렛을 제공되었습니다. 추가로 사진은 없지만 볼펜, 마스크, 스티커, 인프콘 티셔츠를 주머니백에 넣어주셨습니다.이번 인프콘에서는 핸즈온 세션과 함께 5개의 호실에서 총 31개의 세션이 진행됐습니다. 13:45부터 20분 혹은 40분씩 발표가 진행됐습니다. 4개의 호실에서 동시에 진행됐기 때문에 듣고 싶은 세션들을 정할 필요가 있었습니다.세션들만 진행될 거라 생각했는데, 많은 기업들이 인프콘에 참여해서 굿즈와 채용풀로 인재영입을 진행하기도 했습니다. 부스가 준비된 기업은 총 8개였습니다. 교보문고를 제외한 8개의 기업이 부스가 마련되어 있었고 다양한 굿즈들을 제공해주었습니다.개인적으로 굿즈 퀄리티는 오늘의 집이랑 토스가 가장 좋았습니다.세션 선택과 이유제가 참가하기로 정한 세션은 다음과 같았습니다.3, 4 세션은 중간에 굿즈들도 받아야 했고 휴식을 좀 취할 필요도 있었어서 넘겼습니다. 데이터와 관련된 내용들을 가볍게 훑어보고 이후에는 서비스 개발에 필요한 아키텍처 구성, 개발자의 성장과 관련된 세션들을 많이 들었습니다.나름 세션을 선정한 이유는 강연하시는 분들의 명성도 있었고 관심분야도 있었습니다. 간단하게 선정 이유를 설명드리겠습니다.첫번째 세션첫 세션은 가볍게 가기로 결정했습니다. 동시간대 다른 강연들에 흥미가 가는 부분은 없기도 했고 같이 간 친구가 듣고 싶어했던 것도 있었습니다. 무엇보다 파이썬을 활용한 데이터 관련 내용들은 제가 잘 알고 있는 부분도 있었기 때문에 너무 복잡하게 생각하지 않아도 될 거라 생각했습니다.하지만 컨퍼런스에서 조금이라도 무언가를 얻는 게 필요했고 제가 모르는 부분의 라이브러리와 기술적 내용을 얻고자 참여했습니다.두번째 세션두번째 세션은 제가 앞서 말한 기대한 세션 중 하나였습니다. 유튜브에서 이동욱님의 영상을 많이 본 것도 있고 말을 굉장히 잘하신다고 생각해서 꼭 듣고 싶었습니다. 그리고 무엇보다 한 기업의 CTO이고 인프랩이라는 크지 않고 발전하는 기업의 아키텍처와 CTO의 생각을 보면서 아키텍처 구성과 기업 발전 과정에서의 고민들을 알고 싶었습니다.다섯번째 세션리플렛을 제공받기 전까지 김민준님이 누구였는지 기억이 잘 안났습니다. 근데 리플렛에 velopert라는 닉네임이 적힌걸 보고 선택한 부분도 있었습니다. 저 또한 블로그를 하고 있고 어떻게 하면 좀 더 유명하고 유능한 개발자가 될까?를 고민하는 사람이다보니 관련 내용을 어떻게 설계할 수 있을지를 얻고자 참여했습니다.그리고 무엇보다 지금 시대는 자기 PR의 시대라고 불릴만큼 스스로를 잘 어필할 요소들을 만드는게 중요한 시대입니다. 젊은 개발자들에게 인지도가 잘 알려져있는 벨로퍼트님의 발표에서 무언가 얻을 수 있을거라 생각했습니다.여섯번째 세션다른 세션들도 좋았지만 제가 백엔드 API를 개발할 때 보통 Node.js를 많이 사용하기 때문에 Node.js와 관련된 내용을 선택했습니다. Node.js를 확실히 스타트업단계에서는 많이 사용하는 경향이 있었습니다. 스프링에 비해 빠른 러닝커브를 갖고 있기 때문이 아닐까 생각합니다. Flitto의 개발자이신 강동한님께서도 이런 말을 해주셨습니다.일곱번째 세션" }, { "title": "[Deep Dive Python] 3. Tuple / Dictionary / Set", "url": "/posts/tuple_dictionary/", "categories": "Programming, Deep Dive Python", "tags": "Python, Programming, Tuple, Dictionary, Set", "date": "2022-07-13 21:30:00 +0900", "snippet": "Deep Dive Python : Tuple / Dictionary / Set튜플의 특징튜플은 리스트만큼 많이 사용되는 자료구조이다. 어떤 데이터를 묶음으로 반환하는 많은 함수나 메소드들은 리턴값을 튜플로 반환한다. 그렇기 때문에 튜플에 대한 이해는 리스트 못지 않게 중요하다.같이 보면 좋은 저장소는 CPython : Tuple Object이다.참고로 이번 글은 사진보다 글 위주로 될 것이다.튜플도 객체들의 모음앞서 리스트는 객체 참조자들을 저장하는 컨테이너라고 했다. 튜플은 요소를 순서대로 결합한 요소들의 쌍이다. 리스트와 마찬가지로 여러 자료형들을 함께 저장할 수 있다. 이런 점에서 왜 굳이 리스트와 구분되는 자료형이 있는걸까?면접에서도 자주 나오는 질문인데, 파이썬에서 리스트와 튜플의 차이점은 무엇일까?두 자료구조의 가장 큰 차이점은 값을 수정할 수 있는 가변객체냐, 불변객체냐이다. 근데 왜 굳이 이게 중요하냐라는 생각이 들 수 있는데, 데이터를 변화시키면 안되는 경우가 존재할 수 있기 때문이다. 파이썬에서 함수의 리턴값을 여러 개로 하면 팩킹의 형태로 변수가 전달된다. 이는 함수가 리턴하면서 혹시 모르게 값이 변화되는 것을 막아줄 수 있다. 그 외에는 리스트와 큰 차이는 없다. 객체의 참조들을 저장하는 특징도 동일하다.여기서 가변객체냐 불변객체냐는 파이썬의 메모리관리 측면에서 중요한 역할을 한다. 이는 결과적으로 속도와도 연관이 되는데, 다시 맨 처음 포스트로 돌아가보면 -5 ~ 256은 미리 만들어진 객체를 사용한다고 했다. 그래서 해당 범위의 수를 사용하는 경우에는 약간이나마 조금 더 시간이 빠르다.튜플의 특징을 정리하면 다음과 같다. 리스트보다 적은 메모리를 차지하고 성능면에서 더 빠르다. 리스트보다 적은 메모리를 차지하는 이유는 이전에 말했듯이 리스트는 확장에 대비하고자 좀 더 여유있게 메모리를 확보한다. 하지만 튜플은 내부 구조를 변화하지 않으므로 굳이 메모리를 여유있게 확보할 필요가 없다. 내부 값을 실수로 변경하는 것에 대비할 수 있다. 딕셔너리 키로 사용가능하다.튜플과 변수튜플과 리스트를 생성하는 방식에는 각자의 이름을 가진 함수를 호출하는 방식이 있다. 이 둘은 생김새는 동일하지만 동작과정에서 큰 차이가 있다. 이는 결국 가변 객체냐 불변 객체냐로 다시 연결된다.lst1 = [1, 2, 3, 4]lst2 = list(lst1)tp1 = (1, 2, 3, 4)tp2 = tuple(tp1)lst1과 lst2는 서로 다른 객체가 생성되어 각각의 변수가 참조한다고 했다. 그렇다면 튜플은 어떻게 될까? 아주 잠깐 생각해보면 쉽게 답을 유추할 수 있다. 힌트는 불변 객체이다.두 튜플의 아이덴티티는 동일하게 나온다. 왜일까? 이는 튜플의 생성 방식때문에 나타난다.튜플은 동일한 튜플 객체에 대해 같은 참조를 하게된다. 이는 튜플이 불변 객체라 어차피 변경을 할 수 없는 데이터라서다. 그래서 같은 실체를 함께 참조하더라도 큰 문제가 발생하지 않기 때문에 동일한 참조를 가리킨다.튜플의 누적 대입튜플은 불변 객체라서 누적 대입(+=)이 불가능할 것 같지만 사용할 수 있다. 하지만 리스트의 누적 대입의 메모리 활용과 다르게 동작한다. 이유는 역시나 불변 객체라서이다.tp1 = (1, 2, 3, 4)tp1 += (5, )누적 대입을 하게되면 새로운 튜플이 만들어지고 객체 참조가 바뀌게 된다. 이 점을 잘 알아둘 필요가 있다.간단하게 정리하면 튜플은 동일한 객체는 새로 생성하지 않고 참조를 할당하고 조금이라도 변화를 주면 새로운 객체를 만들어서 참조한다.딕셔너리의 특징딕셔너리도 튜플과 리스트 못지 않게 많이 사용되는 자료구조이다. Key-Value의 쌍으로 이루어진 자료구조이고 자료구조 수업시간에 배우는 딕셔너리 구조를 생각하면된다. 여기서 조금 센스있는 사람 또는 열심히 자료구조 공부를 한 사람이라면 스쳐지나가는 생각이 있다. 딕셔너리는 해시테이블 구조를 가질까?정답은 맞다이다. 딕셔너리는 해시 테이블을 통해 구현되었기 때문에 키 탐색이 빠르다는 장점이 있다.딕셔너리의 키딕셔너리는 Key-Value가 한 쌍으로 이루어진 자료구조이다. 대부분의 언어에서 Key-Value 쌍의 자료구조에서 Key가 중복되지 않는 고유한 값이어야한다. 이는 파이썬도 마찬가지이다.우선 딕셔너리는 가변 객체에 속한다. 하지만 딕셔너리의 key는 불변이다. 일반적으로 키에 문자열을 많이 쓰지만 불변 객체라면 모두 사용할 수 있다. 이 말은 튜플도 키로 쓸 수 있다.딕셔너리에서 값을 가져올때는 get을 써라이 글을 읽는 대부분의 사람들이 기본적인 딕셔너리 생성이나 사용법은 알 것이라 생각한다. 딕셔너리에서 value를 가져오는 방법은 크게 2가지가 존재한다. dict[key] dict.get(key)둘 중 어떤걸 쓰든 값이 ‘있다면’, 가져오는 데엔 큰 특징은 없다. 하지만 dict.get(key)로 값을 가져오는 것을 추천한다. 이유는 인덱스식으로 값을 가져올 경우 해당하는 key가 없다면 KeyError 예외를 발생시킨다. 하지만 dict.get(key)은 자신이 원한 값은 default로 설정해서 key가 없는 경우 default값을 반환한다. default를 따로 설정하지 않으면 None이 기본으로 설정되어 있다.딕셔너리 활용딕셔너리에서 update를 사용하면 딕셔러니를 합치거나 새로운 쌍을 추가할 수 있다. 활용은 아래처럼 사용한다.rgb = {&quot;red&quot;: &quot;빨강&quot;, &quot;blue&quot;: &quot;파랑&quot;, &quot;green&quot;: &quot;초록&quot;}fruit = {&quot;yellow&quot;: &quot;바나나&quot;, &quot;red&quot;: &quot;사과&quot;, &quot;purple&quot;: &quot;포도&quot;}rgb.update(fruit)근데 만약 위의 예제처럼 같은 키인데 다른 value를 가지면 어떻게 될까?update라는 이름에 걸맞게 새롭게 들어오는 key-value 기준에 맞춰 데이터가 바뀌게 된다.update를 활용하면 특정 키의 요소를 변경하는 것도 가능하다. 이때, dict.update(key=value)를 적으면 되는데 오타가 아니라 key는 문자열로 적지 않고 key 이름 자체를 적는다.set의 특징set은 이름 그대로 집합이다. 중학교에 들어가서 수학책을 피면 가장 맨 처음 만나는 친구가 집합이다. 아마 공부를 아무리 안 한 사람이라도 집합은 공부했을 것이다. 왜냐? 첫 챕터니까…집합의 가장 큰 특징은 순서와 중복이 없는 값들의 모임이다. 이런 수학적 성질은 파이썬의 set에서도 동일하게 적용된다.set의 선언set을 선언하는 방식은 여러가지가 있다.st1 = {1}st2 = set()st3 = set([1, 2, 3])여기서 주의해야할 것은 빈 set을 만들때는 반드시 set()을 사용해야한다. 이유는 set을 나타내는 연산자는 {}인데, 만약 빈 {}을 사용할 경우 빈 딕셔너리가 만들어진다.set에 대한 이해set은 가변 객체이다. 물론 불변 객체로 사용하는 방법도 있다. 이는 fronzeset 타입이 있고 필요에 따라 라이브러리를 활용해 쓸 수 있다. 물론 불변 객체니까 frozenset은 딕셔너리 키로 쓸 수 있다.set도 값을 추가하거나 삭제할 수 있다. 추가를 할 때는 set.add(e)를 사용하면 값이 추가된다. 물론 이미 있으면 추가되지 않는다.제거하는 과정에는 2개의 메소드가 있다. 하나는 discard, 다른 하나는 remove이다. 둘 중 어느 것이 좋냐?라는 질문을 한다면, 에러를 발생시키지 않는 쪽이 서비스하기에 유리할 것이다. 그렇다면 discard를 써야한다. remove는 값이 없다면 KeyError 예외를 발생시킨다.파이썬을 깊게 다뤄보는 3번째 시간이었다. 자료형을 그냥 쓰면 되는거지…라는 생각이 들 수도 있는데, 파이썬을 익숙하게 다룬다고 말하거나 다뤄야 한다면 변수가 어떻게 동작하는지, 메모리 구조를 어떻게 처리하는지는 알아야하지 않을까 싶다." }, { "title": "[Deep Dive Python] 2. List", "url": "/posts/list/", "categories": "Programming, Deep Dive Python", "tags": "Python, Programming, List", "date": "2022-07-11 23:30:00 +0900", "snippet": "Deep Dive Python : List리스트의 특징리스트는 파이썬을 만나게 된 이후 가장 많이 사용하는 자료구조라고 생각한다. 파이썬에서는 C++이나 Java와 같은 배열 자료구조를 제공하지 않고 그 역할을 리스트가 대신한다. 물론 리스트라는 자료구조는 단순히 배열에 대입해서 보기에는 상당히 복잡한 형태로 구성된 자료구조이다.같이 보면 좋은 저장소는 CPython : List Object이다.리스트는 객체 참조자들의 저장소리스트의 활용성이 높은 것은 다양한 자료형 객체의 저장 컨테이너라는 것과 유동적인 데이터 변경이라는 특징때문이다. 여기서 중요한 것은 객체의 저장 컨테이너라는 점이다. 리스트에 저장되는 모든 요소는 객체를 참조하는 역할을 하기 때문에 컨테이너라고 부른다.리스트와 변수lst1 = [1, 2, 3, 4, 5]lst2 = lst1앞서 다뤄본 변수에 대한 대입연산은 리스트도 동일하게 이뤄진다. 그렇기 때문에 리스트를 다른 리스트에 대입하는 연산을 수행하는 경우 동일 리스트를 복사하는 것이 아닌 서로 같은 리스트를 포인팅하는 것이 된다. 따라서 다른 값을 변경해도 마치 C++의 포인터를 통한 변경처럼 다른 변수에도 영향을 준다.lst1 = [1, 2, 3, 4, 5]lst2 = [1, 2, 3, 4, 5]하지만 만약 동일한 리스트 코드를 각각 생성하면 다른 객체를 참조한다. 여기까지 다룬 내용들에서도 아주 중요한 것들이 많다. 키워드 문장들을 고르면 다음과 같다. 리스트의 각 요소는 파이썬 객체들을 참조한다. 리스트는 대입연산을 사용하면 동일 객체를 포인팅한다. 리스트는 각각 생성하면 다른 객체를 참조한다.이 3가지 핵심 특성은 뒤에서 설명할 리스트 복사의 개념에서 아주 중요한 것들이다. 일단 이렇게 알아두자.리스트의 내부 구조리스트의 내부이전에 자료구조에서 연결 리스트 (Linked List) 를 배워본 기억이 있을 것이다. 보통 링크드 리스트와 함께 비교하는 자료구조는 배열이 있다. 배열의 장점은 여러가지가 있지만 대표적인 것은 Random Access가 가능하다는 것과 연속적인 메모리 할당으로 빠른 접근 속도이다. 하지만 그 단점인 유동적인 크기의 변경이 불가능하다는 문제가 있다. 그래서 배열의 장점을 일부 포기하고 확장성을 갖는 자료구조가 바로 링크드 리스트이다. 링크드 리스트의 문제점 중 대표적인 것은 불연속적인 메모리 할당이라는 점이 있다.파이썬의 리스트의 이름을 처음 들으면 링크드 리스트 구조라고 생각할 수 있다. 실제로 리스트의 특징을 갖고 있으며 배열의 특징도 갖고 있어서 단순히 Random Access를 지원하는 링크드 리스트라 생각할 수 있다. 하지만 파이썬의 리스트는 연속된 메모리에 요소를 저장하는 배열로 동작한다. 그리고 리스트는 확장에 대비해서 여유있게 메모리를 미리 확보하는 특징이 있다.대입과 누적 대입리스트를 확장하는 방법으로는 append, extend, x = x + [1] , x += [1] 과 같은 방식이 있다. append와 extend는 기본적으로 기존 리스트의 참조를 확장하는 방식이다. 하지만 누적대입(+=)과 대입(=)은 파이썬 내부 동작이 다르게 동작한다. 이 점을 고려하면 좀 더 빠른 연산을 고려할 수 있다.x = [1, 2, 3, 4]x = x + [5, 6]기본적으로 대입을 사용하는 리스트 확장은 기존의 파이썬의 대입의 특징을 갖는다. 즉, 새롭게 객체를 만들고 변수는 새로운 객체를 참조한다. 결국 새로운 객체를 생성하는 추가적인 시간이 발생한다.x = [1, 2, 3, 4]x += [5, 6]하지만 누적대입(+=)은 기존의 리스트에 확장이 되는 방식으로 연산한다. 따라서 기존 리스트 객체가 변화하지 않는다.리스트 복사리스트를 사용하다보면 가장 많이 마주치는 문제가 바로 리스트 복사 이슈이다. 특히 2차원 배열 형태의 리스트를 사용하면 단순 list.copy()를 사용하는 것에 문제가 발생하기도 한다. 그래서 좀 더 자세히 리스트 복사와 관련된 내용을 알아보고, 각 방법에 따른 리스트의 구조 변화를 알아보자.리스트의 단순 복사a = [1, 2, 3, 4]b = ab[0] = 1바로 전 포스팅에서 다뤘던 예제이다. 이 경우 이제는 익숙하겠지만 b는 a 리스트 객체를 참조하기 때문에 a에도 영향을 준다. 리스트의 원본을 유지하면서 변화하는 조작은 프로그래밍에서 상당히 많이 사용되는 기법이다. 보통 변화를 주고 조건 확인 후 문제가 발생하면 원본으로 복구하고, 문제가 없다면 원본에 덮어씌우는 방식에 많이 쓴다. 이런 경우 파이썬은 문제가 될 수 있는 부분들이 있다. 문제들을 해결하고자 파이썬은 copy와 같은 메소드를 쓴다.a = [1, 2, 3, 4]b = a.copy()b[0] = 1가장 간단한 방법은 list.copy()를 사용해서 리스트 복사본을 생성하는 것이다. 이렇게 되면 리스트의 동일한 객체를 복사본을 새롭게 생성하여 리턴한다. 즉 새로운 id를 부여받은 객체를 참조한다.a = [1, 2, 3, 4]b = list(a)b[0] = 1또 다른 방법은 list 함수를 사용해서 새로운 리스트를 생성하여 반환하는 것이다. 잠깐 위로 올라가보면…리스트의 주요 특징 중 각각 생성한 리스트는 새로운 객체들을 참조한다는 것이 있었다. 결국 이 방식은 완전히 새로운 리스트를 생성하는 것과 동일하기 때문에 복사와 같은 동작을 한다.a = [1, 2, 3, 4]b = a[:]b[0] = 1마지막으로는 리스트 슬라이스를 모든 인덱스로 하면 새로운 리스트가 생성된다. 두 리스트에 대해 객체 비교를 하면 서로 다르게 나타난다. (하지만 튜플은 같다고 나타난다. 이는 이후 설명할 튜플과 딕셔너리에서 다루겠다.)얕은 복사, 깊은 복사이제 복사를 알았으니 자유롭게 리스트를 변경할 수 있다고 생각하겠지만… 절반만 맞는 말이다. 파이썬의 변수가 객체를 참조한다는 점에서 참 골치가 아픈 일이 발생한다. 아래 코드의 출력 결과는 과연 뭘까?a = [[1, 2], [3, 4]]b = list(a)b[0][1] = 1print(a)위에서 말한 대로라면 [[1, 2], [3, 4]]가 나와야 하지만, [[1, 1], [3, 4]]가 된다.이게 이렇게 되는 이유는 파이썬의 변수는 객체를 참조한다는 것과 리스트는 객체 참조자들의 컨테이너라는 점때문에 발생한다. 위에서 말한 리스트 복사는 변수가 리스트를 참조만 하는 것을 막기 위해 동일한 값을 참조하는 참조자들을 저장하는 리스트를 새롭게 만드는 것이다. 이게 참 머리가 아파지는 부분이다. 이해를 돕고자 그림으로 어떻게 된 상태인지 보여주겠다.간단하게 말하면 우리가 복사하는 리스트는 PyListObject 객체 자체를 복사한다. 결국 참조자들은 모두 동일하게 유지된다. 그래서 여기서는 매우 특이한 특징이 나타나는데, 앞서 다룬 포스팅에서 -5 ~ 256까지는 동일 숫자 객체를 무조건 지시하지만 범위 밖의 숫자 객체를 매번 새롭게 생성된다고 했다. 하지만 만약 리스트 안에 있는 257은 리스트를 복제하면 다른 객체가 될까?정답을 말하면 “아니다” 이다. 이유는 앞서 말한 이유인데, 결국 참조자를 복사하는 것이고 참조자들은 동일 객체를 가리키고 있다. 결국 이런 이유 때문에 앞에 나온 2차원 배열의 문제가 왜 그렇게 나오는지 알 수 있다. 2차원 배열의 구조를 디테일하게 나타내면 아래와 같다.결국 바깥 리스트의 참조자를 복사하는 것은 성공했지만 그 참조자가 가리키는 내부의 리스트는 복사하지 못했기 때문에 발생하는 문제였다. 이 문제를 해결하려면 내부의 리스트 참조자까지 모두 복사를 해야 한다.이를 위해 copy 라이브러리의 deepcopy 메소드를 사용해야 한다.import copya = [[1, 2], [3, 4]]b = copy.deepcopy(a)b[0][1] = 1이렇게 복사를 하면 리스트의 내부까지 복사한 새로운 객체를 만들어서 변수에 할당한다. 따라서 내부의 리스트의 id도 새로 생성한다.deepcopy, copy에 따른 id 비교우선 copy로 복사한 경우 얕은 복사가 되며 내부에서 각 리스트의 id를 보면 동일하게 유지되는 것을 볼 수 있다. 이런 구조에서 내부 리스트를 변경하면 그냥 리스트 객체에 참조를 새로 대입한 것과 동일한 것이다.하지만 deepcopy를 하면 list의 id가 서로 다른 것을 알 수 있다. 이렇게 복사를 하기 때문에 특별한 문제없이 리스트의 변화를 줄 수 있다.리스트는 파이썬에서 가장 많이 다루는 자료형이지만 생각보다 그 내부를 깊이있게 알고 사용하는 사람은 많지 않다. 이는 딕셔너리, 튜플도 마찬가지인데 리스트는 독립적으로 다룰 필요가 있을만큼 내부구조가 복잡하다. 그래서 이번 글에서는 리스트만 따로 분석을 했다. 다음 포스팀에서는 딕셔너리와 튜플을 알아볼 예정이다. 필요에 따라서는 딕셔너리는 따로 분석할 수도 있다." }, { "title": "[Deep Dive Python] 1. Python의 객체와 변수 개념", "url": "/posts/variable/", "categories": "Programming, Deep Dive Python", "tags": "Python, Programming, Variable", "date": "2022-07-10 23:30:00 +0900", "snippet": "Deep Dive Python : Python의 객체와 변수의 개념들어가며면접 과정에서 파이썬과 관련된 질문을 많이 받았는데, 생각보다 내가 파이썬을 잘 모르고 있다는 사실이 상당히 충격으로 다가왔다. 그래서 기본서로 공부할 책과 좀 더 심화적인 스킬들을 공부할 책을 선정해서 파이썬을 깊게 팔 예정이다. 포스팅 시리즈는 Deep Dive 시리즈로 선정했으며 일반적인 파이썬 기초와는 거리가 있을 것이다. Deep Dive Python 시리즈는 다음과 같은 것들을 중점으로 공부할 것이다. Python의 기초 활용보다는 기초에서는 살~짝 벗어난 관점들 Python의 메모리 단계까지의 관점 Python을 더 잘 활용하는 방법 Python의 효율적으로 활용하는 것 CPython과 공식문서로 자세한 분석이 글을 읽는 사람이 만약 Python을 처음으로 입문하는 사람이라면 별로 추천하지 않는다. 이 글은 많은 컴퓨터공학적 지식이 기반에 깔려있다고 생각하고 작성하는 글이기 때문이다. 물론 꾸역꾸역 읽으면 어디가서 아는 척하긴 좋겠지만… 딱히 초보자에겐 추천하지 않는다.Deep Dive Python 첫번째 시리즈로 Python의 객체와 변수 개념에 대해 다룰 것이다.Python의 객체Python의 구현체우선 Python은 무슨 언어로 구현되어 있을까? Python의 표준 구현체는 C언어로 구현되어 있다. 물론 C++이나 파이썬, Java로 구현된 구현체들도 있지만 표준 구현체는 CPython이고 python의 공식 레포지토리도 CPython으로 지원한다.Python Object출처 : 나무위키 - 파이썬파이썬 공식문서에 들어가보면 모든 구현체들이 Python Object라는 객체로 구현된 것을 볼 수 있다. 이렇게 공식문서를 보면 알 수 있듯이 파이썬은 객체 지향으로 개발된 언어라는 것을 알 수 있다. 그래서 보통 Python 자체를 순수 객체지향 언어라고 부른다.다만 파이썬은 객체 지향 언어라고 표현하는 것은 조심할 필요가 있다. 물론 파이썬이 객체 지향으로 개발된 언어인 것이므로 틀린 말은 아니지만 객체 지향은 프로그래밍 패러다임이기 때문에 사용자의 사용성에 따라 불리는 것이 달라질 수 있다. 파이썬은 절차 지향으로 프로그래밍을 배우기도 하고 상황에 따라서는 함수형 프로그래밍도 가능하다.파이썬의 객체는 크게 가변형(Mutable) 과 불변형(Immutable) 로 구분한다. 가변형 : 리스트, 딕셔너리, 집합(set) 등… 불변형 : 숫자, 문자열, 튜플 등…이런 특징 때문에 파이썬은 함수에 매개변수를 어떤 것을 전달하냐에 따라 처리가 달라진다. 불변 객체를 넘겨줄 경우 특수한 처리를 하지 않는다면 Call by Value로 처리하고, 가변 객체를 넘겨주면 Call by Reference로 처리한다. 파이썬 공식문서에서는 Call by Assignment (할당에 의한 호출) 와 Call by Object Reference (객체 참조에 의한 호출) 이라 한다.함수에 대한 얘기는 나중에 자세히 파보도록하고 일단 이번 포스팅에서는 불변 객체 위주의 변수 할당을 알아보겠다.Python의 변수Python의 변수와 객체의 관계파이썬이든, C++이든 자바든 많은 언어들은 = 연산자를 활용해서 변수를 선언한다. C++와 같은 언어는 메모리에 변수가 저장되는 방식으로 관리한다. 새로운 변수에 다른 선언된 변수를 할당하면 새로운 메모리에 동일한 객체가 복사되는 방식으로 저장된다. 그래서 만약 동일 메모리의 변수를 가리키고 싶다면 C++은 포인터(pointer) 를 사용한다.여기서 파이썬이 간단해보이지만 꽤 복잡한 언어인 이유가 나타나는데, CPython의 객체 코드를 뜯어보면 많은 포인터들로 구성되어 있다는 것을 알 수 있다. 일단 간단하게 깔고 들어가면, 파이썬의 변수 대입(=)은 새로운 메모리 할당이 아니라 객체에 대한 참조이다.Python 객체의 기초 지식여기서부터 조금 복잡해지는데, 우선 기본적으로 알고 가야하는 파이썬 지식 중 하나는 파이썬에서 -5 ~ 256까지의 수는 이미 만들어진 객체를 할당한다는 것이다. 이 범위 밖의 정수는 새로 객체를 할당한다. 특이한 점은 같은 257을 두번 할당하면 서로 다른 객체가 선언된다.여기서 객체의 주소와 비슷한 격인 내용을 확인하려면 id(변수명)을 사용하면 객체의 아이덴티티를 알 수 있다.Python의 변수 선언파이썬에서 리스트를 사용하다보면 아래와 같은 코드를 짜고 난감한 결과를 경험한 적이 있을 것이다.a = [1, 2, 3, 4]b = ab[0] = 1위 코드에서 a의 값은 변화가 될까? 라는 질문을 받으면, C++을 메인으로 사용한 사람의 경우 변하지 않는다고 말할 것이다. 하지만 파이썬은 변하게 된다. 이유는 위에서 언급했던 파이썬의 변수 할당 방식때문이다.파이썬은 변수에 값을 복사하는 개념이 아니라 변수가 지시(point)하는 객체가 바뀌는 것이다. 가끔 그런 생각해본 적 없는가? “파이썬은 왜 자료형을 선언하는 방식으로 변수를 선언하지 않지?”파이썬에서 변수의 역할을 생각해보면 자료형을 굳이 선언할 필요가 없다. 왜냐면 파이썬에서 변수는 그냥 지시하는 객체만 바꾸면 되기 때문이다. 우리가 C++이나 자바에서 자료형을 선언하는 이유는 자료형에 따라 메모리에 할당하는 공간의 크기를 결정하기 때문이다. 하지만 파이썬은 객체를 생성하고 변수는 객체를 지시만 하면 되므로 그럴 필요가 없는 것이다.이런 특징과 위에서 언급한 -5 ~ 256까지는 이미 만들어진 객체를 사용한다는 것 때문에 신기한 결과가 나타난다. 같은 257인데, 다르다고 나올 수 있다.동일성 판단 (is)Python에서는 동일성을 판단하는 연산자가 크게 2가지 종류가 있다. ==와 is 연산자이다. 일반적으로 프로그래밍 언어는 불필요한 연산자와 기능을 굳이 만들지 않는다. 그렇다면 얼핏 보기에 비슷한 두 연산자가 왜 있을까? 그 이유는 당연하게도 역할이 다르기 때문이다.자바스크립트를 써 본 사람이라면 ==과 ===의 차이점을 알 것이다. 위 두 연산자의 차이도 이것과 비슷하다.==은 단순히 객체 값의 동일성을 판정하고, is는 객체 자체의 동일성을 판정한다.아래의 코드의 출력결과는 어떻게 될까?a = 3b = 3c = 257d = 257print(a == b)print(a is b)print(c == d)print(c is d)물론 눈치 빠른 사람들은 T T T F라는 것을 알 수 있다. 근데 왜…? 객체가 같지/다르지? 라는 생각이 들 수 있다. 이유는 -5 ~ 256은 이미 만들어진 객체를, 범위 밖의 숫자는 새로 객체를 할당한다. 이런 이유때문에 3은 같게, 257은 다르게 나온다.del변수를 제거하는 연산, 특히 dictionary나 리스트에서 제거할때, del 연산자를 사용하는 것을 본 사람들이 있을 것이다. 이름이 del이라서 처음 배운 언어가 뭐냐에 따라 메모리 할당 제거를 하는 것이라 생각할 수 있으나 del의 역할은 변수와 객체의 연결관계를 분리하는 것이다.그렇다면 여기서 사용되지 않는 객체는 어떻게 관리될까?파이썬은 몇 개의 변수가 객체를 참조하고 있는지 관리하는 변수인 ob_refcnt가 있다. 참조 변수가 만약 0이 된다면 파이썬은 일정 시간 후에 메모리를 해제하는 코드를 동작시킨다.static inline void Py_DECREF(const char *filename, int lineno, PyObject *op){ _Py_DECREF_STAT_INC(); _Py_RefTotal--; if (--op-&amp;gt;ob_refcnt != 0) { if (op-&amp;gt;ob_refcnt &amp;lt; 0) { _Py_NegativeRefcount(filename, lineno, op); } } else { _Py_Dealloc(op); }}이런 방식을 써서 파이썬은 메모리를 관리한다.파이썬은 변수만 봐도 굉장히 깊게 공부할 요소들이 많다. 이 부분은 파이썬이 동작하는 과정에서 중요한 부분이다. 변수 컨트롤을 진행하는 요소나 서비스를 개발하는 과정에서 리소스 관리에서도 중요하게 작용한다.이번에는 불변 객체 위주로 변수를 봤고 다음 포스팅은 가변 객체인 리스트, 딕셔너리, set의 구조와 동작에 대해서 다뤄볼 예정이다." }, { "title": "[Review] 네이버 부스트캠프 AI Tech를 마치며...", "url": "/posts/BCretro/", "categories": "Life, Career", "tags": "네이버, NAVER, BoostCamp, 부스트캠프, AI Tech", "date": "2022-06-28 13:00:00 +0900", "snippet": "네이버 부스트캠프 AI Tech를 마치며… 눈이 많이 내리던 지난해 겨울에 시작한 부스트캠프는 비가 많이 내리기 시작할 여름에 마무리 됐다. 1월부터 6월까진 진행한 부스트캠프 AI Tech 3기 활동을 마무리하며 지난 6개월의 여정을 돌이켜 볼 생각이다.본격적인 시작데이터 직군을 희망한다고 남들한테 많이 말했던 거 같은데 막상 ‘누군가한테 설명할 수 있을까?’ 라는 질문을 스스로에게 던졌을 때, yes라는 대답이 바로 나올 수가 없었다. ‘대충이라도 설명할 수 없다’ 라는 것이 나에게 많은 생각을 하게 만들었고 가장 체계적이고 좋은 공부 방법이 무엇일까? 고민하다가 부스트캠프에 지원했다.오리엔테이션과 첫 느낌오랜 기간을 AI, 데이터 관련으로 준비했어서 자소서를 작성하는데 큰 어려움이 있지 않았다. 자소서를 어떤 느낌으로 썼는지 궁금하다면 부캠 합격 후기를 확인해 보시길…부스트캠프의 시작기대반 걱정반으로 첫번째 OT에 참석했고 첫 데일리 스크럼을 진행했다. Level 1 동안 기초적인 수학, 통계 AI에 대한 지식들을 학습하는 것이 학습목표였고 함께 할 동료들은 부스트캠프 측에서 구성해줬다. 정말 다양한 분야의 사람들이 모였고 항상 컴퓨터공학과 혹은 공대 사람들만 보던 나에게는 새로운 경험이었다.조교 활동을 하면서 비전공자 사람들을 여러번 상대해본 적은 있지만 아무래도 학교 수업을 듣는 학생들은 간절함과 진심이 없는 경우가 많다. 하지만 여기 들어온 사람들은 그때 느끼지 못했던 간절함과 진심을 갖고 있었고 전공자로서 도와줄 수 있는 부분에서 많이 도움을 주고 싶었다.하루하루 회고를 다지며…올 한해 참 많은 스포츠 대회때문에 힘든 부캠을 잘 견딜 수 있었다.Leve 1과 Level 2 기간동안 절대 놓치지 않았던 것들이 있었다. 적어도 강의에서 나온 내용을 최대한 내 방식으로 기록하자였다. 내가 부스트캠프를 시작했던 이유는 내가 하기로 한 분야에 대해서 대충도 설명 못하는데 어떻게 그 분야에서 일할 수 있겠어? 라는 생각때문이었다. 이런걸 해결하려면 반드시 그날 공부한건 그날 정리했다.그리고 그날 그날 회고를 꾸준히 기록했다. 정말 큰거부터 정리 시간이 오래 걸린다는 사소한 부분까지 기록했다. 물론 P-stage와 Product Serving에서는 너무 개인 회고를 적는거 보다 프로젝트 개발, 실험 일지를 적는걸로 대체했다.지금 회고를 적으면서 확인해보니까 벌써 119개의 글을 적었다.사실 product serving 파트는 아직 적지 못한 부분이 많아서 시간 여유가 될 때 필기한 내용들을 정리할 예정이다. 또한 추천 시스템 이론도 이원성 마스터님께서 설명해주신 부분들을 자세히 기록하지 못했어서 해당 부분도 다시 정리할 예정이다.이렇게 다 정리하면 대충 150개 글이 완성될 거 같다. 나름의 방식으로 이해하려 노력했고 최대한 강의 내용을 그대로 적기보다는 추가적인 내용을 덧붙이려 했다. 내용이 많았고 시간적인 부분이 문제가 되는 경우가 많았다. 하지만 내가 얻고 싶은 목표를 달성하고자 꼭 해야할 일 중 하나였다.다양한 대회 경험처음으로 겪은 시작부터 끝중간에 한강뷰 라이브 유튜브 채널을 틀어놓고 지낸 기간도 많았다.운영측에서 꾸려준 팀으로 7주간의 기초교육을 받으며 빈틈밖에 없던 많은 부분을 채워나갔다. 마지막에 진행한 image classification 대회를 맨 처음에 봤을 때는 ‘CV 트랙에게 유리한 대회가 아닌가?’라는 생각이 들었다. 하지만 막상 패를 까보니 NLP팀들이 상위권을 쓸어담고 있었다. 지금도 생각해보면 맨날 최신 트렌드를 알아본다고 MLOps에 대한 관심도 갖고, 정보도 찾아보고 했는데 최근 트렌드인 Data-centric에 대한 접근은 하나도 하지 않았던 것으로 기억한다. 계속 모델에만 인적 리소스를 넣었고 성능 향상도 드라마틱하지는 못했다. 결국 대회 마지막날 모델 output 전수조사를 하면서 데이터 분포를 변경해보면서 성능 향상을 이뤄냈다. 결국 Andrew Ng이 맞았다. 하지만 나는 알고만 있었고 대입하지 못했다. 이런 것도 결국 내가 초보라는 반증이 아니었나라는 생각이 들었다.처음으로 PyTorch를 사용해 처음부터 끝까지 하나의 학습 파이프라인을 만들어 본 경험이었고 코드적인 부분, 논문을 적용하는 부분 등 많은 방면에서 경험을 늘릴 수 있던 시작 그 자체의 역할을 제대로 했다. 첫 대회를 경험하면서 사이언티스트와 엔지니어 그 사이 어딘가에서의 방향에서 어디로 갈 지 결정할 수 있던 계기가 되었다.확실히 수식이나 이론적인 내용을 다루는 것이 재미는 있었지만 내가 컴퓨터공학과에 진학한 이유를 떠올려보면, 결국 프로그래밍을 통해 스스로 무언가를 만들어 보기 위해서였다. 이 대회를 경험하면서 엔지니어 역할이 내가 가장 바라는 것이라는 걸 알 수 있었다.추천 시스템, 그리고 실수, 발전Level 2에 들어가면서 본격적으로 추천 시스템 트랙에 대해 심화된 학습을 진행했다. 추천 시스템을 처음으로 만난 것은 지난 해 CLOVA AI RUSH 2021에서였다. 나는 항상 실제 세계에서 적용되는 것들을 바래왔다. CV, NLP도 많이 실생활에 적용되고 있지만 추천 시스템만큼 real world가 더 중요한 부분이 없다고 생각했다. 이런 부분이 나를 추천 시스템으로 이끌었던 것 같다. 부스트캠프에서 추천 트랙 강의를 들으면서 가장 많이 들었던 것이 ‘CTR’, ‘실제 서비스 환경’ 같은 단어였다. 대회를 경험하고, 마지막에 서비스를 개발하면서도 같은 추천 시스템이지만 완전히 다른 이슈들을 마주했었다.Movie recommendation 대회에서도 결국 데이터였고, 이번에는 모델 분석도 꽤 중요했다. 데이터 분석가 역할을 초반에 맡아서 진행했는데, 당시의 실수가 아직도 기억난다. Movie Lens 데이터를 재구성했다는 점때문에 대회에서 말한 sequential 특징을 완전히 무시해버리는 논리를 펼쳤다는 것이다.지금 돌이켜보면 제일 큰 문제는 데이터를 실제 sequential로 분석해보고 상관관계가 적다는 결과를 낸 것이 아니라 단순히 데이터가 Movie Lens라는 이유로 그랬다. 데이터 분석가라는 사람이 데이터를 철저하게 분석하고 내린 결과가 아닌 직감에 의한 결과라는 것이 문제였다. 이것도 지금보면 결국 초보자, 경험부족에서 발생한 문제였다.대회를 경험하면서…대회 진행 내용을 자세히 적는건 회고에서는 큰 의미가 없다고 생각한다. 만약 진행내용이 궁금하다면 P-stage 후기들을 찾아보길 바란다.대회를 겪으면서 가장 크게 느낀건 난 진짜 초보자 그 자체였다라는 것이다. 그리고 대회가 끝나고 나서 초보자만이 겪을 수 있는 실수였고, 현업에서 겪기 전에 경험했다는 것이었다. 가장 핵심적인 실수들은 다음과 같았다. 최근 트렌드에 대한 정보를 알아보고, 수집했으나 실제 적용하지 못함 데이터에 입각한 분석보다 직관에 의한 분석이 더 앞섰다. 확장적인 생각을 할 수 없었다.현업에서 겪기 전이라고 말은 했지만 이 자체도 섣부른 판단일 수도 있다. 아직 현업에서 일해본 적이 없으니 말이다. 내가 가장 초보라고 느낀 부분은 데이터에 입각한 객관적 분석을 하지 못했다는 것이었다. 데이터 엔지니어, 사이언티스트, 분석가 등 데이터 직군에서 일하는 사람들은 결국 데이터애 입각한 분석결과를 내야하는 것이었다. 물론 ‘일반적으로 이러이러하니까 이러이러할 것이다’라는 가설을 세울 수는 있지만 결국 가설이다. 데이터를 분석하고, 통계적인 수치로 검정을 해야하는 것이다. 이 부분은 아직도 많이 부족하다고 생각하고 스스로 경계하는 부분이다.대회에서 참 많은 스트레스를 받았지만 그만큼 많은 실수를 하면서 여러가지 깨달음을 얻었다. 단순히 지식적인 부분만이 아닌 실수를 통한 경험을 할 수 있다는 것도 장점이었다고 생각한다.서비스를 개발하면서…부스트캠프의 마지막 과정은 Product Serving이다. 데이터 수집부터 서비스 배포까지 모든 단계를 개발하는 과정이다. 이활석 마스터님도 말씀하셨지만 추천 시스템의 특성상 모델 자체의 방향보다 전체 서비스 아키텍처설계가 중요했다. 또한 LINE, Kakao 등 많은 기업들에서 고민하고 있는 문제인 추천 inference time에 대한 고민도 많이 필요했다. 하지만 Level 3 - Product Serving에서 가장 중요한 것은 대회를 경험할 때와 달리 모든 것을 내가 다 할 수 없다라는 것이다.프로젝트를 진행한 경험이 팀에서 가장 많아서 의도하지 않았지만 PM의 역할을 담당했다. 개발, 모델 연구, 데이터 수집을 한 명이 할 수 없다는 것을 인지하는 것이 가장 중요했고 팀원들의 역할분담을 가장 우선적으로 진행했다. 그리고 여기서부터 내가 할 수 있는 것은 팀원을 믿는 것이었다.물론 매일 데일리 스크럼과 피어세션때 자신의 진행상황을 브리핑하는 것이 있었지만 결국 팀원이 얼마나 자신의 역할을 잘 수행하고 있느냐가 핵심이었다. 물론 팀원들도 나를 믿고 자신의 역할을 하는 것이라 책임감을 가지고 진행했어야 했다. 무엇보다 학습 데이터와 추천 목록에 사용되는 데이터를 수집하는 것이 내 역할이다보니 서비스 개발의 핵심적인 역할이라는 부담이 컸다.전공기초 지식을 왜 배워야 하는가?사실 컴퓨터공학을 전공하는 많은 사람들도 전공지식이 실제 서비스 개발에 얼마나 사용될까?라는 생각을 갖는 경우가 많다. 데이터베이스를 배워도 DBMS 프로그램을 사용하고, SQL문을 사용하는 정도이고 내부 구조를 실제 건드리는 경우가 많지 않을 것이라 생각한다. OS를 배워도 어차피 사용하는 OS는 리눅스, 윈도우즈, 맥OS이고 이미 잘 만들어진 OS들이다. 사실 나도 뭔가 쓰인다고는 들었는데 어디서 어떻게 쓰이고 실제 쓰이는 것을 보지도 못했으니 와닿지는 않았다.하지만 이번 서비스 개발에서 데이터 수집 파이프라인을 구축하면서 전공지식의 필요성을 가장 크게 느꼈다. 기존에 데이터를 수집하는 코드의 방식은 monolithic으로 수집과정 전반에 문제가 발생하면 처음부터 다시 처리해야하는 문제가 있었다. 하지만 데이터를 주기적으로 데이터베이스에 저장한다면, 그런 문제를 조금은 해결할 수 있었다. 또한 API 제한이 발생하면 데이터베이스 저장도 문제가 발생하므로 데이터 수집, 저장을 비동기로 분리해야 했다.이 과정에서 상당히 고생을 많이 했었다. 서로 다른 코드를 만들면 좋은데, 문제는 연결하는 방법을 몰랐다. 부캠 슬랙에 도움을 요청했고 같은 캠퍼분 중에 관련된 기술로 잘 아시는 분께서 도움을 주셨다. 당시 설명하신 내용은 redis와 같은 message queue를 이용해서 sender-receiver를 사용하는 것이었다. 이 방식은 내가 OS에서 배웠던 프로세스 통신 방법 중 하나였다. 정말 일부분 중 하나였지만 이런 사소한 이해를 얼마나 잘 적용하느냐가 중요하다는 것을 알았다.이런 부분을 잘 알기 위해서 우리는 전공공부를 하는 것이고 기술면접을 보는 것이라 생각했고 크게 느꼈다.자세한 내용은 최종 프로젝트 개발일지를 읽어보길 바란다.서비스 개발은 현실이다.팀원과의 큰 트러블이 있지는 않았지만 가장 어려웠던 부분도 있었다. 팀원의 구성이 취직 + 대학원(연구)로 나눠져 있었다. 프로젝트를 진행하면서 데이터의 문제, 모델적인 문제들이 많이 존재했었고 심지어는 서비스를 하는 과정에 평가, 소비자의 행동과 같은 문제들도 있었다. 내부 회의에서 수많은 문제들이 제기되었지만 이 문제들을 모두 해결하는 것은 불가능했다. 때로는 현실적으로 바라보면서 일부 문제들은 안고 가야할 필요도 있었다. 그게 실력적인 문제이든, 시간적인 문제이든 그건 지금 중요하지 않다. 결국 우리는 서비스를 완성해야 한다. 완벽한 서비스는 없다. 하지만 완벽해지려고 노력할 수는 있다. 노력을 하려면 일단 만들어야한다. 그러니까 때로는 치명적인 문제가 아니면 안고 끝을 보긴 해야 한다는 것을 알았다.마무리회고를 적기는 했는데 이게 회고인지… 그냥 부캠하고 나서 느낀 생각 주저리 주저리 적어놓은건지 잘 구분이 안가기는 한다. 하지만 그만큼 가식을 없애고 가장 있는 그대로 느낀 것들을 적었다.내가 2년 전에 심리상담을 받으면서 들었던 말이 있다. ‘사소한 성공 경험을 하나씩 하는 것이 중요하다.’부캠정도의 내용이 사소한 성공 경험은 아니지만 나는 또 새로운 도전 한개를 끝까지 마쳤다. 여러개의 할 수 있는 것들이 생겼고 더 해보고 싶은 많은 것들이 생겼다.이제는 실전1년 전의 나와 지금의 나를 비교해 볼 수 있는 아주 좋은 기회가 생겼다. 1년 전 AI Rush를 참가했을 때는 아무것도 모르는 운 좋은 사람이었다. 이제는 아니다. 대회, 프로젝트를 경험하면서 어떻게 공부하고, 모델을 사용하는지를 배웠다. 잘 배운 것을 이제는 잘 녹여내고 적용해 볼 기회다. 이번 AI Rush도 너무 기대되는 것이 지난 AI Rush 2021에서 내 인생의 방향점을 잡을 수 있었다. 과연 이번에는 어떤 방향점을 줄 지 기대가 된다.진짜 마무리사실 어느정도 취업 결과가 나오고 회고를 적으려고 했다." }, { "title": "[BoostCamp AI Tech / Final] Day91 - Consumer 코드 확장", "url": "/posts/final8/", "categories": "NAVER BoostCamp AI Tech, Level 3 - Final Project", "tags": "NAVER, BoostCamp, AI Tech, Proeject, Data Engineering, MLOps, Redis", "date": "2022-06-01 03:00:00 +0900", "snippet": "Final Project : Consumer code 확장목차 Consumer code 확장Consumer code 확장이전에 Producer-Consumer에 사용되는 Consumer 코드를 insert와 update에 맞춰 데이터를 넣어주는 방식을 제안했습니다. 이때, 당시 글에도 남겨놨던 furture question으로 update condition을 넣는 방식을 적었습니다. 이번에 해당 방식을 도입해서 더 확장성 높은 Consumer로 코드를 버전업했습니다.if &quot;update&quot; in msg: update_data = msg[&quot;update&quot;] rid = update_data[&quot;rid&quot;] uid = update_data[&quot;uid&quot;] conn_repo.update_one({&quot;rid&quot;: rid}, {&quot;$push&quot;: {&quot;star_user_list&quot;: uid}})기존의 consumer의 update 부분은 하나의 기능에 대해서만 동작하는 한계가 있었습니다. 이를 해결하고자 update_one의 argument를 분석해서 확장성을 높였습니다.if update: print(update) update_data = msg[&quot;update&quot;] condition = update_data[&quot;condition&quot;] query = update_data[&quot;query&quot;] conn_repo.update_one(condition, query)실제 update_one은 인자로 update 조건과 update query를 받습니다. 즉, Producer에서 condition과 query를 넘겨준다면 정말로 producer만 갈아끼우면 되는 producer-consumer 파이프라인이 만들어집니다.이렇게 해서 consumer 코드의 개발이 어느정도 진행이 된 것으로 보입니다. 추가적인 기능 개선이 있을 수 있지만 현재의 상태로 충분히 데이터 수집 및 적제를 비동기로 처리할 수 있는 쓰임새만큼 발전되었습니다." }, { "title": "[BoostCamp AI Tech / Final] Day91 - Airflow setting 및 배치 파이프라인 설계", "url": "/posts/final7/", "categories": "NAVER BoostCamp AI Tech, Level 3 - Final Project", "tags": "NAVER, BoostCamp, AI Tech, Proeject, Data Engineering, MLOps, Redis", "date": "2022-06-01 01:00:00 +0900", "snippet": "Final Project : Airflow setting 및 배치 파이프라인 설계목차 Airflow setting Batch train 파이프라인 설계Airflow setting저희 서비스에서 사용하는 추천모델은 크롬 익스텐션을 활용하는 만큼 빠른 반응속도를 보여야 합니다. 따라서 추천 모델들 중 속도가 빠른 편인 VAE 계열의 모델을 사용했습니다. 문제는 VAE 계열의 모델들은 기본적으로 입력을 완선된 형태의 user-item matrix로 사용하므로 신규 유저나 정보의 변화에 대응하기 어렵다는 것입니다. 이를 해결하고자 매시간, 매 2시간마다 주기적으로 배치학습을 진행해서 좋은 성능을 보이는 모델로 새로운 user-item matrix 기반의 학습을 하기로 했습니다.왜 Airflow 인가?스케줄링 워크플로 툴들은 다양하게 존재합니다. Luigi, Metaflow, Airflow 등이 있습니다. 그렇다면 왜 Airflow를 선택했을까요? 우선 가장 핵심적인 이유는 Python으로 동작한다는 점입니다. 기본적으로 ML/DL을 활용하는 서비스이다보니 Python 기반의 코드가 많습니다. 이를 고려해서 연동성을 생각하면 Airflow가 좋은 선택지로 보였습니다. 또한 현재 거의 스케줄링 툴의 표준이 된 상황이라 적용해 볼 가치가 있었습니다.Data 및 BackEnd Architecture현재 서비스의 백엔드 아키텍처입니다. 전반적인 데이터베이스, 메인 API Server는 GCP를 활용하고 모델 학습 정보를 저장하고자 Cloud storage를 도입했습니다. 관련된 내용은 이전 글을 참고해주세요.이렇게 배치학습을 하는 부분의 구조는 다음과 같습니다. 기본적으로 RecVAE 코드가 Airflow 서버상에 존재하고 Airflow는 MongoDB Atlas에서 현재 존재하는 repository(item)의 구성을 모두 가져옵니다. 이후 fitering을 거쳐 유의미한 아이템들만 선별하고 RecVAE 학습을 진행합니다.이렇게 학습이 완료된 모델 정보는 실제 model.pt 파일과 파일이 저장된 cloud storage 정보로 나눠져서 각각 cloud storage와 MongoDB Atlas로 전달됩니다.기본적으로 모델 정보를 저장하는 model collection에는 기준 지표가 되는 score와 학습모델, 파일명, 모델이 저장된 버킷명을 저장합니다. 또한 동점 케이스는 최신 모델을 사용하도록 선정했습니다.실제 cloud storage에는 모델의 정보를 담고 있는 model.pt 파일이 업로드 됩니다. 이 정보들을 활용해서 FastAPI 서버는 주기적으로 모델을 최신, 최고 성능의 모델로 교체합니다.Batch train 파이프라인 설계복잡한 구조의 DAG 설계가 필요하지는 않았습니다. 하지만 전처리를 2번 진행해야하는 과정이 있었고 이 과정에서 연속적인 흐름을 진행할 경우 시간 소모의 문제가 있을 수 있었습니다.따라서 DB에서 repository 정보를 불러와서 user-item matrix를 형성해서 저장하는 preprocessing_base_data task, 실제 RecVAE에 들어가게 되는 데이터로 전처리하는 preprocessing_vae로 구별했습니다.이후 모델을 학습하고 부산물로 나오는 model.pt 파일에 대한 정보 파싱을 진행했습니다. 모델 정보를 저장하는 규약으로는 학습모델종류_점수.pt로 지정했으므로 이에 맞춰 model 이름과 score로 파싱하여 이후 모델 정보를 저장하기 위해 xcom으로 data context를 전달합니다.파이프라인 설계 과정의 이슈와 해결과정Timezone 이슈Airflow는 일정 시간을 기준으로 반복을 하는 스케줄링 툴입니다. 따라서 어떤 기준 시각이 존재해야하는데, 일반적으로 UTC기준으로 시간을 산정합니다. 따라서 구동하는 서버의 timezone 설정이 되어 있지 않는다면 일단 우선적으로 timezone 설정 이슈를 만납니다.우분투 기준, 이를 해결하려면 tzselect를 활용해 해당하는 시간대로 설정을 해주시면 원활하게 airflow webserver가 구동됩니다.DAG의 timezone을 KST로 세팅하기역시나 airflow의 start_date도 UTC를 기준으로 설정됩니다. 이를 해결하고자 다음과 같은 방식으로 작성하면 KST 기준으로 코드를 작성하고 Airflow는 UTC 기반으로 해석합니다.import pendulumfrom airflow import DAGkst = pendulum.timezone(&quot;Asia/Seoul&quot;)dag = DAG( dag_id=&quot;train_batch&quot;, description=&quot;RecVAE batch train dag&quot;, start_date=datetime(2022, 6, 2, 22, 30, tzinfo=kst), schedule_interval=&quot;@hourly&quot;, tags=[&quot;recsys&quot;, &quot;recvae&quot;])Reference 오늘의 집 개발 블로그: 버킷플레이스 Airflow 도입기" }, { "title": "[BoostCamp AI Tech / Final] Day88 - User 수집, Cloud storage", "url": "/posts/final6/", "categories": "NAVER BoostCamp AI Tech, Level 3 - Final Project", "tags": "NAVER, BoostCamp, AI Tech, Proeject, Data Engineering, MLOps, Redis", "date": "2022-05-26 01:00:00 +0900", "snippet": "Final Project : User 수집, Cloud storage목차 User producer &amp;amp; consumer 설계 변경 Cloud Storage 설정 및 연결 코드 개발User producer &amp;amp; Consumer 설계 변경User data의 확보지난번에 활용한 producer-consumer 코드로 repository 수집이 완료되었습니다. 이 repository를 활용해서 학습에 사용할 user 데이터를 구축해야 합니다. 여기서 문제는 단순히 아무 유저 정보를 가져올 수도 없을 뿐더러 저희가 갖고 있는 아이템에서 선택한 값이 없다면 기반 모델로 선정한 RecVAE를 학습할 수 없는 문제가 있습니다.이런 문제를 해결하고자 학습 데이터에 한해서는 역으로 데이터를 추출하는 방식을 활용하기로 했습니다.일반적인 방식도 user-item matrix를 생성하긴 합니다.일반적인 방식은 아이템이 많고 서비스 자체에서 user 데이터를 확보한 상태에서 학습을 진행합니다. 하지만 저희의 서비스는 난잡하게 있는 repository 중 awesome에서 선정한 repository를 위주로 선정했기 때문에 random user의 아이템 hit ratio가 굉장히 낮아질 위험이 있었습니다. 최악의 경우 user-item matrix의 모든 값이 0일 수도 있다는 것입니다. 이를 방지하고자 반드시 최소 1개는 hit가 되는 user-item matrix를 구축하기 위해 갖고 있는 repository에서 역으로 user를 추출하는 방식을 사용하기로 했습니다.저희가 hit를 하는 기준은 repository star이므로 repository의 star user list를 가져와서 학습용 user군을 확보하기로 했습니다. 각 repository별로 최대 100명씩 확보하기로 했고 최종적으로 중복된 유저를 제외하고 약 35만명의 user군이 확보될 것으로 확인했습니다.User producer &amp;amp; Repository 정보 update위의 방법을 사용하기 위해 아래 사진과 같은 방식으로 데이터를 수집하게 설계했습니다.여전히 Github oAuth token이 4개라는 제한이 있기 때문에 안정적인 수집을 위해서는 이전에 사용한 ThreadPoolExecutor로 API request를 병렬처리하기에 약간의 리스크가 있었습니다. 따라서 기존 repository에 저장한 star_pages 필드를 활용해서 star_pages를 10으로 제한하고 각 페이지당 100명으로 값을 가져오게 했습니다.API 호출의 효율성을 높이고 user-item matrix를 만들기 위해 필요한 repository star user list를 확보하고자 동시에 update 정보를 consumer로 같이 보내줘야 했습니다. 따라서 producer는 consumer에게 다음과 같은 방식으로 데이터를 전송하게 설정했습니다.{ &quot;insert&quot;: { &quot;uid&quot;: user unique id, &quot;login&quot;: user github id }, &quot;update&quot;: { &quot;rid&quot;: repository unique id, &quot;uid&quot;: star user id }}이렇게 보내면 consumer는 데이터의 조건 분기로 update 정보와 insert 정보를 동시에 처리할 수 있다는 장점이 있고 consumer의 확장성이 늘어날 수 있습니다.Consumer 설계 변경기존의 consumer는 단순 insert만 처리하게 만들었습니다. insert 함수가 사용자가 설정한 주기에 맞춰 수행되어 batch 단위로 DB에 저장되는 방식이었습니다. 이 consumer의 문제점은 update가 발생할 경우 처리하기 어렵다는 문제가 있었습니다. 따라서 update 정보에 대한 확장이 필요했습니다.# 기존의 consumer 데이터 처리 부분while True: schedule.run_pending() msg = q.get(isBlocking=False) if msg is not None: msg = json.loads(msg) print(msg) print() batch_list.append(msg)이를 해결하고자 producer에서 insert와 update를 구분해서 정보를 보내고 해당 정보의 유무에 따라 consumer는 데이터를 처리하게 됩니다.# 수정된 consumer 데이터 처리 부분while True: schedule.run_pending() msg = q.get(isBlocking=False) if msg is not None: msg = json.loads(msg) insert = msg[&quot;insert&quot;] if &quot;update&quot; in msg: update_data = msg[&quot;update&quot;] rid = update_data[&quot;rid&quot;] uid = update_data[&quot;uid&quot;] conn_repo.update_one({&quot;rid&quot;: rid}, {&quot;$push&quot;: {&quot;star_user_list&quot;: uid}}) if insert: batch_list.append(insert) print(msg) print()아직 수정할 부분들이 있습니다. 어차피 데이터는 insert와 update를 모두 갖고 있으므로 update 처리도 insert 처럼 dictionary의 비어있음 여부로 처리하는 것이 안정적 update를 특정 데이터에 대해서 진행하는 것이 아닌 들어오는 데이터에 맞춰 알아서 처리하는 방식으로 확장성을 높이면 좋을 듯 condition을 json에 추가로 작성해도 좋을까? Cloud Storage 설정 및 연결 코드 개발모델의 성능을 저장하고 보존하기 위해 model.pt 파일 정보를 저장할 필요가 있었습니다. 빠른 inference와 동시에 높은 성능을 확보하려면 batch 단위로 학습이 진행되는 기록들을 저장하고 inference에서는 높은 성능의 모델만 가져오면 되는 것입니다.이를 해결하고자 다음과 같은 두가지 방식을 고안했습니다. MongoDB에 model.pt의 값을 dictionary로 저장 GCP의 Cloud Storage에 저장하고 load하는 방식이때, 1번 과정은 시도를 해봤을 때, model.pt 내부의 n차원 torch.tensor 를 dictionary로 변경하는 과정에서 문제가 발생하였고 2번 방식을 사용하기로 채택했습니다.Cloud Storage 설정Cloud Storage 설정 자체는 어렵지 않았습니다. 블로그를 참고하여 설정했고 이를 사용하는 코드를 데이터 사이언티스트 분들이 원활하게 사용하는 코드를 개발하는 것에 좀 더 집중했습니다.저장 및 로드 코드 개발google cloud에서 제공하는 google-cloud-storage 라이브러리를 사용하면 원활하게 저장과 로드가 가능합니다. 하지만 사용과정에서 문제가 발생하는 것을 막고 MongoDB와의 저장, 로드도 병행해야하므로 코드를 atomic하게 작성할 필요가 있었습니다.따라서 file_to_storage 함수와 download_file 함수를 만들어서 팀원들과 공유했습니다.from pymongo import MongoClientfrom google.cloud import storageimport tarfilefrom datetime import datetimedef file_to_storage(file_path, model_name, score, tag=&quot;&quot;, db_conn=None, file_name=None, tar_zip=False): if file_name is None: file_name = file_path.split(&quot;/&quot;)[-1] if tar_zip is True: tar_name = file_name.split(&quot;.&quot;)[0] + &quot;.tar&quot; with tarfile.open(file_name.split(&quot;.&quot;)[0] + &quot;.tar&quot;) as f: f.add(file_path) file_name = tar_name storage_client = storage.Client() bucket = storage_client.bucket(&quot;model-save&quot;) blob = bucket.blob(file_name) blob.upload_from_filename(file_path) if db_conn is not None: db_conn.insert_one({ &quot;name&quot;: model_name, &quot;bucket_name&quot;: &quot;model-save&quot;, &quot;file_name&quot;: file_name, &quot;time&quot;: datetime.now(), &quot;score&quot;: score, &quot;tag&quot;: tag }) print(&quot;model save complete!&quot;)file_to_storage 함수의 기본 로직 file_name을 지정하지 않으면 원본 파일명으로 저장 tar 압축을 요청하는 영우 tar 파일로 압축 cloud storage 연결, bucket, blob 세팅 cloud storage에 파일 업로드 저장한 model의 이름과 태그, 버킷명, 저장 파일 이름등 모델의 메타데이터를 MongoDB에 저장def download_file(model_name, db_conn, tag=&quot;&quot;, latest=True): if latest is True: ret = list(db_conn.find({&quot;name&quot;: model_name, &quot;tag&quot;:tag}).sort(&quot;time&quot;, -1).limit(1))[0] else: ret = list(db_conn.find_one({&quot;name&quot;:model_name, &quot;tag&quot;:tag}))[0] source_blob_name = ret[&quot;file_name&quot;] storage_client = storage.Client() bucket = storage_client.bucket(ret[&quot;bucket_name&quot;]) blob = bucket.blob(source_blob_name) blob.download_to_filename(source_blob_name) print(f&quot;file download complete: {source_blob_name}&quot;)download_file 함수의 기본 로직 model_name과 tag를 활용해서 MongoDB에 저장된 내용을 탐색 이때, tag는 설정하지 않으면 자동적으로 공백 탐색 이후 cloud storage dusruf, bucket, blob 세팅 파일 다운로드보완할 점이번 개발 일지에서 보완할 점은 다음과 같습니다. 좀 더 확장성 높은 consumer 개발 tar 파일 압축했는데 사이즈가 그대로,….??? 지난번 redis producer-consumer에서 발생하는 문제로 너무 빠른 속도로 데이터가 전달되는 경우 데이터 유실이 발생하는 것으로 보이는데, 이게 진짜 유실인지 아니면 저장 과정에서 시간차가 있는건지 확인해 볼 필요가 있음Reference [GCP] Python에서 GCP Cloud Storage 연동하기" }, { "title": "[BoostCamp AI Tech / Final] Day85 - Redis Producer Consumer", "url": "/posts/final5/", "categories": "NAVER BoostCamp AI Tech, Level 3 - Final Project", "tags": "NAVER, BoostCamp, AI Tech, Proeject, Data Engineering, MLOps, Redis", "date": "2022-05-24 01:00:00 +0900", "snippet": "Final Project : Redis Producer Consumer목차 Redis Producer Consumer 데이터 수집 파이프라인 설계Redis Producer Consumer지난번에 이어서 구동시킨 redis server를 통해 message queue 방식을 활용해서 producer-consumer로 데이터를 DB에 적재시키는 파이프라인을 설계 했습니다.급하게 파이프라인을 설계하고 데이터를 지속적으로 넣어줘야 하는 상황이라 data 수집 부분과 데이터베이스 삽입 부분을 분리해야 했습니다.이를 위해서는 OS에서 배웠던 message passing을 사용하면 처리할 수 있는 문제였습니다. message passing은 sender와 receiver가 메일박스를 통해 메시지를 전달하는 방식입니다. 여기서 message queue의 역할은 메시지 브로커인 redis server가 맡게되고 sender와 receiver는 구동되고 있는 redis server를 통해 데이터 전달을 합니다.여기서 일반적으로 Sender를 Producer, Receiver를 Consumer로 말하기도 합니다.Consumer는 계속해서 데이터를 받으면서 프로그래머가 설정한 DB로 데이터를 넣습니다.결국 Producer 코드만 수정해서 producer는 데이터를 계속해서 넣어주면 됩니다.데이터 수집 파이프라인 설계MSA로 설계한 파이프 라인입니다. 기본적으로 Producer는 문제가 발생해도 Consumer는 계속 msessage queue에서 데이터를 받아오게 됩니다. 설계의 장점은 다음과 같습니다. 지속적으로 데이터를 처리해준다는 것 Consumer는 인자로 저장 collection을 바꿔주기 때문에 저장 확장성이 높다는 것입니다. Producer도 데이터에 맞춰서 모든 데이터 루틴을 다 수집하면 프로그램을 종료합니다. 따라서 마치 블록 조각을 조립하듯이 producer만 갈아 끼우면 데이터 수집을 다양하게 할 수 있다는 것입니다.보완할 점설계에서 보완할 점으로는 다음과 같습니다. Consumer의 중복 문제가 발생할 수도 있다는 점 Consumer를 순차적으로 처리할 방법을 찾아볼 필요가 있음 여러 docker를 사용하므로 kubernetes를 활용하면 더 좋을 것 (공부하자) Producer의 병렬화를 하는 방법을 구상해 볼 필요 단순히 메시지 브로커보다는 활용도를 높이게 이벤트 브로커를 사용하는 것이 좋을 것 같음 Kafka를 써본다면 다양하게 활용할 수 있을 것으로 보임 (공부하자…2) Reference [Python] Redis를 이용한 Message Queue 만들기" }, { "title": "[BoostCamp AI Tech / Final] Day83 - GCP Redis 환경 세팅", "url": "/posts/final4/", "categories": "NAVER BoostCamp AI Tech, Level 3 - Final Project", "tags": "NAVER, BoostCamp, AI Tech, Proeject, Data Engineering, MLOps", "date": "2022-05-21 01:00:00 +0900", "snippet": "Final Project : GCP Redis 환경 세팅목차 Redis 세팅 배경 GCP Docker 설치 Docker Redis Python redis 연결Redis 세팅 배경 부캠 최종프로젝트에 사용하는 데이터를 수집하는데 여러가지 이슈가 발생 이슈 내용 github api를 통해 선정한 repository별로 user 정보를 추출해야 함 githup api는 oAuth token을 사용하면 5000/시간 호출 가능 호출 수 초과하면 HTTPError 반환해서 데이터 수집이 중단됨 중단점 발생시 데이터를 저장하고, 이후 중단점부터 다시 데이터를 수집 도저히 메모리에 다 올려놓고 처리하는건 아무리 생각해도 머리가 안 돌아가서 슬랙에 도움을 요청했습니다. 부캠에 있는 고수분께서 해결책을 제시 message 브로커를 활용해서 메시지 큐 내부에 저장해두고 worker가 메시지 브로커의 정보를 주기적으로 DB에 저장하는 방법을 추천 message 브로커는 redis를 활용하신다고 하셨음 pub/sub 환경을 사용하고자 nats를 사용하심일단 구세주분께서 자세히 알려주셔서 알려주신 방법을 차근차근 따라가보기로 했습니다. 추가적으로 변성윤 마스터님께서도 해결책을 제시해주셨습니다. 해당 해결책은 글 마지막에 추가로 작성하겠습니다.GCP Docker 설치 우선 redis를 계속해서 돌리는 환경을 만들어야 함 backgroung보다는 docker로 계속 돌리는 것이 안정적이라 판단 GCP에 docker부터 설치 Docker 공식문서를 확인해서 잘 설치하면 됨Docker Redis docker로 redis server를 background 형태로 돌려야 함 redis image의 기본 port는 6379이므로 GCP의 6379 port forwarding을 진행 docker가 GCP VM에 올라가서 동작하므로 반드시 네트워크의 모든 인스턴스를 대상으로 연결해줘야 함 방화벽 규칙 설정시 대상 태그를 아무렇게나 지으면 안됨 반드시 일치하는 대상을 선정해야 외부 연결에 문제가 없음 이거 때문에 2시간을 고생… ㅠㅠ 정확히 맞는지 모르겠지만 localhost의 6379 포트를 안 열면 에러가 났던걸로 기억한다.포트 포워딩까지 했으니까 이제 GCP에서 redis를 background로 docker 실행해서 계속 동작하게 만들고 외부에서 python을 통해 송수신이 가능한지 확인하겠습니다.Redis docker 실행$ docker pull redis$ docker run --name myredis -d -p 6379:6379 redis docker로 redis 실행은 redisgate문서 참고해서 설정Python redis 연결 python에서 redis를 연결하는 방법으로는 redis-py를 사용 redis-py 사용해서 이제 GCP 상의 redis server와 연결되는지 확인 여기서 포트 포워딩시 대상 지정 잘못하면 연결 안되고 timeout import redisr = redis.Redis(host=&quot;your_host&quot;, port=6379, db=0, decode_responses=True)r.ping() # True redis 연결시 확인할 것이 몇가지 있음 host는 GCP의 docker를 쓰므로 GCP 외부연결 IP를 사용하면 됨 decode_responses를 True로 안하면 redis는 binary 형태로 데이터를 저장하므로 .get계열로 값을 가져오면 모두 binary 데이터양식 원본 데이터 형태로 get하려면 반드시 decode_responses를 True로 설정해야 함 dictionary를 설정할 때 r.hmset(&quot;key&quot;, dictionary_val)로 작성하면 됨 r.hgetall(&quot;key&quot;) 사용하면 해당 value 모두 가져올 수 있음 변성윤 마스터님 피드백 단순하게 API 호출 제한의 문제가 있다면 API 호출을 우회할 수 있는 방법이 있는지(Key를 더 생성할 수 있을지, 혹시 Key가 없다면 User Agent를 추가하거나 요청하는 IP를 변경해서 처리할 수 있을지)를 고민해볼 것 같아요 또 다른 문제로 중간에 “진행하는 과정까지 된 부분만 반영하고 싶다” 이 부분은 Database의 Commit이란 개념을 아시면 더욱 도움이 될 것 같아요 Database를 수업 시간에 다루진 않았지만, 꼭 보시면 좋을 개념이라 따로 공부해보셔요 Database에선 Transaction이 발생하는데, 그 과정에서 commit이란 과정이 존재합니다. Commit이 트랜잭션의 종료를 의미하고, 만약 작업 중 문제가 생기면 Rollback이 진행됩니다 기범님의 코드에서 토큰 이슈로 호출이 안되는 에러가 발생하면, 그 전까지의 기록(배열에 담고있다거나)를 바로 데이터베이스로 넘기면 되는 것이지요 그리고 작업이 시작될 때는 데이터베이스의 id를 가지고 가서 작업하면 됩니다 Database의 개념적인 부분을 활용해 구현하는 케이스라고 보셔도 될 것 같아요 https://wikidocs.net/4096 혹은 동훈님이 하신 것처럼 메세지 시스템에 던지는 역할과 뒤에서 담는 역할을 나누는 것도 방법입니다 Redis 외에 클라우드 서비스엔 AWS SQS도 있는데 트래픽의 증가에 따라 자동으로 트래픽 대응하도록 할 수도 있습니다! Table의 Key를 잘 만드셔서, 혹시 중복이 들어가더라도 이슈가 없도록 만드는 것도 중요합니다구체적인 부분은 디테일한 상황에 따라 다를 것 같은데, 저는 토큰을 갱신하지 못한다고 하면.. Selenium을 병렬로 띄워서 크롤링하는 것도 생각했습니다(셀레니움이 느리긴 하지만 수집 못하는 것은 없으니 병렬로 진행했어요)병렬로 진행하는 과정에서 저는 Ray라는 라이브러리를 주로 사용하고 있습니다. 라이브러리가 약간 불안정하긴 하지만, 간단하게 쓰기엔 괜찮아서 Ray를 사용하고 있어요. 회사에선 순간 엄청 많은 연산량이 필요해서 CPU 224 Core를 선점형 인스턴스(저렴한 인스턴스)에 Ray로 병렬처리해서 10분만에 끝내기! 등을 했던 기억이 있네요" }, { "title": "[BoostCamp AI Tech / Final] Day83 - 데이터베이스 변경 및 설계", "url": "/posts/final3/", "categories": "NAVER BoostCamp AI Tech, Level 3 - Final Project", "tags": "NAVER, BoostCamp, AI Tech, Proeject, Data Engineering, MLOps", "date": "2022-05-20 13:00:00 +0900", "snippet": "Final Project : 데이터베이스 변경 및 설계목차 데이터베이스 변경 데이터베이스 설계데이터베이스 변경앞서서 MySQL로 데이터베이스를 선정했으나 너무 비효율적인 문제가 있기 때문에 다시 NoSQL로 데이터베이스를 변경하기로 했습니다. 익숙하게 사용했던 MongoDB를 다시 꺼냈고 GCP를 사용하는 클라우드 데이터베이스인 MongoDB Atlas를 사용했습니다.데이터베이스 설계user_info collection 유저의 정보를 저장하는 collection 저장 정보 uid : user unique id login : user github id (login id) star_pages : user starpage count Collection 설계{ &quot;uid&quot;: int, &quot;login&quot;: string, &quot;star_pages&quot;: int, &quot;star_in_item&quot;: list}repostiory collection repository 정보 저장 collection 저장 정보 rid : repository unique id uid : repository owner user unique id login : repository owner github id repo_name : repository name stars : star count star_pages : star page by 100 topics : repository topic tag languages : repository configuration languages category category_L : job type (eg. front-end) category_M : platform or language (eg. React.js) category_S : Criteria classified by awesome updated_at : repository update date Collection 설계{ &quot;rid&quot;: int, &quot;uid&quot;: int, &quot;login&quot;: string, &quot;repo_name&quot;: string, &quot;stars&quot;: int, &quot;star_pages&quot;: int, &quot;topics&quot;: list, &quot;languages&quot;: dict, &quot;category&quot;: { &quot;category_L&quot;: string, &quot;category_M&quot;: string, &quot;category_S&quot;: string }, &quot;updated_at&quot;: datetime}similarity collection 아이템별 top k 유사도를 저장하는 collection item별 유사도별로 저장 저장 정보 rid : repository unique id simn : 특정 유사도 기준 top k개 rid 리스트 Collection 설계{ &quot;rid&quot;: int, &quot;sim1&quot;: list, ... &quot;simn&quot;: list}repo_user collection repository와 user의 star linking collection$\\rightarrow$ repository collection에 star_user_list를 편입하는게 나은가? 저장 정보 rid : repository unique id login : repository owner github id repo_name : repository name star_user_list : starred user rid list Collection 설계{ &quot;rid&quot;: int &quot;login&quot;: string &quot;repo_name&quot;: string, &quot;star_user_list&quot;: list}model collection 지속적으로 훈련하는 model의 정보를 저장하는 collection 가장 score가 높은 모델이 최근에 저장됨 저장 정보 name : model name model.pt : model.pt information time : save time score : model score metric Collection 설계{ &quot;name&quot;: string, &quot;model.pt&quot;: dict, &quot;time&quot;: datetime, &quot;score&quot;: float}" }, { "title": "[BoostCamp AI Tech] Day 82 - 두런두런 수근수근 속닥속닥", "url": "/posts/day82/", "categories": "NAVER BoostCamp AI Tech, Review", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech", "date": "2022-05-18 23:00:00 +0900", "snippet": "Day82 : 두런두런 수근수근 속닥속닥부스트캠프벌써 네이버 부스트캠프에서 활동한 지 4개월이 됐습니다. 합격했다고 글 남긴게 엊그제 같은데 벌써 몇주뒤면 수료식이네요. 사실 저는 부스트캠프를 신청한 목적이 좀 여러가지가 있습니다. 머신러닝 공부하려고 하는데 강압적인 뭔가가 없으니까 잘 안함 솔직히 체계적으로 공부하는 방법 모르겠음… 머신러닝, 데이터가 그렇게 인기 많다는데 왜 내 주변엔 같이 할 사람이 없지…? 나도 인적 네트워크를 넓혀보고 싶다 이력서에 좀 입증할만한 무언가 하나 있으면 좋겠고이런 이유들로 부스트캠프에 참가했습니다. 물론 여기 있는 대부분은 목표를 이뤘습니다. 취업에 어시스트를 해주는 것이 부스트캠프의 장점 중 하나였습니다. 그리고 무엇보다 긴 시간 빡세게 굴러다니는 캠퍼들의 멘탈을 위해 그리고 취업시장에 뛰어 들 캠퍼들을 위해 부스트캠프에는 러닝 마스터라는 특별한 마스터님이 계십니다.러닝 마스터 : 변성윤 마스터귀여운 메타몽을 프로필로 쓰신다. 메타몽…? 이거 근데 어디서 본 거 같은데?product serving part 담당 마스터님이신 변성윤 마스터님께서 러닝 마스터도 병행을 하고 계십니다. 사실 맨 처음에는 잘 몰랐는데 첫번째 두런두런 세션과 저 익숙한 프로필의 메타몽… 그제서야 깨달았습니다. ‘어라…? 왜 저 메타몽이 내 사파리 북마크에 있지?’워낙에 데이터 사이언티스트로 꿈을 잡고 있던 터라 관련 자료를 찾다가 굉장히 잘 정리된 블로그가 있어서 북마크에 넣어둔 적이 있습니다. 알게 모르게 데이터 관련으로 관심이 있는 사람들에게 스며들어 계셨습니다.아마 데이터 관련된 직군이나 MLOps 관련으로 구글링을 하면 어쩐지 오늘은이라는 블로그가 상단에 나오는 경우가 많을 겁니다. 정말 뭔가 구글링을 할 때마다 계속 등장하시는 어디에나 계시는 그런 느낌입니다.현재 SOCAR에서 MLOps 엔지니어로 재직중이신 상태이고 개인적으로 MLOps를 찾다보면 변성윤 마스터님의 영상이나 자료가 많이 나오는 것을 알 수 있었습니다. 확실히 최근에 들어서야 MLOps가 사람들의 인식에 들어오게 되었고 직접 MLOps Korea 매니저를 하고 계시는 변성윤 마스터님의 여러 발표들이 많은 것 같습니다.아무튼…. 정말 제가 원하는 커리어를 밟아오신 분이라 기회가 된다면 부캠 강의 말고도 여러모로 많이 배워보고 싶은 생각이 참 많이 듭니다.두런두런 : 캠퍼들의 커리어 발전과 멘탈관리변성윤 마스터님은 product serving 마스터로 강의를 하시지만 러닝 마스터로 두런두런이라는 세션을 강의하시기도 하십니다. 그럼 두런두런이 무엇이냐…? 직군에 대한 이야기 이력서 이야기 삶의 목표 회사에선 어떤 사람을 채용할까 학습 과정의 의미 면접 커리어 이야기이렇게 현실적인 이야기를 많이 해주시는 것이 이 세션의 목적이라고 생각합니다. 어떻게 보면 취준생들이 가장 좋아하는 현업자의 이야기라고 보면 될 거 같네요.오늘(5/18)까지 총 4번의 두런두런 세션이 진행됐습니다. 각 세션별로 소감을 한 번 적어보려고 합니다! 한 번 시작해보겠습니다~1회차 : 직군 이야기, 삶의 지도, 질문하는 법직군 이야기두런두런의 첫 세션입니다. 최근 관심이 몰리면서 화두가 되고 있는 데이터 관련 직군들에 대해 자세히 설명을 해주셨습니다. 모든 세션이 다 좋았지만 정말 시작이라는 타이밍에 알맞은 주제가 아니었나 생각이 듭니다.사실 저도 데이터 직군을 희망했지만 ML engineer, Data Engineer, Data Scientist에서 고민을 계속하던 상황이었습니다. 취업의 현실성과 내가 좋아하는 것을 생각해봤을 때는 Data Engineer &amp;gt; ML Engineer &amp;gt; Data Scientist인 것 같더라구요. 개인적으로 각 직군들이 혼용되어서 사용되는 현재 상황에서 정확히 어느정도 분류를 해주셔서 직업 방향성을 잡는데에 도움이 많이 된 거 같습니다.어떤 기준으로 정했는 지는 삶의 지도에서 자세히 설명하겠습니다!삶의 지도나의 삶의 지도제가 바라 본 저의 삶의 지도를 요약하면 다음과 같습니다. 컴퓨터공학과를 선택한 이유는 내가 생각한 것을 만들어 낼 수 있다는 꿈 때문 다른 말로 말하면 내가 생각한 것을 구현하는 방법을 찾아보자! 직접 컴퓨터로 프로그래밍하는 것이 좀 더 맞는 거 같다! 프로젝트들을 겪으면서 내가 맡았던 것들 데이터 파이프라인 구축 API 개발 간단한 안드로이드 앱 페이지 제작 (회원가입/로그인 파트 개발) 데이터베이스 관리 클라우드 서버 관리 실제로 생각해보면 내가 재밌던 것들은 인프라나 데이터 구축! 데이터 엔지니어가 더 적합할 것 같다! 분석보다는 프로그래밍이 좀 더 재밌다! 최종 목표는 MLOps engineer 일 처리하는 추진력삶의 지도를 적고 마스터님께서 피드백 주신 부분들이 있습니다! 이 부분들을 계속 생각해보고 개인적으로 답을 해봤습니다. 예민하다는 ‘디테일에 신경쓴다!’라는 워딩으로 바꿔볼 수 있다 ‘생각이 많다’라는 것은 과연 문제점이라고 볼 수 있을까? 이 부분은 어느정도는 문제라고 생각하기도 하고 어느정도는 아니라고 생각하기도 합니다 프로그래밍을 할 때 어떤 문제점을 만나면 그 부분을 해결하는 방법에 대해 계속해서 생각합니다. 이런 면에서는 장점이라고 볼 수 있겠네요. 문제점인 이유는 답이 나오지 않는 안 좋은 생각을 끊임없이 반추한다는 것은 문제점이라고 봅니다.$\\rightarrow$ 이점은 마스터님께서 피드백 주신 것을 살짝 수정해서 개인적인 메모장 같은 데에 생각 나는데로 적어보기로 해결하기로 했습니다! 생각이란게 어딘가로 뱉어내지 않으면 계속 머리에서 돌더라구요! 그래서 어디에다가든 그냥 다 뱉어내려고 개인적인 메모장에 쭉 적어두려구요. MLOps 엔지니어를 단순히 ‘재밌어서’라는 것보다 ‘왜 재미를 느끼는지?’를 포인트로 잡아보는 게 좋을 것 같습니다! 실제로 프로젝트에서 많은 부분 핵심적인 기능을 맡는다기 보다는 인프라 구축을 많이 해본 것 같습니다. 이 점이 ‘단순히 이걸 좋아해서인가?’ 라고 생각했는데 제가 리더로 있을 때나 주변 사람들한테 하는 행동들을 생각해보면 많이 비슷한 것이 있더라구요 실제 제가 주인공이 되는 것도 좋아하지만 남들을 어시스트하면서 모두의 일이 잘 흘러가게 기반 역할하는 편이라고 생각합니다. 평소 저의 성격과 같은 역할이 인프라 구축이라는 생각이 드는 것 같습니다. MLOps 엔지니어가 최종 목표인 이유가 저의 삶의 지도에 연결되어 있는거 같네요. 대략적으로 요약해보면 아래와 같습니다.사진을 누르면 좀 더 크게 볼 수 있습니다질문하는 법질문하는 법은 사실 제가 너무나도 공감하는 내용들이었습니다. 학부조교로 활동할 때 그 고생을 했던 기억… 생각해보면 정말 아득합니다.200명이 넘는 학생들이 질문을 남기는데 그냥 아무생각없이 질문만 남겨서 중복되는 질문만 3~40%가 됐던 기억이 있습니다. 진짜 그때 약간 인간에 대한 혐오가 생겼던 기억이 있네요…..ㅠㅠ질문을 잘하는 것도 실력이다라는 말이 있듯이 질문을 잘하는건 정말 중요합니다. 질문하는 사람은 내 질문 1개면 끝나지만 받는 사람은 100개가 될 수도 있습니다 ㅠㅠ2회차 : 이력서이력서이제 취준을 하는 상황에서 어떻게 이력서를 잘 쓸 수 있을까? 라는 생각을 합니다. 마스터님께서 이력서를 볼 때 항상 애매하게 느끼셨던 부분을 많이 말씀해주신 것 같습니다.가장 중요한 것은 이 회사가 지금 필요로 하는 것은 무엇인가? 를 잘 아는 것이 중요하다고 했습니다. 이런 점에서는 역량을 잘 드러내고 깔끔하게 보여주는 것이 중요하다고 하셨습니다.마스터님께서 말씀해주신 내용들을 기반으로 맞춰서 이력서를 작성해서 보여드렸다고 생각했지만 생각보다 많은 부분을 잘못 이해하고 있던 것 같습니다. 왜 그렇게 느꼈는지는 피드백을 하나하나 반추하면서 얘기해보겠습니다. “박기범”은 어떤 사람인가?는 잘 드러나지 않은 것 같고 너무 스킬적인 접근이 강한 것 같습니다. 솔직히 이 피드백보고 흠칫했습니다 ㅋㅋㅋㅋ 이 부분에 대해서 마스터님께 개인적으로 얘기를 나눴었고 마스터님께서도 말씀을 해주셨습니다. 사실 저는 최근 많은 신입 지원자들의 기술 스펙이 올라갔다고 생각했습니다. 이런 상황에서 내가 살아남으려면 더 화려한 공작새의 깃털을 보여줘야겠다고 생각했습니다. 하지만 제일 큰 것을 놓치고 있던 것은 면접관님들도 제가 신입이라는 것을 알고 오히려 뭔가 애매하게 그려진 도화지보다는 좋은 재질의 고급 도화지를 필요로 하실 수도 있다는 것이었습니다. 뭔가 제가 제일 크게 놓치고 있던 방향성을 제대로 알려주셨습니다. 프로젝트가 너무 간단한 거 같네요! 프로젝트를 조금 더 자세하게 적어서 궁금증을 불러 일으킬 필요가 있다고 생각했습니다! 제가 내린 수정 방향은 프로젝트만 적는 페이지를 따로 하면 좋을 거 같네요! ETL 파이프라인이 있는데 간단한 아키텍쳐를 보여주는 것도 좋을 것 같습니다! 모든 사람들이 가질 법한 프로젝트는 오히려 어려움을 극복하는 과정으로 적어보는 것이 좋을 것 같습니다! 요약하면 저의 역량과 어떤 사람인가? 를 드러낼 필요가 있다! 이 부분은 1회차 소감에 적은 내용들을 잘 정리해서 적으면 충분히 핵심을 어필하기 좋을 거라 생각합니다! 이력서에 대한 피드백을 너무 자세히 적어주셔서 감덩…했습니다! 사실 최근 프로젝트를 어떻게 강조하면 좋을까?를 많이 고민하고 있습니다. 예전에는 기술을 뽐내려고 했지만 좀 더 근본적인 것에 치중해서 이력서를 작성해보려고 합니다. 근본적인 것을 좀 더 잘 볼 수 있는 사람! 임을 강조하고 인프라적인 측면, assistant이지만 주도적인 assistant임을 강조하고 싶습니다!회차가 거듭될수록 시야가 트이는 거 같네요!3회차 : 산업, 회사 찾아보기3회차는 슬슬 본격적으로 캠퍼님들이 체력이 부족해지면서 자신감이 많이 떨어진 때인거 같았습니다.(저도요~)이번 고민상담소에서 말씀해주신 자기배반은 정말 인상깊었습니다. 저도 평소에 메타인지나, 사회심리학같은 것들에 관심이 많아서 찾아보는 편입니다. 최근에 나부터 살아야하지 않을까하면서 자기배반적인 일을 많이 한 거 같고 마스터님이 말씀해주신 자기 배반이 많아지면 타인에 대해 부정적이게 된다는 것을 몸소 느끼고 있습니다. 그래서 요즘은 객관적으로 바라보기를 하려고 많이 노력중입니다.Career Framework Career Framework이런게 있는 줄 처음 알았습니다. 사실 쏘카 채용공고에서 비슷한 걸 봤는데 이게 이런 내용이 기반이 된 줄은 몰랐습니다. 한가지 확실한 것은 저는 쏘카의 채용 공공에서 Career Framework 기반의 내용을 보고 굉장히 맘에 들었습니다!쬐끔~ 건방진 얘기지만 회사에게 지원자도 어필을 할 필요가 있지만 회사도 어느정도는 혹하게 만들만한게 있어야 한다고 생각합니다. 저는 솔직히 쏘카가 이런 도표를 활용해서 지원자들에게 자신의 위치를 정확히 알려준다는 점이 너무 좋았습니다!쏘카 데이터 팀 프레임워크이런 것이 좋았던 이유는 내가 어느 레벨에 있는지 비교할 평가지표가 확실하다는 것이었습니다.회사, 기업 분석가고 싶은 회사가 있으면 그 회사를 미친듯이 파보는 게 좋을 거 같다는 느낌이 많이 들었습니다.단순히 기업/직무를 분석한다는 것이 이 직군이 뭘 원하는가? 보다는 기업의 비즈니스 모델을 잘 아는 것이 중요하다고 느겼습니다.순서를 생각해보면 비즈니스 모델 $\\rightarrow$ 해당 비즈니스에서 직군의 역할을 잘 생각해보는 것이 핵심이라고 보입니다! 최근에 자소서 쓰느라 간단하게 카카오 데이터 엔지니어를 직군 분석만 했는데 간단 피드백을 받아보려다가… 좀 많이 부족한거 같아서 더 수정해서 공유해보려 합니다!4회차 : 프로젝트 매니징 &amp;amp; 시스템 디자인 아키텍쳐고민 상담소이번 4회차의 고민 상담소는 제가 큰 도움을 많이 받은 상담소였습니다. 사실 위에 적은 많은 마음가짐들은 오늘 두런두런을 듣기 전까지는 느끼지 못 했고 오늘에서야 생각을 잡을 수 있게 된 것들이었습니다. 조급함을 다스려라 수능을 4번이나 본 사람이 뭘 그렇게 조급하게 하려고 하는걸까? 대학 늦게 들어가면 많이 늦을 거라 생각했지만 막상 와 보니 딱히…? 샘 월튼은 나이 40이 되어서 월 마트를 만들었다. 처음에 데이터 엔지니어링을 뜨겁게 했던 이유는 무엇인가요? 이 답은 위에 제가 적은 거 같습니다. 내가 잘 만든 인프라 위에서 프로젝트가 잘 돌아가는 것이 제가 좋아하는 것이라 생각하기 때문입니다. 아직도 그 마음은 변하지 않았습니다. 하지만 뭔가 일이 안 돌아갈 때 어떻게 마인드 컨트롤을 할 지 생각해 볼 필요도 있을 거 같습니다. 화려한 스킬보다는 본질적인 사고역량! 어차피 신입은 화려한 공작새가 되기 어렵다! 그리고 사실 필요한건 툴러(Tooler)가 아닌 개발자(Developer)이므로 툴은 사실 크게 중요하지 않고 이 직업의 근본적인 것들을 잘 알고 있는가! 를 잘 생각해볼 것 프로젝트 매니징내용을 정리하는 것보다 강의를 듣고 깨달은 것들을 위주로 적어보겠습니다. 드디어 애자일 방법론이 뭔지 대충은 알 것 같습니다! 최근에 프로젝트를 진행하면서 제일 바빠야하는데 오히려 느릿느릿 해진 느낌이 들었는데 설명해주신 스프린트를 보고 깨달았습니다! 스프린트가 인상 깊었습니다. 간단히 말하면 가즈ㅏㅏㅏㅏㅏㅏㅏㅏ 근데 일단 팀이 굉장히 짜임새가 있어야 할 거 같다는 생각이 들었습니다. 단순 프로젝트를 진행할 때는 맘에 맞는 사람들과 하는게 아니면 생각보다 쉽지 않을 것 같기도 합니다. (경험을 안 해봐서 그런가…?) 시스템 아키텍처 설계제가 가장 해보고 싶은 시스템 아키텍처 설계입니다. 사실 시스템 아키텍처 다이어그램을 어떻게 그리면 좋을까 고민을 많이 했습니다.이번 final project architecture인데 매우 초라합니다. 이번에 알려주신 것들을 토대로 좀 더 정교하게 작성해보려고 합니다.draw.io 맨날 썼는데 이런게 있는줄은… 몰랐습니다. 다시 한번 더 설계를 해봐야 겠습니다.저희 팀의 프로젝트가 API 통신이 많고 서로 다른 클라우드 서버를 2개를 띄우고 사용하는 방법이라 정확한 아키텍처를 그리면 복잡할 수 있지만 그만큼 정교해질 것으로 보입니다. 너무 세세하게 작성될 것으로 보이면 큰 틀을 작성하고 각각의 파트별로 아키텍처를 작성해봐도 좋을 것으로 보입니다.마무리벌서 5번의 두런두런 중 4번의 세션이 지나갔습니다. 이번에 후기들을 작성해보면서 중간중간 마스터님 피드백을 되새겨 볼 수 있던 것이 좋았습니다. 사실 피드백을 주셨어도 P-stage와 프로젝트, 취업 준비가 겹치면서 제대로 곱씹어 볼 시간이 없었는데 이렇게 후기를 작성하면서 하나하나 의미를 돌아 볼 수 있던 것 같습니다.그리고 두런두런이 간격이 좀 있어서 까먹고 있었지만 이렇게 정리하니까 하나의 흐름이 확실히 연결되어 보이는 게 있었습니다. 이번에 이 후기는 단순히 후기만이 아닌 이력서, 포트폴리오, 면접에 쓰기 위한 나를 돌아보는 정리본이 된 것 같네요!" }, { "title": "[BoostCamp AI Tech / Level 3 - Product Serving] Day81 - MLOps 개론", "url": "/posts/day81_MLOps/", "categories": "NAVER BoostCamp AI Tech, Level 3 - Product Serving", "tags": "NAVER, BoostCamp, AI Tech, Product Serving, MLOps", "date": "2022-05-17 21:00:00 +0900", "snippet": "Product Serving : MLOps 개론모델 개발 프로세스Research VS Production모델 개발 : Research 모델 개발의 연구분야는 일반적으로 위 프로세스를 수행 고정된 데이터를 사용하는 학습 학습이 어느정도 진행되면 서비스를 배포하는 단계로 넘어감모델 개발 : Production 배포된 모델의 결과가 이상한 경우에는 적절한 대응이 필요함 Input data가 이상한 경우 실제 서비스는 outlier를 제외하기 어려울 수도 있음 (outlier 여부 판단 어려움) 모델의 성능이 계속해서 변화함 비정형 데이터는 정확한 성능 지표를 얻어내기 어려움 새로 배포한 모델이 더 안 좋은 경우 Research와 production의 성능에서 차이가 발생할 수 있음 이전 모델을 다시 활용하고 재학습을 할 필요가 있음 이런 여러가지 문제가 발생하는 것을 처리할 때, 자동화된 과정이 필요함MLOpsNIPS 2015: Hidden Technical Debt in Machine Learning Systems 실제 ML 코드는 전체 머신러닝 프로덕션 과정에서 극히 일부만 차지함 서비스에 수행되는 과정을 자동화 한 것을 MLOps라고 함 MLOps = ML(Machine Learning) + Ops(Operations) 반복적인 필요 업무를 자동화하는 것 ML engineering + Data engineering + Cloud + Infra 핵심적인 문제와 반복적인 작업을 최소화하고 모델링에 집중할 수 있게 인프라를 구축함 추가적으로 현실의 risk에서 잘 버틸 수 있어야 함 MLOps의 목표는 빠른 시간 내에 가장 적은 위험을 부담하여 Production까지 기술적 마찰을 최소화하는 것   Research ML Production ML 데이터 고정(static) 계속 변함(Dynamic-Shifting) 중요 요소 모델 성능(Accuracy, RMSE…) 모델 성능, 빠른 inference, 해석 가능성 도전 과제 더 좋은 성능을 내는 모델(SOTA), 새로운 구조의 모델 안정적 학습 (데이터 고정)모델 구조, 파라미터 기반 재학습 시간의 흐름에 따라 데이터가 변경되어 재학습 목적 논문 출판 서비스에서 문제 해결 표현 Offline Online MLOps Component 기본적인 요소로는 Data, Model, Feature 모든 요소가 항상 존재할 필요는 없음 필요에 따라 하나씩 추가적으로 구축하는 것이 좋음Server/GPU Infra 배포과정에서 어떤 환경에 배포가 이뤄질 지가 필요함 예상 트랙픽, CPU, Memory 성능, 스케일 변경, Local Server, Cloud 최근에는 클라우드 서버 위에 올려놓고 사용하는 경우가 많음 클라우드 : AWS, GCP, Azure 등 On promise : 회사나 대학원 전산실에 서버를 직접 설치 (NVIDIA DGX) Local GPU와 클라우드 GPU를 잘 구분해서 처리할 필요가 있음Serving Production 과정에서 데이터를 전달할 필요가 있음 일반적으로 Batch Serving과 Online Serving, 2가지로 구분됨 Batch Serving Model의 prediction 처리를 많은 양의 데이터로 처리하는 것 많은 데이터를 일정 주기로 모델에 전달하여 결과를 예측 실시간 예측이 필요한 경우 무리가 있음 Online Serving 데이터를 들어오는 그 즉시 실시간으로 처리해서 예측 병목현상, 확장 가능성을 모두 고려해야함 최근 cortex와 tensorflow serving의 사용추세가 높고 BentoML도 사용이 많은 편Experiment, Model Management 모델의 파라미터나 architecture를 기록할 필요가 있음 기록을 기반으로 가장 좋은 성능을 보이는 모델을 적용해야함 모델의 학습으로 나오는 부산물들인 artifact, 이미지 (feature importance, confusion matrix등…)를 저장해야 함 학습과 관련된 metadata를 기록할 수 있음 필요에따라 모델을 여러개 동시에 사용할 수도 있음 대표적인 실험 assistant는 MLFlow가 있음Feature Store tabular data에서 유용한 feature를 미리 준비해두는 경우 미리 feature store를 만들어두면 시간을 아낄 수 있음 대표적인 feature store는 FEAST가 있음 현재 제한적인 사용때문에 feature store 라이브러리가 그렇게 많진 않음Data Validation 모델 성능에 가장 직접적으로 영향을 미치는 부분 MLOps에서 주의 깊에 바라보는 Data Drift, Concept Drift를 볼 필요가 있음 Concept Drift &amp;amp; Data Drift Feature의 분포나 데이터의 분포를 확인할 필요가 있음 다양한 Drift에 대해 모델이 지속적으로 학습을 해야 성능 개선이 필요함 대표적인 라이브러리로는 Tensorflow Data Validation(TFDV)가 있지만 tensorflow 한정으로 동작할 수 있는 것이라 한계가 있음Continuous Training Drift에 대해 잘 대처했어도 성능 개선이 일어나지 않는 경우가 있을 수 있음 다시 학습을 진행해서 모델을 다시 설계할 수도 있음 이 경우 새로운 데이터, 주기, 평가지표를 재정의 할 수도 있음Monitoring 모델 성능, 인프라 성능을 기록하면서 지속적인 모니터링을 해야함 여기서 지표는 단순히 성능만이 아닌 에러 발생이나 특정 값 이상으로 변동되는지를 확인해야 함AutoML 좀 더 모델의 성능을 높이고 효율을 높이고자 데이터를 넣으면 자동으로 모델 아키텍처를 찾아줌 대표적으로 MS NNi가 있음Further Question (작성중…)MLOps가 필요한 이유 이해하기MLOps의 각 Component에 대해 이해하기 (왜 생겼지?)MLOps 관련 자료, 논문 읽고 추가적인 부분 파악해보기MLOps Component 중 내가 매력적으로 생각하는 Top 3" }, { "title": "[Airflow] 1.데이터 파이프라인과 Airflow", "url": "/posts/airflow1/", "categories": "Data Engineering, Airflow", "tags": "Python, Data Engineering, Airflow", "date": "2022-05-12 00:10:00 +0900", "snippet": "데이터 파이프라인과 Airflow‘Apache Airflow 기반의 데이터 파이프라인’ 내용을 기반으로 작성되었습니다.데이터 파이프라인 최근 데이터에 대한 집중이 높아져서 데이터 파이프라인을 구축해서 관리하고 있음 데이터 파이프라인을 핸들링하는 기술은 Airflow, Luigi와 같이 다양한 기술들이 활용되고 있음 데이터 파이프라인은 보통 ETL 파이프라인이라고 하는데, Extract(추출), Transform(변환), Load(적재) 의 과정을 거치는 파이프라인 반드시 ‘데이터 파이프라인 = ETL 파이프라인’ 인 것은 아님 최근 클라우드 시스템을 활용하면서 ELT의 순서를 사용하기도 함데이터 파이프라인이란 이름에서 알 수 있듯이 데이터가 마치 물처럼 흘러서 갈 수 있게 파이프라인을 만들어 주는 것 데이터 파이프라인이란 앞서 간단하게 말했듯이 데이터를 추출해서 가져오는 과정을 담은 파이프라인 ETL은 데이터 파이프라인의 서브셋이므로 그 자체는 아님 데이터 파이프라인을 통해 들어온 데이터는 최종적으로 data warehouse에 일반적으로 적재됨 최근에 데이터 파이프라인을 구축에는 다음과 같은 것들이 사용됨 분산 처리 프레임워크 Hadoop, Spark 데이터가 대규모로 수집되는 현재 시대에서 하나의 컴퓨터로 데이터를 핸들링하는 것보다 분산 처리 방식을 사용해서 대량의 데이터를 병렬로 처리함 keyword : Hadoop EcoSystem, Spark, Scala, MapReduce 데이터 레이크 S3, HDFS 데이터를 저장하는 일종의 데이터베이스, 아마 데이터가 너무 많아서 호수와 같다고 붙여진 이름인 것으로 생각됨 Workflow 관리 시스템 Airflow, Luigi 데이터 셋 생성, 모델 학습과 같은 다양한 태스크를 그래프 로 연결하여 실질적인 파이프라인을 구축하고 관리하는 시스템 데이터 웨어하우스 BigQuery 데이터 레이크에서 데이터를 가공해서 분석하기 좋게 정리한 것을 데이터 웨어하우스라고 함 데이터 파이프라인 그래프 데이터 파이프라인은 태스크간의 순서가 정해진 형태로 진행됨 반드시 이전 태스크가 제대로 수행되어야 다음 태스크를 수행할 수 있게 하는 것이 필요함 이런 순서가 정해진 동작을 의존성이라 함 의존성을 표현하는 방법으로 데이터 파이프라인을 그래프로 표현하는 것이 있음 태스크 그래프는 방향성을 갖고 있기 때문에 방향성 그래프 (directed graph) 라고 함 각 사각형을 태스크 노드라고 하고 화살표는 태스크 의존성이라 함 반드시 태스크 의존성의 끝은 노드를 가리켜야하고 이전 태스크가 실행되어야 다음 태스크가 활성화됨 화살표의 끝점이 순환이나 반복을 의미하지 않기 때문에 방향성 비순환 그래프(Directed Acyclic Graph) 라고 함 태스크 간의 비순환 속성은 데드락을 방지하기 위해 반드시 필요함 keyword 태스크 태스크 노드 태스크 의존성 방향성 그래프 방향성 비순환 그래프 비순환 속성 DAG 파이프라인 그래프 실행 파이프라인 수행 알고리즘 그래프 안의 태스크는 모두 개방된(open) 상태에서 시작 이전 태스크가 완료되었는지 확인 태스크가 완료되면 다음 실행할 태스크를 대기열에 추가 실행 대기열의 태스크 수행 후 완료시 완료 표시 그래프의 모든 태스크가 수행될 때까지 1단계로 돌아감 이전 태스크를 ‘업스트림’이라고 함 최초 태스크들은 업스트림 의존성이 없기 때문에 바로 실행 대기열에 추가됨 의문점들 Q) 여기서 3번째 단계가 이해가 잘 안가는데, 비순환적 속성을 갖고 있는데 만약 1단계로 돌아간다면 순환적 속성이 되는 것이 아닌지..? 혹시 여기서는 의존관계가 아닌건가? 그래프 VS 절차적 스크립트 파이프라인 파이프라인 설계 방식에는 DAG를 활용한 그래프 방식도 있지만 스크립트를 활용한 linear chain 형태로 실행할 수 있음 이런 단일 스크립트의 파이프라인을 Monolithic(모놀리식) 이라고 함 모놀리식 파이프라인의 문제점 점진적인 태스크로 분리할 수 없음 중간 태스크 실패시 스크립트를 재실행해야하는 문제가 있음 예) 내가 맨날 짜는 파이프라인…. (단일 python script) 그래프 파이프라인의 보완점 실패 태스크 이후부터 재실행할 수 있으므로 효율적 구성 파이프라인을 병렬적으로 수행가능 태스크를 분리할 수 있음 모든 태스크가 의존성으로 다 연결된 게 아닐 수도 있음 특정 태스크들을 시작점이 다르게 시작되었다가 특정 태스크에서 합쳐질 수도 있음 이런 경우 각 태스크들을 병렬적으로 수행하면 효율적임 Airflow Airflow는 Airbnb에서 개발된 오픈소스 workflow management tool Argo, Luigi, Metaflow와 같은 다양한 관리 도구가 있음 Luigi와 Airflow는 파이썬을 활용하기에 확장성이 좋음 여기에 Airflow는 스케쥴링 기능을 추가해서 기능적이 측면에서 뛰어남Python을 사용한 파이프라인 정의 Airflow는 python으로 DAG를 구조화하고 구성함 DAG 파일을 파이썬으로 작성하면 Ariflow가 DAG 코드를 파싱해서 수행함 이 과정에서 schedule interval 같은 메타데이터를 지정할 수 있음 python을 쓴다는 점에서 확장성이 좋다는 장점이 있음파이프라인 스케쥴링 Airflow 구성요소 Airflow scheduler : DAG 분석, 스케줄 파악 후 Airflow worker에 DAG 태스크 예약 Airflow worker : 예약된 태스크 선택 후 실행 Airflow web server : 스케줄러가 분석한 DAG 시각화, DAG 실행과 결과를 확인하는 인터페이스 제공 Airflow Scheduler DAG 파일 분석 후 DAG 태스크, 의존성 및 예약 주기 확인 마지막 DAG까지 확인 후 DAG의 예약 주기를 확인 예약된 태스크에 대해 태스크 의존성 확인 후 처리 1단계로 돌아가고 루프를 대기 Ariflow worker 실행 후 결과를 지속적 추적 Airflow를 사용하는 경우Airflow가 적합한 경우 python을 활용하므로 python을 사용한 방법으로 모든 복잡한 파이프라인 구축이 가능 python 기반이라 확장성이 좋음 스케줄링을 통해 정기적인 실행과 점진적인 처리를 통해 부분 재실행이 가능한 효율적인 파이프라인 구축 파이프라인 결과를 지속적으로 모니터링 가능 백필 기능을 통한 과거 데이터 재처리 백필 : 특정 기간 기준으로 다시 실행하는 기능 반복적이고 배치인 태스크를 실행하는 것에 집중되어짐Airflow가 적합하지 않은 경우 Airflow는 앞서 말한 특징을 요약하면 반복적이거나 배치 태스크를 실행하는 것에 맞춰져 있으므로 실시간 처리 workflow에는 적합하지 않음 추가, 삭제가 태스크가 빈번한 동적 파이프라인은 적합하지 않을 수 있음 웹 인터페이스는 DAG의 최근 실행버전의 정의만 표현하므로 실행되는 동안 구조가 변경되지 않는 것이 좋음 가장 중요한 것은 Python을 쓸 줄 알아야함 파이프라인 규모가 커지면 관리가 어려워질 수 있으므로 초기 사용 시점부터 엄격한 관리가 필요함" }, { "title": "[BoostCamp AI Tech / Final] Day77 - 더미데이터 제작 및 GCP 세팅", "url": "/posts/final2/", "categories": "NAVER BoostCamp AI Tech, Level 3 - Final Project", "tags": "NAVER, BoostCamp, AI Tech, Proeject, Data Engineering, MLOps", "date": "2022-05-11 13:00:00 +0900", "snippet": "Final Project : 더미데이터 제작 및 GCP 세팅목차 Dummy data 제작 GCP 세팅 및 인프라 구축Dummy data 제작 github에서 데이터 수집용 API를 제공해서 그것을 활용해서 데이터를 수집함 모델의 간이 학습 결과 및 데이터 분석을 위해 팀원 사이언티스트 팀에서 더미 데이터를 요청함 awesome-nodejs repository의 readme를 기반으로 repository를 선정함 각 repository의 star user list를 1000명 수집 약 480여개의 repository data와 2~3만여명의 user data를 수집함 데이터를 확장하는데, awesome list에서 Deno, Angular 등 일부 javascript 기반의 repository를 선정하기로 함 현재 github api가 5000/시간 제한이 있어서 제한에 걸리면 저장하고 이후부터 이어서 저장하는 방식을 고려하는 중 Airflow를 통해 자동화를 생각중이지만 일단 태스크 분리부터 해야 할 것으로 보임 무엇보다 airflow가 익숙하지가 않음 GCP 세팅 및 인프라 구축 데이터베이스를 firestore로 구축할 필요성을 크게 못 느껴서 RDB로 선정 GCP 상에 데이터베이스를 올리고 MySQL을 사용하기로 함 과거에 서버에 MySQL 직접 올리다가 보안 문제 발생한 안 좋은 기억이 있어서 Cloud SQL을 사용 매우 편리함 ^^ 현재 팀원들 upstage IP 주소 입력해서 보안 세팅도 끝난 상태 Cloud SQL (MySQL 8) Cloud SQL 설정 자체는 매우 간단함 [GCP] MySQL 환경 구축 블로그를 참고해서 세팅함 보안 IP 설정 부분에서만 조금 다르게 해야함 curl ipconfig.io 위 커맨드를 터미널에 입력해서 ip를 받아온 것으로 해야함 ipconfig나 ifconfig로 나온 inet ip주소는 GCP 연결때 오류가 남 Dummy dataset에 대해서는 MySQL에 업데이트 해놓은 상태 DB 수업을 들은지 좀 지나서 제대로 짰는지는 가물가물하지만… 대략적으로 테이블 구조를 짰음user table user table은 기본적인 구조는 user를 구분하는 uid와 user의 login id인 loginrepository table item 역할을 하는 repository table repository의 고유 id인 rid와 소유자의 user id인 uid와 login을 기본적으로 담고 있음 star_pages는 star한 user list를 api로 받아오기 위해 필요한 정보라 따로 계산 $\\rightarrow$ 문제는 github api는 page를 최대 400 page까지 불러올 수 있음 category_L은 Node.js, Deno, Angular 등 큰 아이템 분류값 (awesome 카테고리가 있는 것으로) category_M은 오히려 backend나 frontend 처럼 직군으로 분류 category_S는 특정 awesome 내부에 분류된 카테고리 (ex. Node.js $\\rightarrow$ Logging) topics와 languages는 각각 리스트랑 dictionary인데, MySQL은 이것들을 저장할 수 있는 type이 없기 때문에 전부 문자열로 변경해서 저장함 이 경우 데이터를 불러오고 eval로 전환할 필요가 있음 일반적으로 데이터베이스를 구축할 때 리스트나 딕셔너리는 어떻게 관리하는지 찾아볼 필요가 있을듯 repo_user table 특정 repository를 star한 user 목록을 저장한 일종의 user와 repository를 연결하는 테이블 star_user_list는 user테이블에 있는 user들의 uid 리스트가 문자열로 저장되어 있음 uid와 rid는 특정 repository의 rid와 소유자의 uid를 갖고 있음" }, { "title": "[BoostCamp AI Tech / Final] Day76 - 프로젝트 개요", "url": "/posts/final1/", "categories": "NAVER BoostCamp AI Tech, Level 3 - Final Project", "tags": "NAVER, BoostCamp, AI Tech, Proeject, Data Engineering, MLOps", "date": "2022-05-10 13:00:00 +0900", "snippet": "Final Project : 프로젝트 개요목차 프로젝트 주제 선정 프로젝트 기본 설계 역할분담프로젝트 주제 선정 깃허브 Repository You may also like 사용자의 직군, 사용언어와 같은 기본 정보를 확인하고 이를 토대로 필요할 것으로 추정되는 repository를 추천 기본적으로 item similarity나 user similarity를 기반으로 추천할 것으로 보임 데이터 선정 과정에서 문제가 있을 것으로 보였으나 github에서 제공하는 API가 생각보다 갖고 있는 정보가 많음 API 데이터가 recursive한 형태를 띄므로 이를 활용하면 더 좋을 것으로 생각됨 따로 페이지를 만드는 것은 재사용성도 떨어지고 가치가 높지 않을 것으로 생각해서 크롬 익스텐션 형태로 개발할 예정 Cold Start 문제를 해결할 필요가 있음 필터링을 기본적으로 하는 것이 좋을듯 프로젝트 기본설계 사용자가 특정 action을 수행하면 chrome extension이 메인 서버로 API를 호출 사용자 정보 initialization API Star 목록 변경 확인 및 업데이트 API Repository Star API Inference API cold start API 데이터베이스틑 따로 구축하는 문제로 Firestore를 고려중 (변경가능) 데이터 파이프라인은 기본적으로 Github API를 기반으로 구축 Airflow로 데이터 수집 자동화 고려중 데이터 구축은 현재 Node.js로 dummy data 형성 Main Server에서 Model Server로 Inference API 호출시 모델에서 Inference 결과를 Item id 리스트로 반환 메인 서버에서 Item id로 DB에서 탐색 후 Object로 Front-End로 전달역할분담 (내가 맡은) 데이터 ETL 파이프라인 구축 및 자동화 Python, Airflow 데이터는 prototype 개발시 Javascript 기반 Node.js, Deno, React, Next.js, Vue.js, Angular, Electron 정도 위주로 awesome 데이터 수집 데이터베이스 설계, 구축 및 관리 Firebase or MySQL Main back-end 개발 GCP, Upstage Server back-end, DB 기술 스택 선정 Inference API : FastAPI API : Node.js " }, { "title": "[BoostCamp AI Tech / Level 2 - DKT] Day67 - Sequence Modelling", "url": "/posts/DKT2/", "categories": "NAVER BoostCamp AI Tech, Level 2 - DKT", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech, Level 2 - DKT", "date": "2022-04-26 14:30:00 +0900", "snippet": "DKT : Sequence ModellingSequence Modelling Sequence Modelling이란 단순한 tabular 데이터를 활용하는 것이 아닌 1개의 행이 어떤 유저나 요소의 시간적 데이터의 일부인 sequence data를 적용하는 모델 sequence data를 tabular 형태로 전환해서 feature engineering과 같은 방식을 통해 학습하는 경우도 많음 하지만 이 과정에서 aggregation을 진행하다보니 누락되는 정보가 많이 발생함 이를 해결하고자 sequnce data를 그대로 사용하는 방식을 사용하는 sequence modelling을 사용 하지만 tabular에서도 데이터 누락을 최소화 하는 방식으로 진행할 수 있음Train/Valid Data split validation loss를 계산하기 위해 train-valid split을 진행해야 함 이 과정에서 어떤 기준으로 분할하는가가 중요함 학습과정에서 test data leakage가 발생한다면 test 성능이 좋게 나오지만 실제 서비스에서 좋지 않은 성능이 나올 수도 있음 DKT의 경우 행을 기준으로 나누는 일반적인 split과 다르게 user별로 묶고 user를 기준으로 split을 진행해야함 DKT같은 정보는 새로운 유저에 좋은 효과를 보이는가?가 핵심 목적이기 때문 Sequential ApproachSequential Model One-to-One : 종가예측과 같은 1개의 sequential을 통해 예측 One-to-Many : 1개의 사진을 넣으면 사진에 걸맞는 설명을 보내주는 모델 Many-to-One : DKT처럼 sequence data를 넣어주면 1/0과 같은 classification이나 종가 예측같은 일정 기간의 정보를 주면 regression 정답을 도출하는 모델 Many-to-Many : 일반적인 NLP에서 자주 사용하는 모델 Seq2Seq : n개의 입력으로 k개의 출력의 형태로 변환해주는 모델, 일반적으로 번역기에 많이 사용Sequential feeding sequential 모델에 데이터를 피딩하는 경우 여러가지 방식을 쓸 수 있음 기본적으로 seqeunce length에 데이터를 맞춰야 함 sequential data는 상황에 따라 그 길이가 다른 경우가 있는데, 이 경우 그 길이들을 잘 맞춰줘야 함 데이터를 feeding하는 경우 모든 데이터가 numeric이고 continuos면 단순히 LSTM에 input_size(feature 개수), hidden_size를 결정해주면 됨 만약 연속형과 범주형이 혼합된 데이터라면 범주형 데이터는 embedding을 해야함 LSTM에 들어가는 input은 continuous feature + embedding size의 형태로 처리하는 경우가 많음 Embedding pytorch는 nn.Embedding을 사용하면 특정 값을 embedding size의 lookup table에 임베딩할 수 있음 nn.Embedding(category_range, embedding_dim)을 사용하면 됨 embedding layer로 학습을 통해 loss를 가장 최적화 시켜주는 방향으로 embedding 값을 조정함" }, { "title": "[BoostCamp AI Tech / P Stage 3] Day66 - Project Day 6", "url": "/posts/day66_pstage3/", "categories": "NAVER BoostCamp AI Tech, P Stage", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech, DKT, Project", "date": "2022-04-25 23:00:00 +0900", "snippet": "P Stage 2 : Project Day 6Day 6 역할분담 EDA역할분담 이전 대회에서 아쉬웠던 점을 회고하던 중 역할분담이 부족했다는 피드백이 있었음 이번에는 기간별, 역할 분담을 진행해서 어느정도 cross-check 방식으로 진행하기로 함 내가 맡은 역할은 EDA, Feature Engineering 이번주부터 분담될 역할은 아마 엔지니어링 파트일 것으로 생각EDA 역할에 맞춰서 EDA 진행 timestamp적인 부분보다 feature 하나하나의 특성을 분석하는 방향으로 진행 분석을 통해 얻은 인사이트 시험지 정보를 담고 있는 데이터인 testID에서 앞부분 3자리는 난이도와 직접적인 영향을 미침 기존의 베이스라인에서는 문제의 정답률을 난이도 feature로 활용했는데, 너무 세분화된 정보보다는 분류기준이 명확한 순서형 자료로 feature engineering 하는 것이 효과적일 것으로 보임 난이도별로 KnowledgeTag의 분포가 특이한 형태를 보이는 것을 확인함 난이도가 낮은 문제일수록 태그의 군집을 이루는 형태를 볼 수 있으며 난이도가 높아질수록 태그가 골고루 분산된 형태를 보임 실제로 난이도 5는 군집이 크게 5개 정도로 형성되지만 난이도 6은 4개의 군집이 형성되는데, 난이도 6이 5보다 정답률이 높음 user별 KnowledgeTag의 정답률을 구분하고 각 태그끼리 정답률의 상관관계로 KnowledgeTag간의 상관성을 분석했음 diagonal element 부근에서 0.5이상의 상관성을 가진 경우가 많이 나타남 이는 KnowledgeTag가 특정 범위끼리 군집을 이루거나 유사한 형태를 띈다는 것으로 해석할 수 있음 일반적으로 문항번호가 뒤로 갈수록 어려운 경향이 많음 (ex 수능) 따라서 assessmentID의 하위 3비트 숫자에 따라 정답률을 확인함 전바적인 경향을 문항이 뒤로 갈수록 정답률이 낮아지는 경향을 보임 " }, { "title": "[BoostCamp AI Tech / Level 2 - DKT] Day62 - DKT 이해 및 DKT Trend", "url": "/posts/DKT1/", "categories": "NAVER BoostCamp AI Tech, Level 2 - DKT", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech, Level 2 - DKT", "date": "2022-04-19 17:00:00 +0900", "snippet": "DKT : DKT 이해 및 DKT TrendDKT란? 최근 연구가 활발히 이루어지고 있는 분야 기존에도 지식 상태를 추적하는 Knowledge Tracing(KT) 연구는 있었음 기존의 KT에 Deep Learning을 적용하는 방식이라고 해서 DKT라는 이름이 붙음Knowledge Tracing Knowledge Tracing이란 교육을 진행하고 테스트를 통해 얻은 지식의 정도를 확인하는 것 예를 들어 사칙연산 퀴즈를 봤을 때 학생이 각 연산의 문제를 얼마나 잘 푸는지? 를 연구하는 것 문제의 정답률은 곧 해당 문제의 해결책에 대한 이해라고 볼 수 있음 지식이라는 것은 축적됨에 따라 상태가 계속 변화하므로 지속적인 추적이 중요함 결국 사용자의 문제 풀이 정보가 추가될수록 지식 상태 예측률이 높아짐 학습한 지식 상태를 기반으로 다음 문제를 맞출지 예측하는 task가 DKT 다음 문제의 정답 여부에 따라 지식 상태를 수정 문제점 문제 유형에 맞춰 어느정도 데이터 양이 확보된다면 충분히 정답률에 대한 신뢰성이 보장 하지만 유형에 대한 데이터 정보가 적으면 정답률의 신뢰성이 떨어짐 (= Overfitting 발생) ex) 빼기 문제 유형이 1개인데 맞출경우 정답률은 100%지만 학생이 빼기를 완전히 이해했다고 보기는 어려움 이런 문제점은 competition이냐 실제 서비스냐에 따라 해결책이 달라짐 competition : 주어진 data이므로 feature engineering이나 skill에 달려있음 실제 서비스 : overfitting 문제는 data의 추가로 해결할 수 있고 문제를 새롭게 다시 정의하는 것도 도움이 됨 DKT의 최종목적은 학생의 지식 이해도를 통해 문제를 추천하고 학업 성취도를 파악하는 것이번 주부터 DKT competition이 시작됩니다. 처음 만나보는 분야라 조금 낯설지만 생각보다 흥미로운 주제입니다. DKT에 대한 코멘트를 남기고 싶지만 강의를 통해 좀 더 공부를 해야 의문점들이 생길 것으로 보입니다.대회 소개대회 목적 특정 유저에 대한 문제에 따른 정답 상태가 주어졌고 sequential한 정답 case를 토대로 마지막 문제의 정답 확률을 예측Metric 이해 대회에서 활용되는 평가 지표는 AUROC와 ACC ACC는 단순 정답률을 의미AUROC AUROC란 Area Under the ROC Curve의 약자로 False Positive와 True Positive의 비율을 보여주는 그래프간단하게 10개 데이터를 통해 만든 그래프 단순 Recall과 Precision, F1-Score과 같은 정보의 한계를 극복하고자 나타난 지표 ROC curve 밑부분의 면적값을 AUROC라고 함 단순 ROC Curve는 thresh hold의 영향이 많이 미치기 때문 결과적으로 이진 분류 문제에서 0과 1의 분포가 얼마나 안 겹치는 지 정도를 보여주는 척도간단하게 100개 데이터를 랜덤으로 생성해 만든 그래프 실제로 랜덤하게 AUROC 그래프를 그리면 대각선과 유사하게 나타남 장점 AUROC는 척도 불변 성질이 있어서 예측이 얼마나 잘 되었는지 평가하기 좋음 분류 임계값(thresh hold)불변 성질때문에 임계값을 무시하고 모델 품질의 예측이 가능 문제점 잘 보정된 확률 결과를 필요로 하는 경우 AUROC는 알 수 없음 imbalance data에 대해서 AUROC는 비교적으로 높게 측정되는 경향이 있음 예를 들어 FPR에 대해서 정보가 많아서 폭이 촘촘한 경우 TPR 증가 비율과 FPR 증가비율이 다르기 때문에 그래프의 넓이에 영향을 줌 DKT History DKT는 task의 특성상 sequential modeling의 특성을 띄고 있음 따라서 NLP의 발전사가 크게 영향을 미침 모델의 자세한 설명이 크게 관심 있지는 않아서 적당히 변화만 짚고 넘어갈 것RNN 기본적인 MLP를 재귀순환 형태로 구조화한 모델 BPTT 방식으로 back propagation 진행 장문장의 경우 멀리 떨어진 단어와의 관계가 소멸되는 short-term memory 문제 발생LSTM, GRU RNN의 문제인 short-term memory 문제를 해결하고자 나타난 모델 기억할 부분들과 불필요한 부분들을 설정하는 LSTM 모델 등장 Forget Gate, Input Gate, Output Gate 총 3개의 Gate를 통해 cell state 내용을 조절함 Gate를 감소한 GRU도 탄생Seq2Seq 다른 언어 도메인끼리의 연결을 만들고자 나타난 아이디어 Encoder와 Decoder로 표현되는 context vector를 통해 서로 다른 언어 도메인을 연결 문장이 길어지는 경우 한 개의 context vector에 표현하는 과정에 한계가 발생Attention Seq2Seq의 문제를 해결하고자 context vector + encoder data를 decoder에 전달하는 attention이 개발 하지만 RNN 방식을 사용하는 sequential 연결 방식의 문제인 학습 속도 문제가 발생Transformer Attention 아이디어를 활용하되 sequential을 제거하는 방식을 도입 문제는 순서 정보를 어떻게 활용해야하는가? 각 단어의 순서 정보는 Positional Encoding을 진행 현재는 Positional Embedding " }, { "title": "[BoostCamp AI Tech / P Stage 2] Day60 - Project Day Final", "url": "/posts/day60_pstage2/", "categories": "NAVER BoostCamp AI Tech, P Stage", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech, Movie Recommendataion, Project", "date": "2022-04-15 23:00:00 +0900", "snippet": "P Stage 2 : Project Day FinalDay Fianl 전반적인 회고전반적인 회고 이번 대회에서 내가 맡은 역할 데이터 분석 : 대회 데이터 EDA 코드 최적화 및 모델 개발 이번 대회의 문제 분석을 가장 중요하고 제대로 했다고 생각했음 결과적으로 보면 섣부른 판단이 오히려 방해를 하는 역할이 되었음 가장 큰 수확은 크게 보면 2가지 정도 데이터 분석의 관점 (거시적, 미시적) 코드 최적화에 대한 고찰과 모델 개발 경험 데이터에 대한 고찰 이번 대회에서 내가 맡은 역할 중 가장 큰 실수라고 생각함 데이터 EDA를 진행하는 데이터 분석가 역할을 맡았는데, 데이터 분석을 꼼꼼하게 하지 않았음 이전 image classification과 다르게 많은 정보가 들어있는 tabular 데이터 였는데 미시적인 관점의 분석을 진행하지 않은 것이 문제 유저의 전반적인 장르 분포나 평가 개수 분포 등을 확인했기만 하고 데이터에서 드러날만 한 디테일한 트렌드 분석이 적었음 예를 들면 유저의 시리즈 영화 선택 빈도 유저별로 장르가 얼마나 치우쳐진 정도를 갖는지 팀원들의 전반적인 의견은 sequential적인 의미는 적고 user-item interaction이 훨씬 중요하다고 생각했다는 것 이런 의견이 나온 이유가 데이터 분석가였던 나의 주장이 강했기 때문인데 이런 점이 실수로 작용했음 실제 대회 종료 후 중요했던 것은 단순 interaction만이 아닌 시리즈 물에 대한 추천 같은 sequential 예측도 중요한 역할을 했음 대회를 통해 깨달은 점 tabular data나 추천 시스템과 관련된 데이터를 분석할 때는 거시적인 관점보다는 미시적인 (유저별 트렌드나 특징) 관점을 자세히 바라볼 필요가 있을 것 같다. 추천은 개인화가 트렌드인만큼 유저별 특징을 잘 살리는 것이 중요했다. 이런 점에서 데이터를 분석할 때 미시적인 관점이 중요한 요소로 작용한 것 같다. 개발적 고찰 팀에서 코드 최적화와 모델 개발 및 구현을 담당했음 추천 모델들은 이전에 진행한 image classification과 다르게 SOTA 모델을 쉽게 불러오기 어려웠다. 모델을 사용하려면 구현된 구현체를 직접 코드로 가져왔어야 하는데, 이 과정에서 파이썬 라이브러리의 버전이 달라서 문제가 발생하는 경우도 많았다. 이런 부분에서 코드 최신화를 진행했음 모델을 가져와서도 수정을 할 부분들이 많았음 모델 자체에서 반복문 활용이나 negative sampling을 진행하는 과정에서 시간이 오래 걸리는 방식으로 되어 있는 경우가 많았음 해당 부분의 코드 최적화를 통해 시간을 단축시키는 것을 진행함 논문을 통한 모델 개발 최종 앙상블에서 주요한 역할을 진행한 EASE 모델의 아이디어와 구현을 진행함 VASP 논문에서 제시한 VAE와 EASE의 hadamard product를 진행 앙상블 코드도 직접 개발해서 후반부에 원활한 앙상블을 진행 다른 팀들이 JIRA나 confluence 같은 프로그램을 사용한 점에서 충격이 강했다. 이번에 기회가 된다면 JIRA, confluence를 활용해보면 좋을 것 같음 또한 wandb sweep을 저번에 시도해보려다가 실패했는데 이번 기회에는 꼭 사용을 해봐야겠다. 다음 대회에 시도해 볼 것들 우선 데이터에 대한 분석을 깊이 있게 진행 할 예정 대회의 전반적인 스토리 라인과 데이터의 스토리 라인을 확실하게 정립할 것 엔지니어링적 측면을 살리고 MLOps 관련된 것들도 시도해 볼 것" }, { "title": "[BoostCamp AI Tech / P Stage 2] Day58 - Project Day 18", "url": "/posts/day58_pstage2/", "categories": "NAVER BoostCamp AI Tech, P Stage", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech, Movie Recommendataion, Project", "date": "2022-04-13 23:00:00 +0900", "snippet": "P Stage 2 : Project Day 18Day 18 list 모델 앙상블모델 앙상블EASE + VAE 대회 막바지에 다다른 만큼 이제 슬슬 모델 앙상블을 진행할 때 가장 괜찮은 아이디어는 VASP 논문에서 제시한 EASE모델과 VAE계열의 Hadamard product를 진행하는 것 기본적인 원리는 EASE가 linear 특성을 학습하고 VAE 계열 모델이 non-linear 특성을 학습하여 linear 특성을 가중치로 non-linear 결과에 앙상블하는 것 실제로 기존 모델의 결과보다 약 100개 정도 더 잘 맞추는 것으로 파악됨Semi-Bagging 정확하게 Bagging이라고 보긴 어려울 거 같지만 일종의 배깅 원리를 사용했기 때문에 Semi-Bagging이라고 했음 모델의 seed를 고정하고 학습을 진행했는데, 이 과정에서 seed 모델별로 학습 결과가 달라질 것이라고 생각했음 그래서 RecVAE 모델의 seed를 변경해서 예측 결과를 모두 더하여 예측에 활용하는 방식을 사용함최종 두 앙상블 방식을 모두 사용해서 기존 0.1601에서 0.1632까지 올렸음 현재 0.18대의 점수가 존재하는데 도저히 뭘 사용했는지 감이 안 옴…." }, { "title": "[BoostCamp AI Tech / P Stage 2] Day56 - Project Day 16", "url": "/posts/day56_pstage2/", "categories": "NAVER BoostCamp AI Tech, P Stage", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech, Movie Recommendataion, Project", "date": "2022-04-11 23:00:00 +0900", "snippet": "P Stage 2 : Project Day 16Day 16 list Multi-VAE 논문 공부 RecVAE loss 변경 제시 VAE의 loss와 sampling 분포 고려Multi-VAE 논문 공부 대회 전반적으로 VAE 계열의 representation learning이 좋은 효과를 보인다는 것은 사실상 확정된 것으로 보임 기본적으로 VAE와 Multi-VAE에 대한 이해가 부족한 것이라 생각해서 Multi-VAE 논문인 ‘Variational Autoencoders for Collaborative Filtering`을 꼼꼼하게 읽어보는 중 자세한 리뷰는 곧 올라올 예정Multi-VAE 논문 리딩에서 깨달은 점 이 논문 읽으면서 얼마나 무지했는지를 깨달았음 가장 처음 깨달은 것은 이름의 Multi의 의미 처음에는 Multi-head attention처럼 여러 개를 의미하는 multi인 줄 알았음 논문을 읽어보니 sampling 분포를 Gaussian, Logistic, Multinomial에서 하는데 이 중 Multinomial 분포를 활용한게 Multi-VAE… $\\log p_\\theta(\\mathbf{x}_u \\mid \\mathbf{z}_u)$ 가 VAE 계열 모델의 loss인 ELBO에서 활용되는데 이 부분을 어떻게 수식적으로 처리하는 지를 몰라서 되게 답답했었음 Multi-VAE 논문에서 smapling 분포에 맞춰서 log likelihood 식이 변경되는 것을 알았음 이를 역으로 생각하면 현재 대회에서 사용하는 Multi-VAE 모델의 smapling 분포나 loss의 형태를 변경해 볼 가치가 있다고 생각함 아직 읽는 중이라 더 읽으면 뭔가 얻을 수 있을듯….RecVAE loss 변경 아이디어 제시 현재 우리 팀의 제일 좋은 성능을 내는 모델인 RecVAE에도 동일하게 ELBO를 활용한 loss function이 존재 충분히 loss를 변경해볼 필요가 있다고 생각해서 일반적으로 BCE (Binary Cross Entorpy)를 쓰는 것을 MSE나 Focal로 변경해보는 것을 제시 MSE로 변경하니까 0.1540에서 0.1601까지 올라감 (약간의 Hyperparametr tuning 진행해서, 평균적으론 0.159 정도로 나옴)VAE의 loss와 sampling 분포 고려 Sampling 분포나 loss를 변경해 볼 필요가 있을 것이라 생각됨 실제로 RecVAE나 Multi-VAE의 sampling 근간이 되는 함수인 reparameterize가 Gaussian 가정으로 적혀 있는 것으로 보임 def reparameterize(self, mu, logvar): if self.training: std = torch.exp(0.5 * logvar) eps = torch.randn_like(std) return eps.mul(std).add_(mu) else: return mu Gaussian 가정이라 MSE loss가 더 좋은 효과를 보이고 있는 것으로 보임 이에 맞춰 sampling distribution을 변경해보는 것이 좋아보임" }, { "title": "[BoostCamp AI Tech / P Stage 2] Day54 - Project Day 14", "url": "/posts/day54_pstage2/", "categories": "NAVER BoostCamp AI Tech, P Stage", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech, Movie Recommendataion, Project", "date": "2022-04-07 23:00:00 +0900", "snippet": "P Stage 2 : Project Day 14Day 14 list Basic MF 기반 모델 훈련Basic MF 기반 모델 훈련 VAE 계열의 모델들을 거의 다 실험해서 여기서 점수를 올릴 수 있는 방법이 크게 보이지 않음 1, 2, 3등팀이 0.16 부근 점수대인데 아마 특정 모델을 사용한 것으로 보임 representation learning이 핵심인 것으로 보여서 다시 처음으로 돌아가는 게 괜찮을 거 같다는 생각도 함 Basic MF 기반으로 예측 모델을 설계하고 학습 진행 MF 자체가 학습이 오래 걸리는데 생각보다 좋은 성능이 나타나지는 않음…" }, { "title": "[BoostCamp AI Tech / P Stage 2] Day53 - Project Day 13", "url": "/posts/day53_pstage2/", "categories": "NAVER BoostCamp AI Tech, P Stage", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech, Movie Recommendataion, Project", "date": "2022-04-06 23:00:00 +0900", "snippet": "P Stage 2 : Project Day 13Day 13 list VASP PyTorch 모델 개발VASP PyTorch 모델 개발 지난번에 발표한 VASP 논문의 PyTorch 구현으로 코드를 개발했음 구현은 했으나 완전히 제대로된 코드라고 보기는 어려워서 더 수정을 진행할 필요가 있어보임 특이한점음 nn.Linear 1개로 구성된 NEASE 모델의 Recall 성능이 상당히 좋다는 점 구현 깃허브구현 review FLVAE 모델이 Focal loss를 사용한 ELBO를 loss로 활용하는데 이 과정에서 사용되는 수식적 구현이 조금 의아한 것이 있음 이활석님께서 VAE에 대해 설명하신 강의가 유튜브에 있는데 참고해서 수식적인 공부랑 VAE 공부를 자세히 할 필요가 있을듯 NEASE는 구현상에 큰 문제가 없는데 논문에서 NEASE와 FLVAE가 각각의 loss로 학습하는 것으로 보이는데 일단 이걸 어떻게 병렬적으로 학습하는가? 그리고 과연 Hadamard Product가 진짜 의미가 있을까?라는 의문 최종 output에 대한 loss는 어떻게 학습해야하는가?" }, { "title": "[BoostCamp AI Tech / P Stage 2] Day51 - Project Day 11", "url": "/posts/day51_pstage2/", "categories": "NAVER BoostCamp AI Tech, P Stage", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech, Movie Recommendataion, Project", "date": "2022-04-04 23:00:00 +0900", "snippet": "P Stage 2 : Project Day 11Day 11 list VASP 논문 발표 RecVAE 학습 pipeline 설계 및 리팩토링시간이 늦어서 좀 간단하게 적고 가겠습니다.VASP 논문 발표 MovieLens에서 좋은 성능을 보이고 있는 VASP 논문 발표를 진행함 준비가 좀 덜 된 상태에서 발표하다보니 약간 아쉬움이 존재했음 좀 더 자세한 내용은 현재 포스팅으로 작성중 발표자표 링크RecVAE 학습 pipeline 설계 및 리팩토링 지난 Multi-VAE와 마찬가지로 학습 파이프라인 구축이 덜 된 모델을 파이프라인 형태로 설계했음 해당 과정에서 일부 pandas 버전이 맞지 않는 부분에 대해서도 리팩토링을 진행함 VAE 계열의 모델은 preprocessing 과정이 동일해서 1개의 전처리 코드로 통일하는 것이 좋아보임 RecVAE에 대한 이해가 없는 상태에서 일단 기존 코드를 재가공했는데, Recall@10이 초반에 확 컸다가 끝으로 갈수록 내려가는 이상한 현상이 나타남 모델의 인코더와 디코더를 학습하는 과정이 존재하는데 이 과정에서 모델 자체가 global로 유지되야 할 것 같음 근데 파이썬은 모델을 인자로 넘겨줘도 유지가 될텐데… 내일 팀원한테 물어보거나 모델 코드 원본이나 논문을 좀 읽어볼 필요가 있을듯 리팩토링 과정에서 코드적인 부분에서 몇가지 의문점이 드는데 간단히 작성해두자torch.zeros VS torch.Tensor 후 data.fill_(0)RecVAE에서 CompositePrior의 구현부에서 원본 코드는 다음과 같이 작성되어 있었습니다.self.mu_prior = nn.Parameter(torch.Tensor(1, latent_dim), requires_grad=False)self.mu_prior.data.fill_(0)self.logvar_prior = nn.Parameter(torch.zeros(1, latent_dim), requires_grad=False)self.logvar_prior.data.fill_(0)self.logvar_uniform_prior = nn.Parameter(torch.Tensor(1, latent_dim), requires_grad=False)self.logvar_uniform_prior.data.fill_(10)이 부분에서 왜 굳이 torch.zeros를 안쓰고 torch.Tensor를 활용하고 0으로 초기화하는지 이해가 가지 않았습니다. 큰 영향을 주진 않을 거 같지만 좀 더 좋은 코드를 위해 아래와 같이 수정했습니다.self.mu_prior = nn.Parameter(torch.zeros(1, latent_dim), requires_grad=False)self.logvar_prior = nn.Parameter(torch.zeros(1, latent_dim), requires_grad=False)self.logvar_uniform_prior = nn.Parameter(torch.Tensor(1, latent_dim), requires_grad=False)self.logvar_uniform_prior.data.fill_(10)반복문을 활용한 코드의 압축은 항상 좋을까?이 부분은 사실 미관상 너무 길고 반복된 코드를 좋아하진 않는 편이라 진행한 코드 수정과정입니다. 우선 원본 코드부터 보여드리겠습니다.class Encoder(nn.Module): def __init__(self, hidden_dim, latent_dim, input_dim, eps=1e-1): super(Encoder, self).__init__() self.fc1 = nn.Linear(input_dim, hidden_dim) self.ln1 = nn.LayerNorm(hidden_dim, eps=eps) self.fc2 = nn.Linear(hidden_dim, hidden_dim) self.ln2 = nn.LayerNorm(hidden_dim, eps=eps) self.fc3 = nn.Linear(hidden_dim, hidden_dim) self.ln3 = nn.LayerNorm(hidden_dim, eps=eps) self.fc4 = nn.Linear(hidden_dim, hidden_dim) self.ln4 = nn.LayerNorm(hidden_dim, eps=eps) self.fc5 = nn.Linear(hidden_dim, hidden_dim) self.ln5 = nn.LayerNorm(hidden_dim, eps=eps) self.fc_mu = nn.Linear(hidden_dim, latent_dim) self.fc_logvar = nn.Linear(hidden_dim, latent_dim)RecVAE에서 Encoder 부분의 레이어 선언부입니다. 입력에 연결되는 self.fc1과 self.ln1을 제외하고 나머지 레이어 세트 (fcn, lnn)은 반복되는 형태를 띄고 있습니다. 따라서 적은 수의 반복문을 돌기 때문에 코드 간소화를 하고자 nn.ModuleDict를 활용하여 다음과 같이 수정했습니다.class Encoder(nn.Module): def __init__(self, hidden_dim, latent_dim, input_dim, eps=1e-1): super(Encoder, self).__init__() self.encoder = nn.ModuleDict() # original code is too long for i in range(0, 5): if i == 0: self.encoder[f&quot;fc{i+1}&quot;] = nn.Linear(input_dim, hidden_dim) self.encoder[f&quot;ln{i+1}&quot;] = nn.LayerNorm(hidden_dim, eps=eps) else: self.encoder[f&quot;fc{i+1}&quot;] = nn.Linear(hidden_dim, hidden_dim) self.encoder[f&quot;ln{i+1}&quot;] = nn.LayerNorm(hidden_dim, eps=eps) self.fc_mu = nn.Linear(hidden_dim, latent_dim) self.fc_logvar = nn.Linear(hidden_dim, latent_dim)근데 이렇게 수정하고 문득 들은 생각은 과연 반복문으로 코드를 압축한게 좋은 코드일까?라는 생각이 들었습니다. 우스갯소리로 ‘1~10까지 출력하는 코드를 짤 때 반복문을 쓰는 것도 좋지만 때로는 그냥 출력문 10개 적는게 나을 수도 있다’는 말도 있는데 이게 이런 상황에서 적용되는게 아닐까라는 생각이 들긴했습니다.막 엄청 성능에 영향을 미치지는 않을 것으로 보이나 과연 다른 사람이 봤을때 이 코드는 원본 코드보다 Encoder module의 구조를 잘 알 수 있을까? 라는 생각이 들기는 합니다.이번주 할 것 (Rough) EASE 모델 코드 확인해보기 AutoRec 구현 및 대회에 적용해보기 NGCF 최적화 작업 시도해보기 앙상블 아이디어 고려하기 모델 관점보다 데이터적인 관점을 좀 더 바라볼 필요가 있을듯 추천 모델을 본격적으로 다뤄보다보니 데이터를 너무 간과하고 있었음 EDA를 처음부터 다시 하나하나 구상해볼 필요가 있을듯 " }, { "title": "[Paper Review] Deep Variational Autoencoder with Shallow Parallel Path for Top-N Recommendation (VASP)", "url": "/posts/VASP/", "categories": "Paper Review, Recommender System", "tags": "NAVER, BoostCamp, AI Tech, VAE, VASP, EASE, paper review, recommender system", "date": "2022-04-04 13:00:00 +0900", "snippet": "Deep Variational Autoencoder with Shallow Parallel Path for Top-N Recommendation (VASP) (2021)논문 소개출처 : Deep Variational Autoencoder with Shallow Parallel Path for Top-N Recommendation (VASP)Deep Variational Autoencoder with Shallow Parallel Path for Top-N Recommendation (VASP)VASP는 2021년 2월에 나온 따끈따끈한 논문입니다. 추천 시스템에서 Benchmark 상위를 차지하고 있는 모델이며 추천 시스템에서 크게 효과를 보인 EASE와 VAE를 활용한 모델입니다. 실제 논문 상에서 엄청 특별한 수식적 접근이나 뛰어난 연구적 접근이 있지는 않다고 생각하지만 2개의 모델을 앙상블해서 좋은 효과를 보였다고 생각됩니다.Abstract EASE 알고리즘은 Top-N 추천 task에서 좋은 성능을 보였고 VAE는 최근 추천 시스템에서 관심도가 높음 이 논문에서는 EASE를 현대의 신경망 기술로 학습하여 성능을 향상시킨 Neural EASE를 identity(여기서는 Input으로 보임)에 과적합되지 않으면서 다중 비선형 레이어의 이점을 보여주는 deep autoencoder인 FLAVE를 제시함 FLAVE와 Neural EASE를 병렬적으로 학습하는 방식을 활용Introduction RS에서는 수백만의 유저와 아이템을 다루는 큰 규모의 데이터를 다루는 기술 이런 상황에서 content는 dynamic하게 변화하고 실시간으로 처리되는 데이터에 대해 충분히 빠르게 학습해야하고 높은 재현률을 보여줄 필요가 있음 EASE는 단순한 선형모델임에도 identity에 과적합하는 문제를 잘 해결하고 explainable한 결과를 제공함 이를 활용해서 복잡한 비선형 패턴을 처리하는 deep autoencoder의 잠재력을 활용하며 EASE처럼 설명가능한 모델을 구축할 것 전통적인 dropout을 통한 과적합 방지는 매우 deep한 구조에서는 효과가 좋지 않음 문제제기 The overfitting towards identity identity에 overfitting되는 문제 발생 $\\rightarrow$ EASE에서 해결 deep architecture에서는 dropout이 overfitting 해결에 큰 도움이 되지 않음 새로운 방식의 data augmentation을 진행해야함 추천 시스템의 근원적인 문제인 niche item의 interaction이 부족한 것을 해결할 필요가 있음문제 해결 방향 Top-N 추천 : focal loss를 활용한 long-tail 문제 방지 과적합 방지 : 간단한 data augmentation 기술 활용 앙상블 : 아다마르 곱(element-wise)을 활용한 여러모델 학습 VASP는 deep VAE와 Neural EASE를 결합하여 함께 학습함 이 과정에서 선형적 특징과 비선형적 특징을 모두 모델링함 Related Work 다양한 관련된 연구들에 대해 장단점을 나열하고 있음 핵심적인 모델들 위주로 정리AutoRec 2016년에 나온 AutoRec은 autoencoder 기반의 CF 모델을 활용하여 좋은 성능을 보였음 이 모델에서는 explicit rating을 활용했음Multi-VAE 2018년에 나온 Variational AutoEncoder (VAE)를 활용한 CF 모델. Multi-VAE라고도 함 Multinomial log-likelihood 데이터 분포를 활용한 모델EASE Embarassingly Shallow Autoencoder for Sparse Data에서 공개한 모델 deep architecture에 반대되는 hidden layer가 없는 Autoencoder 모델 SOTA 모델을 달성한 모델로 유명함Wide &amp;amp; Deep 단순하고 shallow하거나 복잡한 architecture를 갖는 다양한 모델들이 나타남 Wide &amp;amp; Deep이라 불리는 위의 2가지 특징을 조합한 모델이 등장 두 특징을 조합하는 과정에서 joint training이라 불리는 기술을 사용함 Wide &amp;amp; Deep에서 사용한 아이디어를 활용해서 item attribute가 아닌 interaction을 사용해서 비선형 패턴을 찾는 방식을 설계함Other fields CV나 NLP에서 사용하는 딥러닝 기술이 추천 시스템에 적용되어 좋은 성능을 보임 대표적인 예는 Residual Network와 RecVAE가 있음 이런 점에서 Focal Loss (FL) 를 추천 시스템에 적용함 Focal Loss는 object detection에서 나타나는 불균형한 class에 적용하는 loss function Class imbalance는 추천 시스템의 niche item interaction과 동일하게 생각할 수 있음 이런 점에서 CF에서 나타는 cold start problem을 해결할 수 있을 것임 Model Architecture 논문에서 VASP의 구조는 크게 세부분으로 나눠짐 EASE 모델을 사용하는 Neural EASE Variational AutoEncoder를 사용하는 FLVAE 두 모델의 결과를 Hadamard product를 활용해서 합치는 VASP 부분기본 notation\\[\\begin{aligned}&amp;amp; u \\in \\{ 1, ... , U \\} : \\text{user} \\\\&amp;amp; i \\in \\{ 1, ... , I \\} : \\text{item} \\\\&amp;amp; \\mathbf{x}_u = [ x_{u1}, x_{u2}, ..., x_{uI} ]^\\intercal \\in \\mathbb{N}^I : \\text{user u interaction history} \\\\&amp;amp; \\hat{\\mathbf{x}}_u = [ x_{u1}, x_{u2}, ..., x_{uI} ]^\\intercal \\in \\mathbb{N}^I : \\text{user u predicted ratings}\\end{aligned}\\]Neural EASE EASE 논문에 따르면 EASE의 수식은 다음과 같음\\[\\hat{\\mathbf{x}_u} = W \\cdot \\mathbf{x}_u, \\quad W \\in \\mathbf{R}^{|I| \\times |I|}\\] identity overfitting 방지를 위해 가중치 행렬인 $W$의 대각성분은 모두 0으로 제한함 EASE 논문에 따르면 $\\mathbf{x}_u$와 $\\mathbf{x}_u$의 square loss는 closed-form을 띄므로 목적함수로 사용할 수 있음 FLVAE와 함께 학습을 하고 backpropagation을 진행해야 하므로 single layer perceptron으로 EASE를 구현함 layer의 대각 weight 성분은 반드시 0으로 제한하며 bias node는 없음Neural EASE의 loss function 성능표 논문에 따르면 cosine proximity를 사용하는 것이 Neural EASE loss function에서 가장 좋은 것으로 확인됨 PyTorch로 구현하는 경우 torch.nn.functional.cosine_similarity를 사용하면 됨MultiVAE with focal loss (FLVAE)\\[\\begin{matrix}\\mathbf{z}_u \\sim N(0, \\mathbf{I}_k) \\\\\\pi(\\mathbf{z}_u) \\varpropto exp\\{f_{\\theta}(\\mathbf{z}_u)\\} \\\\\\mathbf{x}_u \\sim Mult(N_u, \\pi(\\mathbf{z}_u))\\end{matrix}\\] latent representation $\\mathbf{z}_u$는 평균이 0, 표준편차가 $\\mathbf{I}_k$인 가우시안 분포에서 추출 신경망 $f_{\\theta}(\\cdot)$은 아이템 $I$에 대한 확률분포 $\\pi(\\mathbf{z}_u)$를 유도함 interaction history $\\mathbf{x}_u$는 유도된 $\\pi(\\mathbf{z}_u)$를 활용한 다항분포에서 유도\\[\\log p \\geq \\mathbb{E}_q[ \\log p(\\mathbf{x}_u\\mid\\mathbf{z}_u) - KL(q(\\mathbf{z}_u|\\mathbf{x}_u) \\Vert p(\\mathbf{z}_u)) ]\\] VAE의 목적은 average marginal likelihood인 $p(\\mathbf{z}_u \\mid \\mathbf{x}_u) = \\int p(\\mathbf{x}_u \\mid \\mathbf{z}_u)p(\\mathbf{z}_u)dz $ 를 최대화하는 것이 목적 $f_\\theta (\\cdot)$은 신경망이므로 $p(\\mathbf{z}_u \\mid \\mathbf{x}_u)$ 를 유도하는 것이 어렵기 때문에 위의 식인 ELBO로 근사해서 유도 niche item problem을 해결하고자 focal loss를 활용하는데, 이는 다음과 같이 정의함\\[FL(p_t) = -\\alpha_t(1-p_t)^\\gamma \\log(p_t), \\quad p_t = \\begin{cases}\\hat{x}_{ui} &amp;amp; if \\quad x_{ui} = 1 \\\\ 1 - \\hat{x}_{ui} &amp;amp; otherwise\\end{cases}\\] 이때, focal loss에 사용되는 $\\alpha$와 $\\gamma$는 hyperparameter 이에 따라 ELBO를 다음과 같이 재정의할 수 있음\\[\\log p \\geq \\mathbb{E}_q[ \\alpha_t(1 - p(\\mathbf{x}_u \\mid z_u))^{\\gamma}\\log p(\\mathbf{x}_u\\mid\\mathbf{z}_u) - KL(q(\\mathbf{z}_u|\\mathbf{x}_u) \\Vert p(\\mathbf{z}_u)) ]\\]VASP model $m$에 대해서 $m(\\cdot) : \\mathbf{x}_u \\rightarrow \\hat{\\mathbf{x}}_u$ 로 정의 이때 model $m$의 output에는 sigmoid function 처리를 진행해서 $\\hat{x}_{uI} \\in &amp;lt; 0, 1 &amp;gt; $로 처리 $\\odot$으로 표시하는 Hadamard product를 활용한 joint learning을 제시함\\[m_n(\\mathbf{x}_u) = \\bigodot^{n}_{j=1}m_j = m_1(\\mathbf{x}_u) \\odot m_2(\\mathbf{x}_u) \\odot \\cdots \\odot m_n(\\mathbf{x}_u)\\]\\[m_n(\\mathbf{x}_u) = \\hat{x}_{nu} = \\bigodot_{j=1}^{n} \\hat{x}_{ju} , \\quad \\hat{x}_{nu} \\in \\langle 0, 1 \\rangle\\] 기존의 wide &amp;amp; deep에서 사용한 joint training은 모델의 결과를 합하는 logical OR을 활용하는 반면 VASP에서는 Hadamard product를 사용한 logical AND 방식을 사용해서 모델을 결합함 Hadamard product를 사용하면 2개의 모델이 모두 동의(1에 근접)해야 값에 의미가 존재함 최종적으로 NEASE와 FLVAE를 element-wise 곱으로 나타냄\\[\\begin{matrix}m_{VASP}(\\mathbf{x}_u) &amp;amp; = &amp;amp; m_{FLVAE}(\\mathbf{x}_u) \\odot m_{EASE}(\\mathbf{x}_u) \\\\&amp;amp; = &amp;amp; \\hat{\\mathbf{x}}_{FLVAE_u} \\odot \\hat{\\mathbf{x}}_{EASE_u}\\end{matrix}\\]\\[\\hat{\\mathbf{x}}_{EASE_u} = \\sigma(W \\cdot \\mathbf{x}_u)\\]Data augmentationSplit-Brain Autoencoders: Unsupervised Learning by Cross-Channel Prediction 논문 참조 : Split-Brain Autoencoders: Unsupervised Learning by Cross-Channel Prediction Autoencoder에서 핵심은 입출력의 과적합을 방지하는 것 Split-Brain Autoencoders에 따르면 gray-scale representation을 학습하는 영역과 color-channel을 학습하는 영역으로 분할되어 훈련을 진행함 두 영역은 각각 교차로 예측을 수행하여 원본 사진으로 복구를 함 아래는 논문에서 제시한 예시 사진Split-Brain Autoencoders: Unsupervised Learning by Cross-Channel Prediction VASP에서는 전처리 단계로 자동화된 data augmentation을 사용하는 1개의 신경망을 활용할 것 전체 데이터를 절반으로 나눠서 각자 학습하는 형태로 훈련 앞서 설명한 Split-Brain Autoencoders: Unsupervised Learning by Cross-Channel Prediction의 아이디어를 사용해서 data augmentation을 진행 모든 훈련 epoch마다 input $x_u$를 무작위로 $x_{Au}$와 $x_{Bu}$ 2개의 부분으로 나눔\\[\\begin{aligned}x_{Aui} = \\begin{cases}0 &amp;amp; if \\quad x_{ui} = 0 \\\\1 - x_{Bui} &amp;amp; otherwise\\end{cases} \\\\ x_{Bui} = \\begin{cases}0 &amp;amp; if \\quad x_{ui} = 0 \\\\1 - x_{Aui} &amp;amp; otherwise\\end{cases}\\end{aligned}\\] 학습과정에서 $x_{Aui}$와 $x_{Bui}$는 서로 교차로 입출력으로 학습을 진행함Experimental step 실제 구현때 참고할 실험 환경 데이터셋은 MovieLens 20M과 Netflix Prize를 활용함 전체 데이터의 80%를 학습에 활용하고 20%를 prediction으로 사용 metric은 NDCG@k와 Recall@k로 진행모델 구현 Neural EASE kernel constraint를 활용한 대각행렬을 0으로 제한하는 dense layer를 활용 실험과정에서는 loss function을 3개를 활용함 Neural EASE 참고 data augmentation이 사용되지 않음 Variational AE FLVAE는 encoder와 decoder에 모두 dense하게 연결된 residual network를 활용 output에는 sigmoid function 사용 Data augmentation 사용 focal loss hyperparameter $\\alpha_t$ = 0.25, $\\gamma$ = 2.0을 사용함 VASP EASE와 FLVAE를 hadamard product를 활용해서 결합 hyperparameter는 FLVAE와 동일 Hyperparameters latent space : 2048 hidden layer : 4096 encoder 7 residual hidden layer decoder 5 residual hidden layer epoch : 50 초기 세팅 learning rate : 0.00005 bath size : 1024 이후 learning rate 0.00001로 낮추고 20 epochs 추가 학습 이후 learning rate 0.000001로 낮추고 20 epochs 추가 학습" }, { "title": "[BoostCamp AI Tech / P Stage 2] Day50 - Project Day 10", "url": "/posts/day50_pstage2/", "categories": "NAVER BoostCamp AI Tech, P Stage", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech, Movie Recommendataion, Project", "date": "2022-04-01 19:00:00 +0900", "snippet": "P Stage 2 : Project Day 10Day 10 list inference code 수정Day 6, 7, 8, 9안타깝게도…. 코로나19 양성으로 이번주 일정이 다 날라갔습니다. ㅠㅠ백신 3차를 안 맞아서인지 아니면 그냥 병 자체를 독하게 걸린건지… 감염 5일차인 오늘까지도 큰 회복이 이뤄지진 않고 이번주 내내 잠만 잔 거 같습니다.대회 2주차 마지막인 10일차인 오늘은 좀 컨디션이 좋아져서 팀원들과 회의를 진행했습니다.팀원들 토의에서 핵심적인 요소는 bn.argpartition이었습니다.inference code 수정 다른 팀들은 동일한 Multi-VAE를 썼는데도 0.14대의 Recaal@10을 기록하고 있는 것이 뭔가 이상했음 피어세션 회의에서 bottleneck.argpartition에 대한 의문이 제기됨 np.argsort로 변경해서 inference하는 것을 제안함 수정부분 # 변경 전 idx = bn.argpartition(-output, k, axis=1) # 변경 후 idx = np.argsort(-output, axis=1) 실제로 Recall@10 0.1258 -&amp;gt; 0.1386으로 향상사후분석이 argpartition 때문에 문제가 발생한 것이라 왜 이런 문제가 발생했는가? 를 분석했습니다. np.argsort는 결과를 알고 있어서 bottleneck 공식문서의 argpartition을 확인했고 간단한 10개짜리 np.array로 실험도 진행했습니다. 문제는 두 결과의 차이는 없었고 차이라면 부분 정렬이냐 아니냐 였습니다.문제는 이 사소한 차이때문에 발생했었습니다…;;저희가 수행한 inference 로직은 다음과 같았습니다. Multi-VAE output을 bn.argpartition(-output, 10, axis=1)로 상위 10개 index sort 진행 item을 다시 원본 아이템 번호로 decoding 유저의 이미 시청 아이템이 있으면 제외하고 10개를 추천 리스트에 포함문제는 여기서 1번과 3번 과정의 통합적인 부분에서 발생했습니다. 이해를 돕고자 예시로 설명하겠습니다.만약에 밑에 argpartition으로 처리된 index에서 40과 33 아이템이 user가 interaction한 아이템인 경우 차순위인 17과 30을 처리하게됩니다. 이렇게 될 경우 실제 차순위 아이템인 30과 20이 아닌 30과 17이 들어오므로 Recall에 영향을 주게됩니다.즉 부분 정렬을 해주는 argpartition 특성상 k번째 이후는 score에 따른 순서가 보장되지 못한다는 문제가 있었습니다.따라서 np.argsort로 수정한 후 성능이 향상하는 것을 확인할 수 있었습니다." }, { "title": "[BoostCamp AI Tech / P Stage 2] Day45 - Project Day 5~6", "url": "/posts/day45_pstage2/", "categories": "NAVER BoostCamp AI Tech, P Stage", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech, Movie Recommendataion, Project", "date": "2022-03-26 22:00:00 +0900", "snippet": "P Stage 2 : Project Day 5~6Day 5~6 list NGCF 대회용 코드 작성 NCF 대회용 코드 작성 User-based CF 코드 작성NGCF 대회용 코드 작성 NGCF코드를 대회에 맞춰서 코드를 작성했음 코드 자체는 이전에 공부할때 참고한 코드를 기반으로 잘 파이프라인을 구성했는데, 학습 시간에서 시간이 오래 걸리는 문제가 발생함 Graph Neural Net 자체에 대한 이해가 부족해서 inference 코드를 어떻게 작성할 지 모르겠음 이 부분에 대해서는 NGCF 관련 구현 코드들을 TF는 Torch든 상관없이 더 찾아볼 필요가 있을듯 NGCF에서 Recall@K를 구현하는 부분도 잘 봐야할 거 같음 모델 학습을 위한 데이터셋 구성에서 시간이 오래 걸리는 문제가 발생하므로 데이터셋 구성할 때 더 효율적으로 고려할 수 있는 부분을 확인할 필요가 있음NCF 대회용 코드 작성 Neural Collaborative Filtering 코드를 대회에 맞춰서 작성 문제는 참고한 코드가 user X item 사이즈의 데이터를 활용하므로 학습 과정에서 너무 오래 걸리는 문제가 발생 2048 batch_size를 해도 batch 수가 7만 정도였음….;; NCF 논문을 좀 더 자세히 읽을 필요도 있을듯 함 모든 유저-아이템 조합을 활용하지 않는 방법이 있을까?User-based CF 간단한 classic user-based CF 코드 작성 괜찮은 효율을 기대했으나 클래식 방법이라 큰 효과는 없었음 implicit으로만 데이터를 추천해서 time정보가 활용되지 않았음 time 데이터를 좀 더 활용할 필요가 있을듯이후 해볼 것들 MovieLens 데이터셋에 대해서 EASE와 VASP를 활용한 것에 효과가 좋은 것으로 보임 EASE랑 VASP 논문 리딩 후 코드 구현에 대해서 고민해 볼 필요가 있을듯 Classic MF 논문도 읽고 MF 구현에도 좀 고려를 해 볼 필요가 있을듯 현재 구현에 참고한 MF는 학습시간에 너무 오랜 시간이 걸림 일단 메모리가 크다는 장점이 있어서 user X item matrix를 형성할 수 있음" }, { "title": "[BoostCamp AI Tech / P Stage 2] Day44 - Project Day 4", "url": "/posts/day44_pstage2/", "categories": "NAVER BoostCamp AI Tech, P Stage", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech, Movie Recommendataion, Project", "date": "2022-03-24 22:00:00 +0900", "snippet": "P Stage 2 : Project Day 4Day 4 list Multi-VAE 실험 기준 변경Multi-VAE 실험 기준 변경Multi-VAE로 진행한 실험의 모델 저장 기준이 기존에는 NDCG@100이었는데 대회 목적에 맞는 Recall@10으로 했습니다.public recall@10 score 0.1228로 1등으로 올라갔습니다. 문제는 기존에 제공된 코드기때문에 다른 팀들도 충분히 올라올 수 있는 것들이라 다른 방법도 고려할 필요가 있습니다. 현재 이전에 논문 리뷰한 NGCF 코드를 작성중인데 컨디션 난조로 속도가 더디고 있습니다. 컨디션 관리랑 같이하면서 진행해야할 듯 합니다." }, { "title": "[BoostCamp AI Tech / P Stage 2] Day43 - Project Day 2~3", "url": "/posts/day42_pstage/", "categories": "NAVER BoostCamp AI Tech, P Stage", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech, Movie Recommendataion, Project", "date": "2022-03-23 22:00:00 +0900", "snippet": "P Stage 2 : Project Day 2~3Day 2~3 list 대회 의도 파악 Multi-VAE 실험용 코드 재가공대회 의도 파악대회를 진행하기에 앞서 대회에서 요구하는 걸 정확하게 하는 게 중요하다고 생각했습니다. 대회 자체에서 요구하는 것은 sequential로 나타나는 영화 정보에서 일부 item이 drop된 상황을 가정하여 drop된 영화와 마지막 sequential 영화를 예측하는 것 입니다.그래서 문제 자체의 모델 엔지니어링을 진행하기 이전에 drop된 부분이 있는지 확인하고자 train_ratings.csv를 확인했습니다. train_ratings.csv 자체에는 특별히 null value가 있지는 않았습니다. 여기서부터 살짝 느낌이 이상하더군요.그러면서 동시에 든 생각은 sequential이 의미가 있을까? 였습니다. 이런 생각이 든 이유는 다음과 같습니다. 대회라는 동일 환경에서 공정하게 진행이 되려면 랜덤하게 drop시킨 inference를 진행하는 것은 말이 안된다. 만약 inference 코드를 주고 반드시 이 코드를 활용해서 inference를 해야한다고 했으면 공정성이 성립될 수 있음 -&amp;gt; seed 고정으로 동일한 영화를 drop 근데 train ~ inference까지 모두 우리가 코드를 작성해야하므로 이건 아니라는 결과가 나옴 그렇다는 것은 결국 유저에게 가장 어울리는 상위 10개를 예측하는 문제로 바꿀 수 있다는 것이다. 즉, sequential의 의미가 크게 없어짐 추가적인 문제는 이 데이터는 movie lens를 재가공한 것이라 애초에 영화 시청기록이 아님 이는 시청기록이 몇 초단위로 기록되는 게 말이 되고 결과적으로 실제 유저 행동패턴과 약간은 다를 수 있음 다시 한번 sequnetial이 의미가 없음이 입증되는 부분 팀원들에게 이런 이유를 들어서 sequential이 의미가 없다는 것을 제시했고 sequential보다는 어울리는 아이템 추천으로 시선을 바꿔서 접근하게 되었습니다. 필요에 따라 sequential은 assist 정도로 활용할 수 있을 것으로 보입니다.Multi-VAE 코드 refactoring과제에서 제공해준 Multi-VAE를 실험에 편리한 형태로 코드 refactoring을 진행했습니다. 리팩토링 내용은 다음과 같습니다. 노트북으로 제공된 분리된 class, 함수들 통합 파이프라인 형성 pandas 최신 버전 미적용 문제 최신 버전으로 최적화 최종 Inference 코드 효율성 향상노트북 환경보다 argument를 직접 컨트롤하면서 실험을 진행하는 것이 좋고 이후 hyperparameter tuning에도 좋을 것으로 생각되어 파이프라인 코드를 작성했습니다. 판다스 최신 버전 미적용 문제도 최신 버전에 맞게 설정하여 refactoring했습니다.팀원 분께서 Inference 부분을 잘 짜두셔서 해당 부분은 그대로 사용하였고 마지막에 중복 영화 처리부분만 수정했습니다.깃허브 이슈로 해당부분에 대해 언급하였고 실제 파이프라인 코드에는 적용해서 작성했습니다.&amp;lt; original &amp;gt;index = 0for user in tqdm(user_unique): temp_items = list(pred_dic[user]) for movie in seen_dic[user]: temp_items.remove(movie) top_k_items = temp_items[:10] for i in range(10): test_df.loc[index + i, &#39;item&#39;] = top_k_items[i] index += 10&amp;lt; refactoring &amp;gt;index = 0for user in tqdm(user_unique): temp_items = np.array(list(pred_dic[user])) seen_list = np.array(seen_dic[user]) temp_items = temp_items[np.isin(temp_items, seen_list) == False] top_k_items = temp_items[:10] for i in range(10): test_df.loc[index + i, &#39;item&#39;] = top_k_items[i] index += 10numpy 논리 연산을 활용해서 반복문 활용을 최소화하였고 3분 정도 걸리는 최종 inference time을 1분 안쪽으로 줄이는 효과가 있었습니다." }, { "title": "[BoostCamp AI Tech] Day41", "url": "/posts/day41/", "categories": "NAVER BoostCamp AI Tech, Review", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech", "date": "2022-03-21 23:00:00 +0900", "snippet": "Day41 Review당신은 오늘 하루 어떻게 살았나요? Data EDA오늘 하지 못한 것들 Matrix Factorization Techniques for Recommender Systems 리딩 NGCF 구현내일은 어떤 것을 할까? Matrix Factorization Techniques for Recommender Systems 리딩 NGCF 구현이번주에 할 것들 NGCF 구현 시도 Matrix Factorization Techniques for Recommender Systems word2vec 논문 정리마무리 오늘은 시간이 늦어서 pass" }, { "title": "[BoostCamp AI Tech / P Stage 2] Day41 - Project Day 1", "url": "/posts/pstage_day41/", "categories": "NAVER BoostCamp AI Tech, P Stage", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech, Movie Recommendataion, Project", "date": "2022-03-21 22:00:00 +0900", "snippet": "P Stage 2 : Project Day 1Day 1 list 대회 Data EDA NGCF 적용 대비해서 Graph 변형 코드 작성대회 데이터 EDA이번에도 저번처럼 대회 데이터 EDA를 진행했습니다. 다른 캠퍼분께서 기본적인 EDA를 제공해주셨으나 좀 더 스토리라인을 잡은 EDA를 작성했기 때문에 이번에도 공유를 진행할 생각입니다.NGCF Graph 변형 코드이번 대회에서 Graph Neural Net을 사용할 수도 있을거라 생각했습니다. 그래서 우선 user-items 형태의 text 파일을 생성하는 코드를 작성했습니다.현재 NGCF 구현 코드도 작성 중이라 구현이 완료되는 대로 깃허브 논문 구현에 업로드도 진행할 예정입니다.실제 NGCF를 구현한 코드는 기존의 rating matrix -&amp;gt; graph 과정이 없어서 해당 부분도 같이 업로드할 예정입니다.import osimport pandas as pddef make_user_item(train=True): trains = [&#39;u1.base&#39;, &#39;u2.base&#39;, &#39;u3.base&#39;, &#39;u4.base&#39;, &#39;u5.base&#39;] tests = [&#39;u1.test&#39;, &#39;u2.test&#39;, &#39;u3.test&#39;, &#39;u4.test&#39;, &#39;u5.test&#39;] df = pd.DataFrame() path = &#39;/opt/ml/input/data/lens/&#39; root = &#39;/opt/ml/input/data/lens/ngcf&#39; if train: for train in trains: tmp = pd.read_csv(os.path.join(path, &#39;ml-100k&#39;, train), sep=&#39;\\t&#39;, encoding=&#39;latin-1&#39;, header=None) df = pd.concat([df, tmp], ignore_index=True) df.columns=[&#39;user_id&#39;, &#39;movie_id&#39;, &#39;rating&#39;, &#39;timestamp&#39;] users = df[&#39;user_id&#39;].unique() if &#39;train.txt&#39; in os.listdir(root): t = &#39;w&#39; else: t = &#39;w+&#39; with open(os.path.join(root, &#39;train.txt&#39;), t) as f: save_txt = &#39;&#39; for user in users: save_txt += f&#39;{str(user)} &#39; items = &#39; &#39;.join(map(str, df[df[&#39;user_id&#39;] == user][&#39;movie_id&#39;].values)) save_txt += items save_txt += &#39;\\n&#39; f.write(save_txt) else: for test in tests: tmp = pd.read_csv(os.path.join(path, &#39;ml-100k&#39;, test), sep=&#39;\\t&#39;, encoding=&#39;latin-1&#39;, header=None) df = pd.concat([df, tmp], ignore_index=True) df.columns = [&#39;user_id&#39;, &#39;movie_id&#39;, &#39;rating&#39;, &#39;timestamp&#39;] users = df[&#39;user_id&#39;].unique() if &#39;test.txt&#39; in os.listdir(root): t = &#39;w&#39; else: t = &#39;w+&#39; with open(os.path.join(root, &#39;test.txt&#39;), t) as f: save_txt = &#39;&#39; for user in users: save_txt += f&#39;{str(user)} &#39; items = &#39; &#39;.join(map(str, df[df[&#39;user_id&#39;] == user][&#39;movie_id&#39;].values)) save_txt += items save_txt += &#39;\\n&#39; f.write(save_txt)if __name__ == &#39;__main__&#39;: make_user_item(True) make_user_item(False)" }, { "title": "[BoostCamp AI Tech] Day40", "url": "/posts/day40/", "categories": "NAVER BoostCamp AI Tech, Review", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech", "date": "2022-03-18 23:00:00 +0900", "snippet": "Day40 Review당신은 오늘 하루 어떻게 살았나요? 추천 Basic 10강 정리오늘 하지 못한 것들 Matrix Factorization Techniques for Recommender System 논문 리딩내일은 어떤 것을 할까? Matrix Factorization Techniques for Recommender Systems 리딩 NGCF 구현이번주에 할 것들 NGCF 구현 시도 Matrix Factorization Techniques for Recommender Systems word2vec 논문 정리 [ ] BPR 논문 리딩 및 리뷰 [ ] BPR / ALS도 구현 시도마무리 엔지니어링의 역할을 잘 하는 것이 중요할 것 같다. 팀원들은 학구적인 쪽으로 product serving을 하고 싶어하는 거 같은데 나는 실제 데이터 ETL부터 모델 학습, 서비스 배포까지로 해보고 싶다. 근데 torch contribution이 사실 쉬운 것도 아니고 현실적으로 바라보면 서비스 서빙이 훨씬 나을 수도 있다. product serving 즈음에 좀 강하게 어필을 할 필요가 있을 거 같다. 일단은 다음주 프로젝트를 잘 해보자…" }, { "title": "[BoostCamp AI Tech / RecSys] Day40 - Bandit for Recommendation", "url": "/posts/day40_recsys6/", "categories": "NAVER BoostCamp AI Tech, Level 2 - 추천 시스템 이론", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech, Recommender System, Reinforcement Learning, Bandit, MAB", "date": "2022-03-18 11:00:00 +0900", "snippet": "RecSys : Bandit for RecommendationMulti-Armed Bandit (MAB)One-Armed Bandit 카지노의 슬롯머신을 돌리는 상황 one-armed를 외팔이 강도라고 하는데, 좀 더 이해를 돕고자하면 확률이 일정한 슬롯머신을 돌리는 상황임 예를 들면 동전 던지기 만약 이런 슬롯머신이 K개 존재한다면 어떻게 할까?Multi-Armed Bandit 제한된 횟수 N번동안 K개의 슬롯머신을 돌려서 최대의 이익을 찾는 문제 단, k개의 슬롯머신은 모두 다른 보상과 모두 다른 확률 분포를 갖고 있음 쉽게 말하면 probablistic distribution을 갖는 게임이 k개 있는 것 ex) 동전 던지기, 주사위 굴리기 가 함께 있는 상황 이때 고려할 것은 크게 2가지 어떤 상황에서 어떤 머신을 작동할 지 (policy) 머신들을 어떤 순서로 동작할 지 MAB의 문제와 논점 MAB의 문제는 슬롯머신의 reward 확률을 정확하게 알 수 없다는 것임 확률 분포를 갖는 경우 굉장히 큰 시도를 진행하면 수렴을 할 수 있지만 제한된 횟수만 진행하므로 이를 알기는 어려움 따라서 수행 정책을 세우고 이에 따라 진행해야 함 정책은 방향성으로 구분하면 크게 2가지로 구분 MAB 정책 Exploration (탐색) 더 많은 정보를 얻고자 새로운 arm을 동작 모든 슬롯머신을 비슷하게 당기는 것 탐색에 비용이 많이 들기 때문에 높은 reward를 얻기는 어려움 Exploitation (활용) 슬롯 머신을 몇 번 댕겨보면서 남은 횟수동안은 제일 높은 확률인 것 같은(경험, 관측) 슬롯머신에 몰빵 좋은 정책으로 생각될 수 있으나 잘못된 exploitation이면 오히려 reward가 감소 예를 들어 내가 가장 높다고 생각한 슬롯머신 바로 옆이 더 높은 reward일 수도 있는 것 이런 trade-off 문제가 발생하기 때문에 exploration과 exploitation의 적절한 균형점을 찾아야 함MAB 공식\\[q_{*}(a) \\doteq \\mathbb{E}[R_t \\mid A_t = a]\\] Notations $t$ : time step, play number $A_t$ : $t$에 수행한 action $R_t$ : $t$에 받은 reward $q_*(a)$ : action a에 따른 reward의 실제 기대값 정확한 $q_*(a)$를 아는 게 가장 좋지만 treu distribution을 알기는 어려움 이에 따라 추정치인 $Q_t(a)$를 최대한 비슷하게 구하는 것이 목표 추정을 최대로 하는 action을 선택하는 것을 greedy action이라 함 greedy action : exploitation 새로운 action : exploration MAB Algorithm MBA 알고리즘은 어떻게 action을 수행할 지 policy를 결정함Greedy Algorithm\\[\\begin{aligned}Q_{t}(a) \\doteq \\frac{\\text{sum of rewards when } a \\text{taken prior to } t}{\\text{number of times} a \\text{taken prior to } t} = \\frac{\\sum_{i=1}^{t-1} R_i \\cdot \\mathbf{1}_{A_i=a}}{\\sum_{i=1}^{t-1} \\mathbf{1}_{A_i=a}} \\\\\\\\A_t = \\underset{a}{\\text{argmax}} Q_t(a) \\qquad\\qquad\\qquad\\qquad\\qquad\\end{aligned}\\] 가장 기본적인 연산방식으로 각 머신들의 표본 평균을 활용 가장 간단한 policy로 평균 reward가 최대가 되는 것을 선택 문제점 policy가 처음 선택되는 action과 reward에 큰 영향을 받음 특정 action의 초반 reward가 낮으면 배제될 수도 있음 이런 문제는 exploration이 부족한 단점이 발생할 수 있음 Epsilon-Greedy Algorithm\\[A_t = \\begin{cases}\\underset{a}{\\text{argmax}} Q_t (a) \\quad \\text{with probability } 1 - \\epsilon \\\\\\text{a random action} \\quad \\text{with probability } \\epsilon\\end{cases}\\] greedy algorithm의 단점인 exploration을 해결하는 policy 일정 확률로 슬롯머신을 랜덤으로 선택함 문제점 시행 횟수가 커지면 true distribution에 근접하는데, 항상 $\\epsilon$ 확률의 랜덤 샘플이 나오므로 후반부에는 손해가 날 수도 있음 Upper Confidence Bound (UCB)\\[A_t \\doteq \\underset{a}{\\text{argmax}} \\left[ Q_t(a) + c\\sqrt{\\frac{\\ln t}{N_t(a)}} \\right]\\] notations $t$ : time step, play number $Q_t(a)$ : t시간에 수행한 action a에 대한 reward 추청 (simple average) $N_t(a)$ : action a를 선택한 횟수 $c$ : exploration을 조정하는 하이퍼파라미터 새로 추가된 term은 action의 선택 확률을 조정해줌 action a의 선택 수 $\\downarrow$ $\\Rightarrow$ 분몬 $\\downarrow$ $\\Rightarrow$ 불확실성 $\\uparrow$ $\\Rightarrow$ a에 대한 확률 $\\uparrow$ $\\Rightarrow$ action 선택 확률 $\\uparrow$ 슬롯이 3개인 상황에서 랜덤으로 슬롯머신을 추출하여 추가된 항의 변화 그래프 실제로는 확률이 적용되어 랜덤하게 슬롯이 선택되진 않지만 action의 선택수가 증가하면 실제로 매우 낮은 값으로 수렴하는 것을 볼 수 있음 결국 특정 action a가 많이 선택되면 추가된 항이 0에 수렴하기 때문에 기존의 simple average가 됨결론“Reinforcement Learning: An Introduction – Second edition, in progress”, p48 실제로 UCB가 전반적으로 좋은 성능을 보이나 실제로는 적절한 exploration과 exploitation의 균형을 이루는 것이 중요MAB를 추천에 적용한 경우 서비스 지표인 클릭/구매를 모델의 reward로 가정 reward를 최대화하는 방향으로 학습을 진행 무겁지 않은 간단한 bandit에도 CTR, CVT 지표가 좋아짐 개별 아이템을 action으로 구분 유저에게 아이템을 추천하는 방식을 MAB에서 policy exploration : 변화하는 유저의 취향 탐색, 아이템 후보 확장 exploitation : 유저 취향의 아이템을 추천 사용자의 클릭여부를 MAB의 reward로 사용함문제점과 해결방안 유저 추천 개별 유저별로 처리를 할 경우 데이터 수의 부족으로 bandit(슬롯머신)의 수렴을 하기 어려운 문제가 있음 클러스터링으로 유저를 그룹화하여 그룹별 bandit을 구축함 bandit수 = 유저 클러스터 수 X 후보 아이템 수 아이템 추천 주어진 아이템과 유사한 후보 리스트로 bandit을 적용 유사 추출은 MF, item2vec 등으로 계산 MAB 알고리즘 - 심화Thompson Sampling k개의 action에 해당하는 확률분포를 구하는 문제 각 action 추정치는 $\\beta$ 분포를 따른다는 가정을 함\\[Beta(x \\mid \\alpha, \\beta) = \\frac{1}{B(\\alpha, \\beta)}x^{\\alpha-1}(1-x)^{\\beta - 1}\\] 초기상태는 $Beta(1, 1)$로 세팅 각 배너들을 랜덤하게 노출하면서 초기 세팅을 형성 (그냥 $Beta(1,1)$로 해도 괜찮은듯 함) 이때, 클릭하면 $\\alpha + 1$, 클릭하지 않으면 $\\beta + 1$ 어느정도 분포가 형성이 되면 각 배너의 값에 맞춰 베타분포에서 샘플을 추출 추출된 샘플에서 가장 높은 확률의 배너를 노출 이때, 클릭하면 $\\alpha + 1$, 클릭하지 않으면 $\\beta + 1$ 3~4를 많은 수로 노출 총 9번의 케이스를 1000회 노출을 진행한 후 각 아이템별 베타분포 좀 더 현실적으로 재현하고자 광고 미클릭 확률이 클리보다 높게 설정함fig, axes = plt.subplots(3, 3, figsize=(15, 10))for ax in axes.flatten(): x = [i/100 for i in range(0, 100)] banners = { 1 : [1, 1], 2 : [1, 1], 3 : [1, 1] } cases = { 1 : [], 2 : [], 3 : [] } for i in range(1000): samples = np.array([np.random.beta(a, b) for a, b in banners.values()]) idx = np.argmax(samples) + 1 view = np.random.randint(5) % 2 banners[idx][view] += 1 for k, v in banners.items(): sns.lineplot(x=x, y=sp.beta(banners[k][0], banners[k][1]).pdf(x), ax=ax) ax.set_title(str(banners))plt.tight_layout()plt.show()LinUCB\\[A_t \\doteq \\underset{a}{\\text{argmax}}\\left[ \\mathbf{x}^\\intercal_{t,a}\\theta^*_a + \\alpha\\sqrt{\\mathbf{x}^\\intercal_{t,a} \\mathbf{A}^{-1}_a \\mathbf{x}_{t,a}} \\right], \\quad \\text{where } \\mathbf{A}_a = \\mathbf{D}^\\intercal_a\\mathbf{D}_a + \\mathbf{I}_d\\] $\\mathbf{x}_{t,a}$ : d-차원의 컨텍스트 벡터 user, context feature (성별, 연령 등등… + item의 정보) $\\theta^*_a$ : action a에 대한 d 차원 학습 파라미터 action별로 파라미터를 update $\\mathbf{D}_a$ : t 시점의 m개의 컨텍스트 벡터로 구성된 $m\\times d$행렬 context vector는 유저의 관찰 데이터 행렬 $\\mathbf{D}_i$는 i번째 아이템을 선택한 user $x$의 특징 벡터 파라미터가 업데이트되면 각 파라미터는 해당 아이템을 많이 소비한 특징들의 정보를 갖게됨" }, { "title": "[BoostCamp AI Tech / RecSys] Day39 - DeepCTR", "url": "/posts/day39_recsys5/", "categories": "NAVER BoostCamp AI Tech, Level 2 - 추천 시스템 이론", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech, Recommender System, CTR", "date": "2022-03-17 15:00:00 +0900", "snippet": "RecSys : DeepCTRCTR Prediction 서비스에서 가장 중요하게 바라보는 것은 CTR을 예측하는 것 CTR예측의 정확성은 매출 향상에 직결되는 요소라 광고 서비스에서 자주 활용됨 현실 CTR 데이터의 한계 High Sparsity non-linear association 이런 한계를 해결하고자 딥러닝 모델이 도입됨Wide &amp;amp; DeepWide &amp;amp; Deep Learning for Recommender Systems Paper: Wide &amp;amp; Deep Learning for Recommender Systems Google PlayStore에 활용하기 위한 모델 선형모델과 비선형 모델을 결합하여 서로의 장점을 취한 논문문제 제기 추천 시스템에서는 2개의 해결해야 할 과제가 존재 Memorization : 빈번하게 등장하는 아이템이나 특성 관계를 학습 (자주 나타나는 관계는 암기) Generalization : 드물게 나타나거나 전혀 발생하지 않은 관계도 고려할 필요가 있음 (일반화 문제) Memorization은 전형적인 Logistic Regression문제와 연결이 깊음 모델 자체가 단순하고 수학적 관점이라 해석이 용이함 하지만 Regression이라는 것 자체가 기본적으로 주어진 데이터에서 찾아내는 것들이라 학습 데이터에 없는 feature 조합에 취약하다는 단점이 존재 Generalization은 FM이나 DNN 같은 임베딩 기반 모델이 유리함 sparsity 문제가 발생함 서로의 단점이 보완가능하기 때문에 둘을 결합한 모델을 제안문제 해결 모델Wide &amp;amp; Deep Learning for Recommender Systems 크게 2개 구성으로 이루어져 있음 The Wide Component The Deep Component The Wide ComponentWide &amp;amp; Deep Learning for Recommender Systems wide component는 기본적으로 logistic regression 원리를 갖게됨 핵심은 변수간의 interaction을 구하는 것\\[\\phi_{k}(x) = \\prod_{i=1}^{d} x_{i}^{c_{ki}} \\quad c_{ki} \\in \\{ 0, 1 \\}\\] &amp;lt; Cross-Product Transformation &amp;gt; 모든 cross product를 구하면 파라미터의 수가 기하급수적으로 증가 주요 feature 2개를 쓰는 second-order product만 진행 논문에서는 User Installed App과 Impression App만 사용 기존의 polynomial logistic regression처럼 2개의 feature의 AND interaction을 표현함The Deep ComponentWide &amp;amp; Deep Learning for Recommender Systems &amp;lt; Feed-Forward Neural Network &amp;gt; 3 lyaer로 구성되었고 ReLU 함수 사용 연속형 변수는 그대로, 범주형 변수는 feature embedding 진행전체 구조 및 loss functionWide &amp;amp; Deep Learning for Recommender Systems\\[P(Y=1\\mid \\mathbf{x}) = \\sigma(\\mathbf{w}^\\intercal_{wide}[\\mathbf{x}, \\phi(\\mathbf{x})] + \\mathbf{w}^\\intercal_{deep}a^{(l_f)} + b)\\] Cross Product Transformation에 들어가는 feature는 유저가 설치했던 아이템과 현재 추천하려는 아이템에 대한 정보가 들어감 $\\phi(\\mathbf{x})$는 cross product transformation 결과DeepFMDeepFM: A Factorization-Machine based Neural Network for CTR Prediction Paper: DeepFM: A Factorization-Machine based Neural Network for CTR Prediction wide &amp;amp; deep 모델의 한계인 feature engineering을 진행해야한다는 문제를 해결한 모델문제 제기 추천 시스템은 implicit feature interaction을 학습하는 것이 중요함 기존 모델은 low-order나 high-order 중 한 쪽에만 강한 문제가 있음 즉 feature 그 자체를 쓰거나 통째로 활용하는 방식 이를 해결한 것이 wide &amp;amp; deep model이지만 wide component에서 feature engineering이 필요하다는 한계가 존재 (feature engineering은 효과는 좋지만 어렵고 시간이 오래 걸리는 문제가 있음) 이를 해결하고자 기본 아이디어가 second-order인 Factorization Machine을 wide component로 활용문제 해결 모델DeepFM: A Factorization-Machine based Neural Network for CTR Prediction 모델은 크게 2개의 구조로 구성 FM Component Deep Component FM ComponentDeepFM: A Factorization-Machine based Neural Network for CTR Prediction low-order feature interaction을 처리하는 역할을 기존의 FM과 동일한 구조를 활용함 FM의 특성상 second-order feature interaction 처리에 효과적임Deep ComponentDeepFM: A Factorization-Machine based Neural Network for CTR Prediction high-order feature interaction을 처리하는 역할로 DNN을 활용 모든 feature는 동일한 차원(k)으로 임베딩 (논문에서는 5차원) 임베딩에 사용되는 가중치는 FM과 동일한 가중치($V_{ij}$)와 동일 실제 전체 모델 구조를 보면 알 수 있듯이 임베딩을 각 Component가 공유하는 것을 알 수 있음 다른 모델과의 비교DeepFM: A Factorization-Machine based Neural Network for CTR Prediction FNN은 **Factorization Machine의 학습이 필요하다는 문제가 존재 PNN은 feature interaction 처리는 가능하지만 low-order, 즉 feature 단일의 특징을 처리하는 데에 문제가 있음 Wide &amp;amp; Deep은 앞서 말했듯이 feature engineering이 필요하다는 문제가 존재Deep Interest Network (DIN)Deep Interest Network for Click-Through Rate Prediction Paper: Deep Interest Network for Click-Through Rate Prediction user behavior feature를 처음으로 사용한 논문문제 제기 기본 DL기반 모델은 모두 유사한 embedding과 MLP 아이디어를 사용함 결국 목표는 sparse data embedding후 fully-connected training임 이런 방식을 사용하는 경우 사용자의 행동으로부터 나오는 여러가지 관심사 반영이 어려움 예를 들어 어떤 유저는 특정 카테고리를 보다가 추천 목록 상품을 클릭할 수도 있음 이는 사용자의 context를 반영해야 함 결국 사용자의 소비 목록들이라는 user behavior feature를 생성하여 소비 아이템과 예측 대상의 관련성을 학습할 필요가 있음문제 해결 모델Deep Interest Network for Click-Through Rate Prediction 모델은 크게 3개의 구조를 가짐 Embedding Layer Local Activation Layer Fully-connected Layer 다른 부분은 다 비슷하고 핵심은 Local Activation LayerLocal Activation LayerDeep Interest Network for Click-Through Rate Prediction user behavior에서 1개의 광고를 뽑아서 후보군이 되는 광고와 Activation Unit에서 연관성을 계산 Activation Unit을 통과하면 activation weight가 나오는데, 이는 현재와 과거의 소비 아이템의 연관성을 나타내는 가중치 Weighted Sum Pooling : 구해진 가중치를 각 representation vector에 곱하고 합을 통해 연산함 activation unit은 후보군이 되는 광고에 따라 유저의 과거 소비 제품의 weight가 달라짐Behavior Sequence Transformer (BST)Behavior Sequence Transformer for E-commerce Recommendation in Alibaba Paper: Behavior Sequence Transformer for E-commerce Recommendation in Alibaba Transformer를 사용한 CTR 예측 논문 DIN에서 local activate unit이 transformer의 attention 같은 역할을 하는 데에서 착안논문 아이디어 CTR 예측과 NLP 사이에는 공통점이 존재함 대부분 sparse data 아이템이건 단어건 실제로 하나의 케이스에 있는 데이터는 전체 데이터에 비해 적음 low, high-order feature interaction이 모두 존재 문장도 순서가 중요하듯 사용자도 행동 순서가 중요한 역할을 하기도 함 사람들은 핸드폰을 사고 그에 맞는 핸드폰 케이스를 삼 (반대의 경우는 매우 드묾) 이런 점을 토대로 NLP에서 좋은 성능을 보이는 transformer를 적용하는 아이디어를 제시모델 구조Behavior Sequence Transformer for E-commerce Recommendation in Alibaba transformer의 encoder 부분만을 가져와서 user behavior의 가중치를 파악Transformer Encoder Layer\\[\\begin{aligned}&amp;amp;\\mathbf{F} = \\text{FFN}(\\mathbf{S}) \\\\ &amp;amp;\\mathbf{S}&#39; = \\text{LayerNorm}(\\mathbf{S} + \\text{Dropout}(\\text{MH}(\\mathbf{S}))) \\\\&amp;amp;\\mathbf{F} = \\text{LayerNorm}(\\mathbf{S}&#39; + \\text{Dropout}(\\text{LeakyReLU}(\\mathbf{S}&#39;\\mathbf{W}^{(1)} + b^{(1)})\\mathbf{W}^{(2)})) \\\\\\\\&amp;amp;\\mathbf{S}^{(i+1)} = \\text{MH}(\\mathbf{F}^{(i)}) \\\\&amp;amp;\\mathbf{F}^{(i+1)} = \\text{FFN}(\\mathbf{S}^{(i+1)})\\end{aligned}\\] transformer encoder 부분에서는 일반적인 transformer의 처리 방식과 동일한 수식이 적용 기존 transformer와 다른 점은 dropout과 leaky relu를 적용했다는 점에서 차이가 존재 transformer block은 1~4개를 사용한 결과 1개 층을 사용하는 것이 더 좋은 효과를 보임 아마도 NLP 보다 sequence 복잡성이 낮기 때문으로 생각 attention의 positional encoding은 기존의 $\\cos$나 $\\sin$을 쓰지 않고 물리적인 시간 차이인 $t(v_t) - t(v_i)$를 사용함" }, { "title": "[BoostCamp AI Tech] Day38", "url": "/posts/day38/", "categories": "NAVER BoostCamp AI Tech, Review", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech", "date": "2022-03-16 23:00:00 +0900", "snippet": "Day38 Review당신은 오늘 하루 어떻게 살았나요? 추천 Basic 8강 정리 Efficient Estimation of Word Representations in Vector Space오늘 하지 못한 것들 Matrix Factorization Techniques for Recommender System 논문 리딩내일은 어떤 것을 할까? 8-9강 정리 Matrix Factorization Techniques for Recommender Systems 리딩 NGCF 구현이번주에 할 것들 NGCF 구현 시도 Matrix Factorization Techniques for Recommender Systems BPR 논문 리딩 및 리뷰 BPR / ALS도 구현 시도마무리 멘토님께서 여러가지 조언을 해주신 것이 좋았다. 우리 조에 대해서 칭찬을 해주셨는데 요즘 같이 취준을 하는 시기에 심적으로 불안정하고 힘든데 큰 힘이 되는 것 같다. 개인적으로 내가 하고자 하는 것에는 열정적이고 최대한 배우고 정보를 알려고 노력하는데 이게 멘토님께서 인상이 깊으셨던 것 같았다. 이제 4학년인데 어떻게 그렇게 준비가 잘 되어있는지, 내가 생각한 추천 시스템의 트렌드를 깊게 공감하셨다고 하셨다. 그래도 내가 꽤 열심히 준비했다는 게 뿌듯했고 인정 받는 느낌이었다. 사실 살아가다보면 자신이 하고 있는 방향이 맞는지 의심이 들 때가 있는데 최근에 많이 그랬다. 근데 멘토님께서 충분히 좋은 방향으로 가고 있다는 말씀을 해주셔서 내 방향성에 확신이 조금은 들은 것 같다." }, { "title": "[BoostCamp AI Tech / RecSys] Day38 - Context-aware Recommendation(GBM)", "url": "/posts/day38_recsys4/", "categories": "NAVER BoostCamp AI Tech, Level 2 - 추천 시스템 이론", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech, Recommender System, GBM, Ensemble", "date": "2022-03-16 17:00:00 +0900", "snippet": "RecSys : Context-aware Recommendation(GBM)Gradient Boosting Machine (GBM)Greedy Function Approximation: A Gradient Boosting Machine Paper: Greedy Function Approximation: A Gradient Boosting Machine문제 제기 CTR 예측으로 개인화 추천 시스템을 위한 모델 특별한 문제를 제기했다기보단 기존의 앙상블을 진행했다는 것이 의미가 있음 실시간 서비스는 feature가 자주 변화하므로 하이퍼파라미터에 robust한 모델이 필요Boosting 앙상블 기법의 일종 핵심 아이디어는 의사결정 나무 (decision tree)로 된 weak learner들을 연속적으로 학습하고 결합하는 방식 weak learner는 정확도와 복잡도는 낮지만 그만큼 간단한 learner 이전의 weak learner가 취약한 부분 위주로 샘플링, 가중치 부여를 통해 부족한 부분 위주로 다음 learner가 학습 AdaBoost, Gradient Boost Machine(GBM), XGBoost, LightGBM, CatBoost, …GBM 학습Greedy Function Approximation: A Gradient Boosting Machine gradient descent로 loss function 학습 기존의 gradient descent와 다르게 파라미터가 아닌 weak learner 자체를 gradient항으로 연산\\[\\arg\\min_{\\mathbf{a}, \\beta}\\sum_{i=1}^{N}\\left[ \\hat{y}_i - \\beta h(\\mathbf{x}_i ; \\mathbf{a}) \\right]^2\\] 통계학적으로 보면 Gradient Boosting은 잔차(residual) 를 활용해서 학습을 진행함 이전 단계의 weak learner까지 residual을 활용해서 다음 learner를 학습 회귀 문제는 예측과정에서 residual을 사용, 분류에서는 $\\log(odds)$를 사용 각 decision tree는 분기점의 기준이 다름 loss 값이 threshold 이하로 내려가거나, leaf node에 속하는 데이터 수가 적어지면 학습 종료GBM 특징 장점 Bagging 원리를 쓰는 random forest보다 좋은 성능을 보임 단점 학습 속도가 느림 overfitting 발생 가능 " }, { "title": "[BoostCamp AI Tech / RecSys] Day38 - Context-aware Recommendation(FM, FFM)", "url": "/posts/day38_recsys3/", "categories": "NAVER BoostCamp AI Tech, Level 2 - 추천 시스템 이론", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech, Recommender System, FM, FFM", "date": "2022-03-16 17:00:00 +0900", "snippet": "RecSys : Context-aware Recommendation(FM, FFM)What is Context-aware Recommendation문제점과 해결책 기존의 추천 시스템 방식 유저 관련 정보 아이템 간련 정보 유저-아이템 상호작용 기존의 추천 방식은 유저, 아이템의 ID만 사용하는 문제가 있음 특히 이런 것은 MF 계열의 CF에서 자주 나타남 상호작용의 2차원 행렬 표현 방식 유저의 지리, 나이, 성별 특징, 아이템의 카테고리, 태그 등을 넣기에 무리가 있음 상호작용을 넣었지만 그 맥락적인 상호작용 정보가 적음이라는 문제 때문에 cold start 문제가 발생 이를 해결하고자 컨텍스트 기반 추천 시스템 방법이 제시됨 유저-아이템 상호작용 + 맥락 정보를 함께 반영하는 방법 Why Context-aware Recommendataion? 실제 서비스에서는 metric 자체보다는 CTR(Click-Through Rate)가 더 현실적인 지표 기업의 최종 목표는 유저의 CTR 예측이 핵심 CTR 예측 방식 CTR은 결국 클릭의 여부인데, 일반적으로 이진분류로 많이 함 하지만 실제 이진 분류보다는 유저의 클릭 확률을 예측하는 것이 더 효율적 실제 광고 예측과 같은 유저의 클릭 환경에는 다양한 맥락이 존재함 따라서 실제 예측 과정에서 유저 ID는 필요가 없는 경우가 많음How to CTR Prediction?\\[\\text{Logistic Regression} : logit (P(y = 1 \\mid x)) = \\left( w_0 + \\sum_{i=1}^n w_i x_i \\right), \\quad w_i \\in \\mathbb{R}\\] 대표적으로 많이 쓰는 로지스틱 회귀 모형 여러 feature의 상호작용을 표현하지 못하므로 CTR 예측에 사용하기에는 무리가 있음 feature 단일의 영향력을 보기에는 충분함\\[\\text{Polynimial} : logit(P(y=1 \\mid x)) = \\left( w_0 + \\sum_{i=1}^n w_i x_i + \\sum_{i=1}^n \\sum_{j=i+1}^n w_{ij} x_i x_j \\right), \\quad w_i , w_{ij} \\in \\mathbb{R}\\] 변수 간의 상호작용을 표현한 로지스틱 회귀 모형 문제는 $x_i$, $x_j$의 수에 맞춰 $w_{ij}$가 나타나는데, 결국 파라미터 수는 $N^2$이 되므로 파라미터 수가 급격하게 증가하는 단점이 존재데이터의 종류 Dense Feature 벡터로 표현하는 경우 작은 공간에 밀집되는 수치형 변수 연속형 변수는 그 자체가 의미를 갖는 경우가 많으므로 수치로 표현해도 충분함 Sparse Feature 벡터로 표현하는 경우 넓은 공간에 있는 범주형 변수 범주형 변수도 수치로 임베딩할 수 있지만 이는 수학적으로 의미를 가질 위험이 있음 이를 해결하고자 대부분 원핫 인코딩형태로 표현 원핫 인코딩 특징상 0으로 표현되는 데이터가 더 많음 CTR Prediction에서는 대부분 sparse fature가 많음 Sparse Feature는 말했듯이 아이템의 종류 = feature 수가 되기 때문에 파라미터 수가 과도하게 많아짐 이를 해결하고자 Item2Vec, LDA, BERT 등을 활용해서 feature 임베딩을 진행함Factorization Machine (FM)Factorization Machines Paper: [IEEE 2010] Factorizaion Machines SVM과 Factorization Model의 장점을 결합한 모델문제 제기 딥러닝이 등장하기 전에는 Support Vector Machine(SVM) 을 가장 많이 사용함 SVM의 강점인 커널 공간으로 비선형 데이터 셋에서 강점이 나타남 SVM의 문제점은… CF환경인 Sparse Data 환경이 많은데, SVM은 sparse field에서 취약한 단점을 가짐 그런데 CF에 유리한 MF의 문제점은… X: (user, item) $\\rightarrow$ Y(rating) 같은 특수 환경에서만 가능함 비선형 데이터의 처리가 어려움 문제 해결 아이디어\\[\\hat{y}(x) = \\underbrace{w_0 + \\sum_{i=1}^n w_i x_i}_{\\text{Logistic term}} + \\underbrace{\\sum_{i=1}^{n}\\sum_{j=i+1}^{n} \\langle v_i, v_j \\rangle x_i x_j}_{\\text{Factorization term}}\\]\\[w_0 \\in \\mathbb{R}, \\quad w_i \\in \\mathbb{R}, \\quad v_i \\in \\mathbb{R}^k\\] 논문에서 제시한 모델의 수식은 크게 2개의 항으로 나눠짐 logistic term은 단일 feature 정보를 학습 Factorization term은 k차원의 표현과 같은 역할을 함 Factorization term은 polynomial logistic의 아이디어를 가져왔으나 식이 일부 다른데, weight 역할을 하는 $w_{ij}$가 아닌 dot product를 사용해서 k차원에 대한 더 일반화된 수식을 적용함 k차원의 경우 factorization dimension인데, 이는 hyperparamter 원문 : A row $\\mathbf{v}_i$ within $\\mathbf{V}$ describes the $i$-th variable with $k$ factors. $k \\in \\mathbb{N}_0^+$ is a hyperparameter that defines the dimensionality of the factorization. Sparse 데이터 활용 예측\\[(\\text{user}_1, \\text{movie}_2, 5) \\rightarrow [ \\underbrace{1, 0, 0, 0, 0}_{\\text{user one-hot}}, \\underbrace{0, 1, 0, 0, 0}_{\\text{movie one-hot}} ]\\] 평점 데이터 구조는 {(유저 ID, 영화 ID, 평점), …} 일반적인 CF 입력과 같음 평점 데이터를 일반 입력으로 변화하면 원핫 인코딩 표현으로 변형 (유저 5, 영화 5 인 상황으로 가정) 결국 입력 값의 차원은 유저 수 + 아이템 수 유저 A의 영화 ST에 대한 평점을 예측하는 경우 아래와 같은 방식으로 학습Factorization Machines $V_{ST}$의 경우 유저 B와 C가 해당 영화를 봤기 때문에 그 유저들이 본 다른 영화 정보를 활용하여 학습 $V_A$의 경우 A가 본 영화인 SW를 통해 B, C에 대한 정보를 학습 그 외에도 뒤쪽에 context를 추가해서 상호작용 정보를 추가로 학습할 수 있음FM의 장점 SVM에 비해서… sparse data에 취약한 단점을 극복 선형 복잡도 $O(kn)$을 갖기 때문에 큰 규모의 데이터도 빠르게 학습함 대신 파라미터 수가 선형적으로 비례함 MF에 비해서… 다양한 예측 문제에 사용가능해서 기존에 CF환경에만 가능한 단점을 극복 아이템 ID를 제외한 다른 부가 context를 추가로 학습 가능 Field-aware Factorization Machine (FFM)Fiel-aware Factorization Machine (FFM) Paper : Field-aware Factorization Machine 기존 FM의 변형을 가한 모델문제 제기 FM이 예측 문제라는 범용성을 확대한 상황에서 CTR 예측에 좋은 성능을 보임 단, FM의 문제는 사용하는 field의 수가 2개였기 때문에 이를 해결하는 3차원 field 모델인 Pairwise Interaction Tensor Factorization(PITF)가 등장함 PITF는 (user, item, tage)의 3차원 field 예측을 시도 (user, item), (user, tag), (item, tag) 쌍을 구성해서 서로 다른 latent factor를 정의 PITF에서 아이디어를 착안하여 여러 field의 latent factor를 제시문제 해결 아이디어\\[\\hat{y}(x) = w_0 + \\sum_{i=1}^n w_i x_i + \\underbrace{\\sum_{i=1}^{n}\\sum_{j=i+1}^{n} \\langle v_{i,f_j}, v_{j, f_i} \\rangle x_i x_j}_{\\text{Factorization term}}\\]\\[w_0 \\in \\mathbb{R}, \\quad w_i \\in \\mathbb{R}, \\quad v_{i, f} \\in \\mathbb{R}^k\\] 입력 변수를 필도별로 나누고 필드별로 latent factor를 갖게 factorize 진행 field는 모델 설계시 정의 같은 의미를 갖는 변수의 집합 유저 : 성별, 디바이스, OS 아이템 : 광고, 카테고리 context : 어플리케이션, 배너 feature 수만큼 필드를 정의할 수 있음 Factorization term에서 $V_{i, f}$는 $x_i$이 있는 쌍에서 남은 반대 field의 factorize parameter Clicked Publisher(P) Advertiser(A) Gender(G) Yes ESPN Nike Male 위의 상황인 경우 필드는 P, A, G로 정의 1개의 변수에 대해 필드의 수($f$)와 factorization 차원($k$)의 곱만큼 학습 여기서 factorization dimension은 hyperparameter 원문 : A row $\\mathbf{v}_i$ within $\\mathbf{V}$ describes the $i$-th variable with $k$ factors. $k \\in \\mathbb{N}_0^+$ is a hyperparameter that defines the dimensionality of the factorization. FFM field 구성 범주형 변수 FM\\[\\begin{aligned}&amp;amp;\\text{label \\; feat:val1 \\; fat2:val2 } \\cdots, \\\\&amp;amp;\\rightarrow\\qquad \\text{YES \\; P-ESPN:1 \\; A-Nike:1 \\; G-Male:1 }\\end{aligned}\\] FFM\\[\\begin{aligned}&amp;amp;\\text{label \\; field1:feat:val1 \\; field2:fat2:val2 } \\cdots, \\\\&amp;amp;\\rightarrow\\qquad \\text{YES \\; P:P-ESPN:1 \\; A:A-Nike:1 \\; G:G-Male:1 }\\end{aligned}\\] 수치형 변수 기존 변수의 encoding과 좀 반대로 수치형 변수를 mapping하는게 복잡하다 dummy field numeric feature 1개 당 한의 필드를 할당 수치와 필드가 1:1 매핑 (필드명은 큰 의미가 없음) 예시) $\\text{ Yes \\; AR:AR:45.73 \\; Hidx:Hidx:2 \\; Cite:Cite:3}$ discretize numeric feature를 n개의 구간으로 나누어 사용 논문에서는 소수를 반올림해서 정수로 변환하여 해당 정수 값을 카테고리 변수처럼 다룸 예시) $\\text{ Yes \\; AR:45:1 \\; Hidx:2:1 \\; Cite:3:1}$ " }, { "title": "[BoostCamp AI Tech] Day36", "url": "/posts/day36/", "categories": "NAVER BoostCamp AI Tech, Review", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech", "date": "2022-03-14 23:00:00 +0900", "snippet": "Day36 Review당신은 오늘 하루 어떻게 살았나요? 추천 Basic 7강 정리 NGCF 논문 리뷰오늘 하지 못한 것들 Collaborative Filtering for Implicit Feedback Datasets 논문 리딩내일은 어떤 것을 할까? Efficient Estimation of Word Representations in Vector Space Matrix Factorization Techniques for Recommender System이번주에 할 것들 NGCF 구현 시도 Collaborative Filtering for Implicit Feedback Datasets BPR 논문 리딩 및 리뷰 BPR / ALS도 구현 시도마무리 일단…. 시간이 늦어서 나중에" }, { "title": "[BoostCamp AI Tech / RecSys] Day36 - RecSys with RNN", "url": "/posts/day36_recsys2/", "categories": "NAVER BoostCamp AI Tech, Level 2 - 추천 시스템 이론", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech, Recommender System, RNN", "date": "2022-03-14 17:00:00 +0900", "snippet": "RecSys : Recommender System with RNNRNN과 추천 시스템 Session based 추천에서 자주 사용되는 아이디어 session은 일련의 과정에 따라서 유저이 현재 선호하는 정보를 처리해야 할 필요가 있음 GRU4RecSession-based Recommendations with Recurrent Neural Networks 고객이 현재 원하는 상품을 찾는 것이 목적이며 RNN을 추천 시스템에 적용한 논문GRU4Rec 아이디어와 구조Session-based Recommendations with Recurrent Neural Networks session sequence를 GRU 입력으로 넣어서 sequence의 다음에 나올 확률이 높은 아이템을 추천하는 것 모델의 구조는 다음과 같음 입력 one-hot encodeing session 논문에서는 임베딩을 시도하기도 안하기도 했으나 성능은 임베딩이 없는 것이 더 좋았다고 함 GRU 레이어 기존 RNN처럼 sequence 아이템 맥락 학습 출력 다음 골라질 아이템에 대한 선호도 스코어 GRU4Rec 학습 과정Session-based Recommendations with Recurrent Neural Networks 세션의 길이는 일정하지 않고 짧은 경우와 긴경우가 존재함 세션의 특징상 짧은 경우가 많지만 길이가 긴 경우도 있음 짧은 길이의 세션을 사용해서 학습할 정보가 많아지면 효율이 떨어지므로 짧은 세션을 긴 세션에 연결하는 병렬적 세션으로 미니 배치 학습을 진행함 실제로 아이템이 너무 많다는 것이 문제가 됨 이를 해결하고자 negative sampling 방식을 도입해서 일부 후보군에서 추천 목록을 형성 negative sample 기준은 인기도로 처리 만약 아이템의 인기도가 높은데도 상호작용이 없다면 관심이 없는 아이템이라 가정 사용자가 interaction 하지 않은 경우에서 인기도 기반으로 negative sampling을 진행 " }, { "title": "[BoostCamp AI Tech / RecSys] Day36 - RecSys with GNN", "url": "/posts/day36_recsys1/", "categories": "NAVER BoostCamp AI Tech, Level 2 - 추천 시스템 이론", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech, Recommender System, GNN", "date": "2022-03-14 15:00:00 +0900", "snippet": "RecSys : Recommender System with Graph Neural NetNeural Graph Collaborative Filtering의 자세한 내용은 Paper Review 카테고리에 있습니다.Graph Neural NetworkGraph 점(node)과 간선(edge)으로 구성된 자료구조 일반적으로 $G = (V, E)$로 정의Naive Graph Neural Net Neural Network에 그래프가 도입된 이유는 크게 2가지 정보간의 관계, 상호작용같은 추상적 개념 표현에 적합 상호작용으로 table로 표현하면 너무 복잡하다는 단점이 있음 추천 시스템에서 유저-아이템 관계를 표현하기에 적합함 Non-Euclidean Space 표현이 가능 이미지, 텍스트, tabular는 대부분이 격자 공간에 투영 가능한 euclidean space 데이터 SNS데이터, 상호작용, 분자구조는 격자 공간에 투영할 수 없는 non-euclidean space 그래프를 인접행렬 방식으로 표현하여 하나의 노드정보를 MLP에 학습하는 naive approach 방법으로 시작함 문제점 노드의 수가 많아지면 인접행렬 그래프 표현의 한계인 연산량 증가문제가 발생함 노드의 수만큼 MLP 노드가 추가되므로 모델의 차원 수가 증가하고 결국 모델의 복잡도로 연결됨 인접행렬 특성상 표현하는 순서에 따라 의미가 달라지는 경우가 발생 Graph Convolutional NetworkPaper : [ICLR 2017] Semi-Supervised Classification with Graph Convolutional Networks Naive 방식의 문제점을 극복하고자 convolution 연산을 적용한 Graph Convolution Network가 등장 Local Connectivity : 모든 노드를 확인하는 것이 아닌 특정 노드의 주변부를 대상으로 convolution 연산을 진행 Shared Weight : convolution 연산 효과인 주변 노드들과 weight를 공유 multi-layer : convolution을 여러층 쌓으면 떨어져 있는 정보를 확보할 수 있음 연산량을 효과적으로 줄이면서 간접적인 관계 파악이 가능 Neural Graph Collaborative Filtering출처 : ACM2019 Neural Graph Collaborative Filtering [ACM 2019] Neural Graph Collaborative Filtering 자세한 논문 내용은 논문 리뷰 포스트에 있습니다. 참고 바랍니다.문제 제기 논문은 기존 CF의 연산 방식의 핵심을 다음과 같이 정의 유저-아이템 임베딩 : latent factor 상호작용의 모델링 : $P^\\intercal Q$ (dot product를 통한 선형연산) 기존 CF에는 상호작용 표현의 한계가 존재함 기존 CF의 문제를 interaction function ($P^\\intercal Q$)에만 의존하므로 sub-optimal(누락된 부분 정보)한 임베딩을 사용한다고 제시문제 해결 아이디어Neural Graph Collaborative Filtering 논문에서 계속 언급하는 Collaborative Signal은 유저-아이템 상호작용(소비관계)을 의미 좌측은 기존 CF방식의 상호작용 표현방식인데, 이 방법은 유저와 아이템 수가 늘어날수록 연결되지 않은 다른 item의 간접적인 상호작용 표현에 한계가 존재함 High-order Connectivity를 사용하면 직/간접적 상호작용을 표시할 수 있음 GNN을 사용하면 user-item-user 연결관계로 따로 유사도를 계산하지 않아도 user간의 연결관계로 유사도를 파악할 수 있음 Graph로 관계를 표현하면 $u_1$에 대한 아이템의 추천 강도도 계산할 수 있음 $i_4$는 $i_4 \\rightarrow u_2 \\rightarrow i_2 \\rightarrow u_1$ path와 $i_4 \\rightarrow u_3 \\rightarrow i_3 \\rightarrow u_1$ path, 총 2가지 경로로 $u_1$에게 전달됨 $i_5$가 $u_1$에게 전달되는 경로는 1개밖에 없으므로 추천 강도는 $i_4 &amp;gt; i_5$가 됨 NGCF 구조Neural Graph Collaborative Filtering NGCF 모델 구조는 크게 3개의 레이어로 구성됨 Embedding Layer one-hot encoding의 유저-아이템 초기 임베딩을 k 차원 임베딩으로 변환 Embedding Propagation Layer NGCF에서 가장 중요한 레이어 유저-아이템 연결관계를 갖는 high-order connectivity를 학습하는 레이어 Prediction Layer 아이템, 유저 각 전파 레이어에서 전달된 임베딩을 concat해서 결과 score를 연산 Embedding Layer\\[\\mathbf{E} = [ \\; \\underbrace{\\mathbf{e}_{u_1}, \\cdots, \\mathbf{e}_{u_N}}_{\\text{users embeddings}} , \\underbrace{\\mathbf{e}_{i_1}, \\cdots, \\mathbf{e}_{i_M}}_{\\text{item embeddings}} \\;]\\] embedding을 생성하는 레이어 임베딩 방식은 기존의 CF방식 (MF, Neural CF)과 동일하지만 기존 CF는 이걸 바로 interaction function에 입력함 NGCF는 GNN에 임베딩을 전파하여 refine Collaborative Signal(상호작용)을 직접적으로 레이어에 전달하는 과정 Embedding Propagation Layer First-order Propagation collaborative signal을 전달할 message를 구성하고 결합하는 단계 \\[\\mathbf{m}_{u\\leftarrow i} = f(\\mathbf{e}_i, \\mathbf{e}_u, p_{ui}) = \\frac{1}{\\sqrt{ \\left\\vert\\mathcal{N}_{u}\\right\\vert \\left\\vert\\mathcal{N}_{i}\\right\\vert }}\\left( \\mathbf{W}_1\\mathbf{e}_i + \\mathbf{W}_2 (\\mathbf{e}_i \\odot \\mathbf{e}_u) \\right)\\] Message Construction 단계는 유저-아이템간의 affinity(밀접관계)를 표현하는 message를 형성함 $p_{ui}$는 연결 item이 과도하게 많을 경우를 방지하는 decay factor 주변 이웃 노드 수($\\left\\vert \\mathcal{N} \\right\\vert$)로 나눠서 normalize함 \\[\\mathbf{e}_{u}^{(1)} = \\text{LeakyReLU}\\left( \\mathbf{m}_{u \\leftarrow u} + \\sum_{i \\in \\mathcal{N}_{u}} \\mathbf{m}_{u \\leftarrow i} \\right)\\] Message Aggregation 단계는 $u$의 이웃 노드들에서 전파된 message를 결합하여 해당 유저의 1단(1 layer) 임베딩을 형성함 High-order Propagation Neural Graph Collaborative Filtering $l$개의 embedding propagation layer를 쌓으면 유저노드는 $l$ 거리만큼 떨어진 이웃의 메시지를 사용할 수 있음 이런 이웃들을 $l$-hop neighbor라고 함 \\[\\mathbf{e}_{u}^{(l)} = \\text{LeakyReLU}\\left( \\mathbf{m}^{(l)}_{u \\leftarrow u} + \\sum_{i \\in \\mathcal{N}_{u}} \\mathbf{m}^{(l)}_{u \\leftarrow i} \\right)\\]\\[\\begin{cases}\\mathbf{m}^{(l)}_{u\\leftarrow i} = p_{ui} \\left( \\mathbf{W}^{(l)}_1\\mathbf{e}^{(l-1)}_i + \\mathbf{W}^{(l)}_2 (\\mathbf{e}^{(l-1)}_i \\odot \\mathbf{e}^{(l-1)}_u) \\right) \\\\\\mathbf{m}^{(l)}_{u\\leftarrow u} = \\mathbf{W}^{(l)}_1\\mathbf{e}^{(l-1)}_u\\end{cases}\\] Prediction Layer\\[\\begin{aligned}\\mathbf{e}^{*}_u = \\mathbf{e}^{(0)}_u \\Vert \\cdots \\Vert \\mathbf{e}^{(L)}_u , \\quad \\mathbf{e}^{*}_i = \\mathbf{e}^{(0)}_i \\Vert \\cdots \\Vert \\mathbf{e}^{(L)}_i \\\\\\\\\\hat{y}_\\text{NGCF}(u, i) = {\\mathbf{e}^{*}_u}^\\intercal\\mathbf{e}^{*}_i \\qquad\\quad\\quad\\end{aligned}\\] L번째 layer까지 임베딩 벡터를 형성했으면 user, item 별로 각각 concatenate하여 최종 임베딩 벡터를 생성함 그리고 각 임베딩 벡터를 내적해서 최종 선호도를 계산LightGCN GCN의 핵심부분을 가져오고 일부 수정하여 더 가벼운 형태를 제시LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation 기존 NGCF에서 모든 embedding에 대해 weight parameter를 적용했으나 LightGCN에서는 이웃 노드의 임베딩을 가중합 처리 layer가 깊을수록 멀리있는 정보이므로 강도가 약할 것이라는 아이디어를 사용함\\[\\mathbf{e}^{(k+1)}_u = \\sum_{i\\in\\mathcal{N}_u}\\frac{1}{\\sqrt{\\lvert \\mathcal{N}_u \\rvert}\\sqrt{\\lvert \\mathcal{N}_i \\rvert}}\\mathbf{e}^{(k)}_i\\] 주변에 연결되어 있는 노드만 사용하므로 self-connection 항이 제거됨 실제 학습 파라미터는 0번째 임베딩 레이어에만 존재함 one-hot encoding이 embedding으로 표현되는 부분에서만 weight가 존재함\\[\\mathbf{e}_u = \\sum_{k=0}^{K}\\alpha_k\\mathbf{e}^{(k)}_u\\] 최종 예측에서도 k층 레이어의 임베딩에 가중치 $\\alpha$를 곱해서 가중합 처리를 함 이때, $\\alpha_k = (k+1)^{-1}$로 설정해서 레이어가 깊을수록 가중치가 0에 가까워져서 영향력을 낮추게함" }, { "title": "[Paper Review] Neural Graph Collaborative Filtering (2019)", "url": "/posts/ngcf/", "categories": "Paper Review, Recommender System", "tags": "NAVER, BoostCamp, AI Tech, GNN, NGCF, paper review, recommender system", "date": "2022-03-14 13:00:00 +0900", "snippet": "Neural Graph Collaborative Filtering (2019)논문 소개출처 : ACM2019 Neural Graph Collaborative Filtering[ACM 2019] Neural Graph Collaborative FilteringNGCF는 GNN의 최초 등장 논문은 아닙니다. 이전에 이미 GNN에 대한 개념이 존재했었고 어느정도 연구도 있었습니다. 하지만 추천 시스템에 적용하여 효과적인 성능을 보여줬으며 이후 GNN을 활용한 추천 시스템 연구에 도화선 역할을 하게 되었습니다.이후 LightGCN 등 여러가지 GNN을 활용한 추천 시스템 연구들이 등장하였습니다. 이번 포스팅에서는 지난 AlexNet과는 다르게 핵심적인 NGCF이론 위주로 설명하겠습니다.지난번에 알렉스넷 해석처럼 리뷰했다가 반나절 걸려서 그렇게는 못 하겠습니다…Abstract 최신 추천 시스템 연구는 item과 유저의 벡터 표현 (임베딩)을 학습하는 방식을 활용함 MF나 딥러닝이 적용된 방식은 ID나 attribute 같은 유저(아이템)의 속성을 활용해서 임베딩을 했음 이런 임베딩 방식은 유저-아이템 상호작용인 collaborative signal을 임베딩 과정에서 인코딩 하지 않는다는 문제가 존재함 이 논문에서는 이분 그래프 구조의 user-item 상호작용을 임베딩 과정에 포함하는 방법을 제시함 user-item graph 구조를 임베딩 전파를 통해 전달하는 NGCF를 개발 이때, user-item graph는 high-order connectivity를 사용해서 표현하고 전달함Introduction문제제기 논문은 기존 CF의 연산 방식의 핵심을 다음과 같이 정의 유저-아이템 임베딩 : latent factor 상호작용의 모델링 : $P^\\intercal Q$ (dot product를 통한 선형연산) 기존 CF에는 상호작용 표현의 한계가 존재함 이런 한계가 존재하는 이유는 기존 CF (MF, NCF)의 연산 순서때문 $\\mathbf{e}_u = P$, $\\mathbf{e}_i = Q$ 로 유저와 아이템을 임베딩하고 두 임베딩 matrix의 inner product인 $P^\\intercal Q$ 순서로 연산 이 과정을 보면 각각 임베딩 후 상호작용을 연산하므로 유저-아이템의 상호작용 자체를 전달하지 못하는 한계가 발생함 (latent 형태로 연산하므로 정보의 누락 발생) 이런 기존 CF의 문제를 interaction function ($P^\\intercal Q$)에만 의존하므로 sub-optimal(누락된 부분 정보)한 임베딩을 사용한다고 제시문제 해결 아이디어Neural Graph Collaborative Filtering 논문에서 계속 언급하는 Collaborative Signal은 유저-아이템 상호작용(소비관계)을 의미 좌측은 기존 CF방식의 상호작용 표현방식인데, 이 방법은 유저와 아이템 수가 늘어날수록 연결되지 않은 다른 item의 간접적인 상호작용 표현에 한계가 존재함 GNN을 활용하여 유저-아이템 상호작용은 임베딩 부분에서 처리하는 방식을 제시 High-order Connectivity를 사용하면 직/간접적 상호작용을 표시할 수 있음 GNN을 사용하면 user-item-user 연결관계로 따로 유사도를 계산하지 않아도 user간의 연결관계로 유사도를 파악할 수 있음 Graph로 관계를 표현하면 $u_1$에 대한 아이템의 추천 강도도 계산할 수 있음 $i_4$는 $i_4 \\rightarrow u_2 \\rightarrow i_2 \\rightarrow u_1$ path와 $i_4 \\rightarrow u_3 \\rightarrow i_3 \\rightarrow u_1$ path, 총 2가지 경로로 $u_1$에게 전달됨 $i_5$가 $u_1$에게 전달되는 경로는 1개밖에 없으므로 추천 강도는 $i_4 &amp;gt; i_5$가 됨 MethodologyNGCF 구조Neural Graph Collaborative Filtering NGCF 모델 구조는 크게 3개의 레이어로 구성됨 Embedding Layer one-hot encoding의 유저-아이템 초기 임베딩을 k 차원 임베딩으로 변환 Embedding Propagation Layer NGCF에서 가장 중요한 레이어 유저-아이템 연결관계를 갖는 high-order connectivity를 학습하는 레이어 Prediction Layer 아이템, 유저 각 전파 레이어에서 전달된 임베딩을 concat해서 결과 score를 연산 Embedding Layer\\[\\mathbf{E} = [ \\; \\underbrace{\\mathbf{e}_{u_1}, \\cdots, \\mathbf{e}_{u_N}}_{\\text{users embeddings}} , \\underbrace{\\mathbf{e}_{i_1}, \\cdots, \\mathbf{e}_{i_M}}_{\\text{item embeddings}} \\;]\\] embedding을 생성하는 레이어 임베딩 방식은 기존의 CF방식 (MF, Neural CF)과 동일하지만 기존 CF는 이걸 바로 interaction function에 입력함 NGCF는 GNN에 임베딩을 전파하여 refine Collaborative Signal(상호작용)을 직접적으로 레이어에 전달하는 과정 Embedding Propagation LayerFirst-order Propagation collaborative signal을 전달할 message를 구성하고 결합하는 단계\\[\\mathbf{m}_{u\\leftarrow i} = f(\\mathbf{e}_i, \\mathbf{e}_u, p_{ui}) = \\frac{1}{\\sqrt{ \\left\\vert\\mathcal{N}_{u}\\right\\vert \\left\\vert\\mathcal{N}_{i}\\right\\vert }}\\left( \\mathbf{W}_1\\mathbf{e}_i + \\mathbf{W}_2 (\\mathbf{e}_i \\odot \\mathbf{e}_u) \\right)\\] Message Construction 단계는 유저-아이템간의 affinity(밀접관계)를 표현하는 message를 형성함 $\\mathbf{e}_i \\odot \\mathbf{e}_u$에서 $\\odot$은 element-wise 연산인데, element-wise 연산 특성상 두 벡터 연결관계와 강도를 잘 표현함 (interaction term) $p_{ui}$는 연결 item이 과도하게 많을 경우를 방지하는 decay factor 주변 이웃 노드 수($\\left\\vert \\mathcal{N} \\right\\vert$)로 나눠서 normalize함 \\[\\mathbf{e}_{u}^{(1)} = \\text{LeakyReLU}\\left( \\mathbf{m}_{u \\leftarrow u} + \\sum_{i \\in \\mathcal{N}_{u}} \\mathbf{m}_{u \\leftarrow i} \\right)\\] Message Aggregation 단계는 $u$의 이웃 노드들에서 전파된 message를 결합하여 해당 유저의 1단(1 layer) 임베딩을 형성함 $\\text{LeakyReLU}$를 사용한 이유는 collaborative signal에서 긍정과 부정 정보를 모두 인코딩해야 하기 때문 정보의 손실을 방지하고자 self-connection term ($\\mathbf{m}_{u \\leftarrow u}$)를 추가함 이걸 추가하는 이유는 GCN에서 그래프를 표현하는 방식때문임 왜 self-connection term이 필요한가? self-connection항이 들어가는 이유는 본 논문에서 제시하는 그래프 표현 방식때문입니다. 일반적으로 그래프를 표현할 때 앞서 말한 것처럼 인접행렬의 형태를 사용하려고 합니다. 아래 그래프로 예를 들어봅시다. \\[\\text{Adjacency matrix}(\\mathbf{A}) = \\begin{bmatrix}0 &amp;amp; 1 &amp;amp; 0 \\\\1 &amp;amp; 0 &amp;amp; 1 \\\\0 &amp;amp; 1 &amp;amp; 0\\end{bmatrix} \\quad \\text{Degree matrix}(\\mathbf{D}) = \\begin{bmatrix}1 &amp;amp; 0 &amp;amp; 0 \\\\0 &amp;amp; 2 &amp;amp; 0 \\\\0 &amp;amp; 0 &amp;amp; 1\\end{bmatrix}\\] 문제는 단순히 인접행렬만으로 표현하면 특정 노드에 인접한 노드의 개수(차수)를 $O(1)$시간에 알기에 무리가 있습니다. 그래서 그래프 표현에 Degree matrix를 추가해서 표현합니다. 문제는 두 행렬이 따로 존재하면 여러가지 의미로 효율성이 떨어집니다. 그래서 이 두 행렬의 정보가 서로 공존하게 만드는 Laplacian Matrix (라플라시안 행렬) 을 만듭니다. (wikipedia : laplacian matrix)\\[\\mathcal{L} = \\text{Degree matrix} - \\text{Adjacency matrix} = \\begin{bmatrix}1 &amp;amp; -1 &amp;amp; 0 \\\\-1 &amp;amp; 2 &amp;amp; -1 \\\\0 &amp;amp; -1 &amp;amp; 1\\end{bmatrix}\\] 이렇게 만들어진 라플라시안 행렬은 일반적으로 대각성분이 모두 1인 Normalized Laplacian Matrix로 변환합니다.\\[\\mathcal{L}^{\\text{norm}} = \\mathbf{D}^{-\\frac{1}{2}}\\mathcal{L}\\mathbf{D}^{-\\frac{1}{2}} = \\begin{bmatrix}1 &amp;amp; -\\frac{1}{\\sqrt{2}} &amp;amp; 0 \\\\-\\frac{1}{\\sqrt{2}} &amp;amp; 1 &amp;amp; -\\frac{1}{\\sqrt{2}} \\\\0 &amp;amp; -\\frac{1}{\\sqrt{2}} &amp;amp; 1\\end{bmatrix}\\] 그래서 원래는 대각성분이 모두 1로 존재하기 때문에 self-connection이 필요하지 않습니다. (아마도요….)근데 본 논문에서는 모델의 계산 편의성을 위해 대각원소를 모두 0으로 처리한 matrix로 정의했습니다. 그래서 self-connection term이 필요합니다.\\[\\mathcal{L}^{\\text{norm}} = \\mathbf{D}^{-\\frac{1}{2}}\\mathbf{A}\\mathbf{D}^{-\\frac{1}{2}} \\text{ and } \\mathbf{A} = \\begin{bmatrix}\\mathbf{0} &amp;amp; \\mathbf{R} \\\\\\mathbf{R}^\\intercal&amp;amp; \\mathbf{0}\\end{bmatrix} \\quad (\\text{Paper&#39;s Laplacian})\\] 여기서 $\\mathbf{R}$은 user-item intercation matrix 입니다. High-order PropagationNeural Graph Collaborative Filtering 1개의 레이어는 1개의 order를 처리함 따라서 $l$개의 embedding propagation layer를 쌓으면 유저노드는 $l$ 거리만큼 떨어진 이웃의 메시지를 사용할 수 있음 이런 이웃들을 $l$-hop neighbor라고 함 수식적으로 $l$ layer에서 생성된 message는 아래와 같이 나타냄\\[\\mathbf{e}_{u}^{(l)} = \\text{LeakyReLU}\\left( \\mathbf{m}^{(l)}_{u \\leftarrow u} + \\sum_{i \\in \\mathcal{N}_{u}} \\mathbf{m}^{(l)}_{u \\leftarrow i} \\right)\\]\\[\\begin{cases}\\mathbf{m}^{(l)}_{u\\leftarrow i} = p_{ui} \\left( \\mathbf{W}^{(l)}_1\\mathbf{e}^{(l-1)}_i + \\mathbf{W}^{(l)}_2 (\\mathbf{e}^{(l-1)}_i \\odot \\mathbf{e}^{(l-1)}_u) \\right) \\\\\\mathbf{m}^{(l)}_{u\\leftarrow u} = \\mathbf{W}^{(l)}_1\\mathbf{e}^{(l-1)}_u\\end{cases}\\] 최종적으로 위의 식의 계산은 아래 수식을 통해 구현되어 실제 처리됨\\[\\mathbf{E}^{(l)} = \\text{LeakyReLU}\\left( (\\mathcal{L} + \\mathbf{I})\\mathbf{E}^{(l-1)}\\mathbf{W}^{(l)}_1 + \\mathcal{L}\\mathbf{E}^{(l-1)} \\odot\\mathbf{E}^{(l-1)}\\mathbf{W}^{(l)}_2 \\right)\\] 솔직히 굉장히 뭐가 많이 튀어나와서 정신이 없을텐데 여기서 살짝 첨언을 하자면 도대체 저 $\\mathcal{L}$은 어디서 갑자기 튀어 나온건가..?하면 $\\mathbf{e}^{(l)}_u$ 식을 자세히 보면 내부에 크게 2개의 항으로 나뉜다. $\\mathbf{m}^{(l)}_{u \\leftarrow u}$ : self-connection 이 항을 계산할 때는 자기 자신 노드에 대한 연산만 하기 때문에 굳이 user-item interaction을 생각할 필요가 없다. 그래서 그냥 identity matrix를 곱한다. $\\mathbf{m}^{(l)}_{u\\leftarrow u}$ : user-item interaction 문제는 이 항에서는 user-item interaction을 고려해야한다. 따라서 앞에서 user-item 관계를 담고 있는 그래프 정보인 $\\mathcal{L}$을 곱해주는 것이다. 이해가 잘 안되면 $\\mathbf{E}^{(l)}$랑 $\\mathbf{e}^{(l)}_u$를 그냥 다 전개해서 나란히 두고 비교해보면 어느정도 느낌이 날 것이다. 별로 안 복잡하다. Prediction Layer\\[\\begin{aligned}\\mathbf{e}^{*}_u = \\mathbf{e}^{(0)}_u \\Vert \\cdots \\Vert \\mathbf{e}^{(L)}_u , \\quad \\mathbf{e}^{*}_i = \\mathbf{e}^{(0)}_i \\Vert \\cdots \\Vert \\mathbf{e}^{(L)}_i \\\\\\\\\\hat{y}_\\text{NGCF}(u, i) = {\\mathbf{e}^{*}_u}^\\intercal\\mathbf{e}^{*}_i \\qquad\\quad\\quad\\end{aligned}\\] L번째 layer까지 임베딩 벡터를 형성했으면 user, item 별로 각각 concatenate하여 최종 임베딩 벡터를 생성함 그리고 각 임베딩 벡터를 내적해서 최종 선호도를 계산Optimization\\[\\textit{Loss} = \\sum_{(u, i, j)\\in \\mathcal{O}} -\\ln\\sigma(\\hat{y}_{ui} - \\hat{y}_{uj}) + \\lambda \\lVert \\Theta \\rVert^2_2 , \\quad \\Theta = \\{ \\mathbf{E}, \\{ \\mathbf{W}^{(l)_1}, \\mathbf{W}^{(l)}_2 \\}^L_{l=1} \\}\\] loss function은 이전에 BPR에서 사용된 BPR loss를 사용 BPR loss의 수식구조에 따라 $\\mathcal{O} = { (u, i, j) \\mid (u, i) \\in \\mathcal{R}^+, (u, j) \\in \\mathcal{R}^- }$이 되는데, $\\mathcal{R}^+$는 관측한 상호관계이고 $\\mathcal{R}^-$는 비관측 상호관계를 의미 activation function은 sigmoid $\\lambda$는 $L_2$ regularization으로 오버피팅 방지 optimizer는 Mini-batch Adam 사용Dropout Message Dropout layer에서 생성된 message 중 일부를 dropout user와 item의 단일 연결관계의 존재 여부에 더 강한 robustness를 임베딩해주는 효과 Node Dropout 특정 노드를 임의로 차단하고 모든 출력 message를 차단 $l$번째 전파 레이어에서 라플라시안 행렬의 노드들 중 임부를 임의로 drop 특정 user-item 관계에 대한 영향을 줄여줌 Conclusion Embedding propagation layer가 많을수록 성능이 좋아짐 다만 너무 많으면 오버피팅 발생하므로 3~4개 층이 가장 최적 성능 Embedding propagation으로 representation을 좀 더 정확하게 해줬기 때문에 수렴속도와 recall에서 좋은 효과를 보임 또한 유저-아이템 임베딩 공간이 더 명확하게 구분됨" }, { "title": "[BoostCamp AI Tech] Day35", "url": "/posts/day35/", "categories": "NAVER BoostCamp AI Tech, Review", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech", "date": "2022-03-12 23:00:00 +0900", "snippet": "Day35 Review당신은 오늘 하루 어떻게 살았나요? 추천 Basic 5-6강 정리 버킷플레이스 코테오늘 하지 못한 것들 없음내일은 어떤 것을 할까? Neural Graph CF 관련 논문 리딩 및 세미나 준비 (3/13 완료) 코세라 MLOps Specialization 2주차 정리 (3/13 완료)이번주에 할 것들 추천 시스템 basic 내용 정리 Neural Graph CF 관련 논문 리딩 및 세미나 준비 (3/13 완료) 삶의 지도 그려보기 + 변성윤 마스터님 두러두런 1회차 질문에 답해보기 (3/13 완료) 코세라 MLOps Specialization 2주차 정리 추천 시스템 기본과제마무리 정신없는 금토가 지나갔다. 토요일에 버킷플레이스 상반기 공채 코테를 봤는데 진짜 처참했다. 아쉬움도 많았고…. 다음주부터 1일 1알고를 해야할듯하다. 부캠 글 정리도 어느정도 많이 진행되었다. 이걸 좀 책자처럼 하나의 안내 팜플렛처럼 깃허브에 남겨두면 좋을 것 같다. 토스 상반기 공채 이력서도 작성해야 할 필요가 있다. 할게 너무 많다. 일단 월요일 논문리딩이 중요하니 내일 Neural Graph CF 논문 리딩 및 준비를 해야겠다." }, { "title": "[BoostCamp AI Tech / RecSys] Day35 - Recommender System with DL", "url": "/posts/day35_recsysbasic9/", "categories": "NAVER BoostCamp AI Tech, Level 2 - 추천 시스템 이론", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech, Recommender System, Autoencoder", "date": "2022-03-12 14:00:00 +0900", "snippet": "RecSys : Recommender System with DL이 내용은 논문들을 전반적으로 훑는 내용들입니다.향후 논문 리딩을 통해 더 자세한 내용이 paper review 카테고리에 추가될 예정입니다.추천 시스템의 딥러닝 최근에는 추천 시스템에 딥러닝의 아이디어를 도입하는 연구가 많이 진행됨 딥러닝을 도입하는 이유는 크게 다음과 있음 Nonlinear Transformation MF나 factorization machine 등은 기본적인 원리가 linear를 가정한 것이므로 model에서 표현의 한계가 존재(기존의 XOR과 같은 문제) DNN은 데이터의 non-linearity를 효과적으로 나타낼 수 있음 user-item pattern은 복잡한 관계를 가지는데 linear로는 이 관계를 효과적으로 모델링하기 어려울 수 있음 Representation Learning DNN은 raw data로부터 featrue representation을 직접 학습 Sequence Modeling next-item prediction, session-based recommendation(동일 세션 추천)과 같은 sequential task에 효과가 좋음 동일 세션의 시간순 데이터를 통한 예측 Flexibility Tensorflow, PyTorch등 여러 프레임워크를 활용해 실질적 서비스에 적용할 수 있음 Neural Collaborative Filtering MF의 한계를 지적하며 신경망 구조의 모델을 제시 Paper : Neural Collaborative Filtering문제 제기 Matrix Factorization의 한계 user와 item embedding이 선형조합 형태라는 것 선형관계는 표현의 한계를 가짐 similarity를 찾는 과정에서 latent space에서 정확한 유사관계를 표현하는데 문제가 발생함 실제 논문에서 언급하는 MF의 한계로는 user-item embedding에서는 실제로 $sim_{41}$과의 관계에서 jacard 유사도로 계산하면 $sim_{43}$이 $sim_{42}$보다 더 가깝지만 latent vector space에서는 어떠한 방법을 써도 $sim_{42}$가 더 가깝게 위치함 이를 해결하는 방법으로는 2차원의 경우는 3차원으로, 3차원이면 4차원으로 차원을 늘리는 형식의 vector space 변환을 진행해야하지만 차원을 증가하는 것은 한계가 존재문제 해결 모델 모델 구성을 크게 2가지 파트로 구분됨 MLP 파트 MF 파트 MLP 파트 Input Layer one-hot encoding user(item) vector : $v_u (v_i)$ Embedding Layer user(item) latent vector : $P^\\intercal v_u (Q^\\intercal v_i)$ Neural CF Layers $\\phi_{X}(\\cdots\\phi_{2}(\\phi_{1}(P^\\intercal v_u, Q^\\intercal v_i))\\cdots)$ $\\phi_{X}$는 x번째 neural network $\\phi_{1}(P^\\intercal v_u, Q^\\intercal v_i)$는 각 latent vector결과를 concatenate한 것을 신경망에 feed하는 것을 의미 Output Layer activation function : Logistic or Probit $ \\widehat{y}_{ui} = \\phi_o (\\phi_X(\\cdots\\phi_2(\\phi_1 (P^\\intercal v_u, Q^\\intercal v_i))\\cdots)) $ $\\widehat{y}_{ui} \\in [0,1]$ 최종 모델 Neural Matrix Factorization GMF라는 MF 일반화 모델과 MLP를 앙상블 기존 MF의 장점인 sparsity 해결과 같은 장점을 가져오면서 단점인 표현 한계를 MLP로 극복 두 모델은 서로 다른 embedding layer를 사용 $\\phi^{GMF} = (p_u^G)^\\intercal q_i^G$ $\\phi^{MLP} = \\phi_{X}(\\cdots\\phi_{2}(\\phi_{1}(p_u^M, q_i^M))\\cdots)$ $\\widehat{y}_{u,i} = \\sigma\\left( h^\\intercal \\begin{bmatrix} \\phi^{GMF} \\ \\phi^{MLP} \\end{bmatrix} \\right)$ MovieLens, Pinterest 데이터셋에 대하여 NCF의 추천 성능이 MF(BPR), MLP 모델보다 높음Deep Neural Networks for YouTube Recommendations Paper : Deep Neural Networks for YouTube Recommendations문제 제기 Scale 유저와 아이템의 수가 매우 많음 컴퓨팅 파워의 제한 효율적인 서빙과 추천 알고리즘이 필요 Freshness 기존의 학습 컨텐츠와 새로운 컨텐츠를 실시간으로 잘 조합해야 함 (exploration / exploitation) Noise 높은 Sparsity와 다양한 외부 요인 때문에 유저 행동 예측이 어려움 Implicit Feedback과 같은 낮은 품질의 메타데이터를 써야 효과가 좋음 모델 전체 구조Deep Neural Networks for YouTube Recommendations 모델은 크게 2개의 추천 구조로 이루어져있음 Candidate Generation High Recall을 위해 수많은 데이터 중 유저에게 어울리는 수백개의 Top N개 추천 후보군을 추출 이때 다양한 유저의 정보를 활용 Ranking Candidate Generation에서 생성된 추천 후보군에서 유저, 비디오 등 feature를 다채롭게 사용하여 최종 추천 리스트 제공 Candidate GenerationDeep Neural Networks for YouTube Recommendations 주어진 특정 유저 $u$가 선호할 video의 리스트를 생성하는 것이 목적 특정 시간(t)에 유저 U가 C라는 context에서 비디오(I)를 볼 확률을 계산 수백개의 아이템에서 후보군을 선택해야하므로 Extreme multiclass classification 문제 마지막에 내적을 통한 Softmax 함수를 사용하여 유저와 비디오의 관계를 계산하여 분류 진행 Deep Neural Networks for YouTube Recommendations CG에서 가장 중요한 embedding vector 다른 추천 모델에서는 유저의 행동 정보를 잘 활용하지 않았음 유저의 행동기록 임베딩 과거의 시청 이력과 검색 이력을 임베딩하여 vector 형성 마지막 기록들이 너무 강한 영향을 미치는 것을 방지하고자 전반적인 임베딩의 평균을 적용 Deep Neural Networks for YouTube Recommendations 성별 등 인구통계학 정보, 지리적 정보를 vector에 포함 Example Age 과거 데이터에 편향된 학습이 이뤄지는 것을 해결하고자 시청 로그가 학습 시점에서 경과한 정도를 피쳐로 추가 모델은 오래된 아이템의 특성상 과거의 인기 아이템에 편향되어 학습을 하는 경우가 있음 이런 경우 최신의 아이템은 과거에 비해 밀리게 되므로 아이템의 최신성을 고려하고자 추가로 feature에 학습 Deep Neural Networks for YouTube Recommendations\\[P(w_t = i | U, C) = \\frac{e^{v_i u}}{\\sum_{j\\in V}e^{v_j u}}\\] 위에서 생성된 모든 벡터를 concatenate 후 layer에 feed하여 나오는 것이 User Vector User Vector를 입력으로 하여 softmax를 통해 나온 아이템의 임베딩과 내적을 통해 유저와 아이템의 유사정도를 계산 이 과정은 아이템 자체가 수백개라 연산시간이 너무 오래 걸려 실제 serving에서는 무리가 있음 따라서 실제 serving에는 user vector와 video vector embedding index를 활용해 수백개의 리스트에서 Annoy와 Faiss등으로 Top-N을 추출하여 전달RankingDeep Neural Networks for YouTube Recommendations Candidate Generation에서 받은 비디오 후보군에서 최종 추천 아이템 순위를 책정 클릭확률을 계산하는 Logistic 회귀를 사용 loss function에서 weighted logistic은 비디오 시청시간을 가중치로 한 값이 사용됨Deep Neural Networks for YouTube Recommendations 입력 feature로는 user의 행동 패턴 feature가 사용됨 유저가 특정 채널에서 얼마나 많은 영상을 봤는가? 특정 토픽 영상을 본 지 얼마나 지났는지 등… feature engineering 부분이라 도메인 전문가의 역량이 중요Deep Neural Networks for YouTube Recommendations 여러가지 feature를 전달받은 neural network를 통과한 결과는 비디오가 실제로 시청될 확률 매핑 binary classification으로 시청 여부만을 예측 loss function에서 weighted logistic을 활용하는 이유는 영상 시청 시간은 유저의 만족도와 비례하기 때문 이 과정에서 cross-entropy 사용 예를 들어 사용자가 좋아하는 부류의 영상은 전체 동영상 길이에 근접한 시청시간을 갖지만 광고나 관심이 없는 영상은 실수로 시청을 했더라도 매우 적은 시청시간을 갖게될 것 논문의 의의 최초의 2단계 추천 딥러닝 모델을 제안 기존 CF에서는 참고 feature의 종류가 적었던 부분을 보완하는 방식을 채택 실제 시청시간을 예측하므로 더 뛰어난 성능을 보여줌 CG에서도 많은 feature를 사용하지만 추가적으로 ranking과정에서도 많은 feature를 쓰므로 성능이 좋음AutoEncoder 오토인코더란 입력 데이터를 hidden layer를 통해 feature representation을 진행하여 입력을 재구축하는 출력을 만드는 모델 중간에 있는 hidden layer는 입력 데이터들의 feature representation 정보를 담고 있음 오토인코더는 깔끔한 데이터에 overfitting되는 경향이 존재해서 일부러 noise넣어 학습하는 Denoising Autoencoder가 등장함 noise나 dropout으로 입력 데이터를 손상하여 학습 AutoRec Paper : AutoRec: Autoencoders Meet Collaborative Filtering Autoencoder를 CF에 적용하여 feature representation을 더 효과적으로 적용 Rating Vector를 입력과 출력으로 하여 AutoEncoder를 학습 중간에 hidden layer는 유저-아이템 rating vector를 latent feature로 저장하는 역할 기존 MF와의 차이는 linear라는 한계로 복잡한 관계를 표현하기 어려움이 존재모델AutoRec: Autoencoders Meet Collaborative Filtering 모델 자체는 매우 간단하다. 실제 구현도 nn.Linear 2개로 구현한다….;; 임베딩은 아이템과 유저 중 1개만 진행 $\\mathbf{r}^{(i)}$ : 아이템 $i$의 rating vector $R_{ui}$ : 유저 $u$의 아이템 $i$에 대한 rating $V$ : 인코더 가중치 행렬, 실제로는 nn.Linear $W$ : 디코더 가중치 행렬, 실제로는 nn.Linear학습\\[\\begin{aligned}\\underset{\\theta}{\\min}\\sum_{\\mathbf{r}\\in \\mathbf{S}} \\lVert \\mathbf{r} - h(\\mathbf{r} ; \\theta) \\rVert_2^2 \\qquad \\\\\\\\h(\\mathbf{r} ; \\theta) = f(\\mathbf{W} \\cdot g(\\mathbf{V}\\mathbf{r} + \\boldsymbol{\\mu}) + \\mathbf{b})\\end{aligned}\\] 기존 rating matrix와 출력으로 생성되는 reconstructed rating matrix의 RMSE를 목적함수로 설정 관측된 데이터에 대해서만 역전파와 파라미터 업데이트 구현시 np.where(y != 0)으로 관측 데이터를 확인 $g(\\mathbf{V}\\mathbf{r} + \\boldsymbol{\\mu})$: representation 항Collaborative Denoising Auto-Encoder Paper : Collaborative Denoising Auto-Encoders for Top-N Recommender Systems Denoising Autoencoder를 적용한 논문 AutoRec는 추천 아이템을 찾는 것보다는 Rating prediction이 핵심 목표 CDAE는 ranking을 계산하여 Top-N을 추천하는 모델 평가지표로 NDCG를 많이 활용함 rating prediction이 목표가 아니기 때문에 유저-아이템 정보를 binary 형태로 바꾼 아이템 선호정보(preference)로 변화하여 학습모델Collaborative Denoising Auto-Encoders for Top-N Recommender Systems AutoRec과 다르게 noise를 추가 noise는 dropout을 활용하여 일부 선호정보를 탈락시킴 $P(\\tilde{y}_u = \\delta y_u) = 1-q, P(\\tilde{y}_u = 0) = q$ $\\tilde{y}_u$는 q확률에 의해 dropout된 벡터 유저 벡터 $V_u$를 학습하면서 다른 유저에 대한 특징을 같이 학습 (collaborative 성격) input으로 제공되는 $\\tilde{y}$는 각 user의 선호 정보인데 기존 데이터에서 일부는 0으로 처리되는 noise를 제공한다. ex) origin = [0, 1, 1, 0, 1], $\\tilde{y}$ = [0, 0, 1, 0, 1] 추가적으로 개별 유저 정보도 같이 넣어서 학습을 진행함 원문: Note that $\\mathbf{V}_u$ is a user-specific vector, i.e., for each of the users we have one unique vector. From another point of view, $\\mathbf{W}_i$ and $\\mathbf{V}_u$ can be seen as the distributed representations of item i and user u respectively. encoder의 hidden layer를 통해 latent representation을 만들고 디코더로 재생성" }, { "title": "[BoostCamp AI Tech / RecSys] Day35 - Approximate Nearest Neighbor", "url": "/posts/day35_recsysbasic8/", "categories": "NAVER BoostCamp AI Tech, Level 2 - 추천 시스템 이론", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech, Recommender System, ANN, Nearest Neighbor", "date": "2022-03-11 17:00:00 +0900", "snippet": "RecSys : Approximate Nearest Neighbor (ANN)ANN의 필요성 추천 시스템은 기본적으로 Vector Space에 존재하는 새로운 query vector와 가장 유사한 vector를 찾는 방식을 많이 활용함 MF에서도 아이템과 유저가 같이 임베딩된 latent vector space에서 유저와 아이템의 유사도 연산을 진행하거나 아이템과 아이템의 유사도를 계산함 문제는…. 특정 K개를 뽑기위해 모든 vector space를 탐색하는 Brute Force KNN은 vector의 차원과 개수에 비례한 연산을 진행함 100차원 기준 100만개의 벡터의 유사도를 계산할 때 0.1초가 걸린다고 가정하면 1,000,000 X 0.1 = 100,000초 = 약 27.7시간이라는 시간이 소모됨 (실제 소요시간은 0.1초보다 기므로 더 시간이 길어짐) 결과적으로 정확도와 시간은 trade-off이므로 정확도를 포기하고 시간을 어느정도 확보하는 방법으로 접근 정확한 Nearest Neighbor가 아닌 근사적 접근을 활용한 Approximate NN을 찾는 방법을 사용ANNOYANNOY 개념과 알고리즘Github : spotify/ANN spotify에서 개발한 tree-based ANN 방법 Github : spotify-ANN / paper 주어진 벡터들을 여러 개의 subset으로 나누어 tree 자료구조로 저장하여 탐색하는 방법https://www.slideshare.net/erikbern/approximate-nearest-neighbor-methods-and-vector-models-nyc-ml-meetup?from_action=save Vector Space에서 임의의 두 점을 선택한 뒤, 두 점사이의 hyperplane으로 vector space를 나눔 (두 점 사이의 직선의 수직 이등분하는 hyperplane) Subspace에 있는 점들의 개수를 node로 하여 binary tree 생성하거나 갱신 Subspace 내의 점이 K개를 초과했다면 해당 Subspace에 대해 1, 2를 반복하여 모든 subspace가 K개 아래로 점을 갖게 진행https://www.slideshare.net/erikbern/approximate-nearest-neighbor-methods-and-vector-models-nyc-ml-meetup?from_action=save 특정 query case가 입력되면 트리구조의 특성상 $O(\\log N)$ 시간안에 동일 노드에 존재하는 Nearest Neighbor들을 찾을 수 있음ANNOY의 문제점과 해결방안 붉은색 원안에 있는 것처럼 점이 경계선 근방에 존재한다면 오히려 같은 노드에 존재하는 점들보다 다른 노드에 더 가까운 점이 있지만 NN을 카운트 하지 못함 이 문제는 hyperplane을 형성하는 점의 선택이 random으로 이루어지기 때문에 발생하는 것https://www.slideshare.net/erikbern/approximate-nearest-neighbor-methods-and-vector-models-nyc-ml-meetup?from_action=savehttps://www.slideshare.net/erikbern/approximate-nearest-neighbor-methods-and-vector-models-nyc-ml-meetup?from_action=save 이를 해결하고자 다음과 같은 방법을 사용해서 해결 priority queue를 사용하여 가까운 다른 노드를 탐색 binary tree를 여러 개 생성하고 병렬적 탐색 parameter number_of_trees : 생성하는 binary tree 수 search_k : NN을 구할 때 탐색하는 node 수 여전히 해결되지 않은 문제는… 기존 생성된 bianry tree에 새로운 데이터를 추가할 수 없음 GPU 연산의 미지원 기타 ANNHierarchical Navigable Small World Graphs (HNSW) 벡터를 그래프의 node로 표현하고 인접 벡터를 edge로 연결 물리적으로 거리가 작은 node들만 edge를 형성해야 함 Layer 0 $\\rightarrow$ Layer n으로 갈수록 노드를 랜덤 샘플링해서 형성함 nmslib나 faiss가 대표적인 라이브러리 최상위 layer n에서 임의의 노드에서 시작 현재 탐색 layer에서 타겟 노드와 가장 가까운 노드로 이동 더 가까워지는 게 불가능하면 다음 하위 layer로 이동 타겟 노드에 도달할 때까지 2, 3을 반복 2 ~ 4를 진행할 때 방문한 노드들만 후보로 NN 탐색Inverted File Index (IVF) clustering 과정을 통해 vector를 n개의 클러스터로 나눠서 저장 vector의 index를 cluster별 inverted list로 저장 query vector에 대해 cluster 탐색 $\\rightarrow$ 해당 cluster의 inverted list의 vector들을 탐색 경계 data에서 NN이 발생하는 것을 해결하고자 주변 cluster의 확장과정을 필요로 함 클러스터 수가 늘어나면 정확도와 속도의 trade off가 발생Product Quantization - Compression (PQ) 기존에 존재하는 벡터를 n개의 sub-vector로 나눔 각 sub-vector군에 대해 k-means clustering을 통해 centroid(중심점)을 찾음 기존의 모든 vector를 n개의 centroid로 압축해서 표현 centroid간의 유사도를 활용해서 벡터 유사도를 계산 PQ와 IVF를 동시에 사용하면 더 빠르고 효율적인 ANN이 수행 가능 faiss 라이브러리에서 제공 " }, { "title": "[BoostCamp AI Tech / RecSys] Day35 - Item2Vec", "url": "/posts/day35_recsysbasic7/", "categories": "NAVER BoostCamp AI Tech, Level 2 - 추천 시스템 이론", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech, Recommender System, Item2Vec", "date": "2022-03-11 11:00:00 +0900", "snippet": "RecSys : Item2Vec임베딩 (Embedding)Embedding 데이터를 낮은 차원의 벡터로 표현하는 방법 Sparse Representation 아이템의 전체 종류 = 차원의 수 아이템 개수가 많을 수록 벡터 대표적인 예시로는 one-hot encoding이 있음ex) 면도기 = [0, 0, 0, 1, 0, 0] Dense Representation 아이템 전체 종류 » 차원의 수 기존 벡터를 압축해서 표현하므로 binary가 아닌 실수값으로 표현된 벡터 (compact vector)ex) 면도기 = [0.4, 0.2] Word Embedding 텍스트 분석을 위해 단어를 벡터로 변환하는 것 벡터공간에 단어를 배치하기 때문에 단어 간 유사도를 계산할 수 있음 임베딩 표현을 위해서는 학습 모델이 필요 Matrix Factorization에서 latent matrix는 일종의 embedding Word2Vec Neural Network Language Model(NNLM) 기반의 단어예측 기술 dense vector로 표현하여 대량의 문서를 vector space에 투영 대표적인 학습 방법 CBOW Skip-Gram Skip-Gram w/Negative Sampling (SGNS) Word2Vec은 Item2Vec을 이해하기위한 베이스 정도의 지식만 잡을 것Continuous Bag of Words (CBOW) 주변에 있는 단어를 활용하여 중심 단어를 예측하는 방법 인간은 전반적인 맥락을 파악해서 단어를 유추하는데, 이 과정에서 주변부의 단어를 통해 빈칸을 유추하는 경우가 많음 앞 뒤 단어의 개수를 window라는 파라미터의 크기로 결정 실제 예측에 사용하는 단어는 앞 뒤 n개로 2n개의 단어 사용 \\[\\begin{aligned}\\text{Embedding:} &amp;amp; &amp;amp; x_{brown} \\times W_{V\\times M} = V_{brown} \\qquad\\qquad\\quad \\\\\\\\\\text{hidden layer:} &amp;amp; &amp;amp; v_i = \\frac{V_{i-n} + V_{i-n+1} + \\cdots + V_{i-1} + V_{i+1} + \\cdots + V_{i + n}}{2n} \\\\\\\\\\text{output:} &amp;amp; &amp;amp; z = v_i \\times W&#39;_{M\\times V} \\; \\rightarrow \\; \\hat{y} = softmax(z) \\qquad\\qquad\\end{aligned}\\] 학습에 사용되는 단어들을 우선 one-hot encoding으로 표현 input dimension $\\rightarrow$ embedding dimension으로 임베딩 주변부 2n개의 단어 임베딩 결과를 평균내어 hidden layer에 전달 최종 결과는 실제 one-hot encoding과 output의 softmax 결과의 오차를 계산해서 학습Skip-Gram 컨셉은 CBOW의 입력과 출력을 뒤집은 형태 중심 단어로부터 주변부 단어들을 예측하는 multi-class classification 모델 output 결과는 ${ \\text{center word}, n \\text{ window words} }$ 여기서 Skip-Gram의 문제점은 주변에 존재하는 word 위주로만 학습하기 때문에 overfitting의 위험성이 존재함 이 문제를 해결하고자 일부러 주변부에 속하지 않는 단어를 일부 sampling하는 negative sampling 방식을 적용Skip-Gram Negative Sampling (SGNS) SGNS에 사용되는 하이퍼파라미터에는 positive sample 1개당 몇개의 negative sample을 생성할지를 결정하는 k가 있음 중심단어와 주변단어는 서로 다른 lookup table(Embedding Matrix)에 임베딩함 두 Embedding Matrix는 반드시 동일 차원이어야 함 각 Embeddng matrix에서 중심 단어와 주변 단어의 lookup table들을 가져와서 내적과 sigmoid를 통해 레이블을 binary로 분류 두 값의 차이를 통해 error를 연산하고 각각의 embedding layer를 업데이트 둘 중 1개만 쓰거나 둘의 평균치를 활용Item2Vec SGNS 아이디어에서 착안한 item-based CF에 word2vec에 적용한 논문 핵심 아이디어는 SGNS라 SGNS의 아이디어와 동일 유저가 소비한 아이템 리스트를 문장으로 설정하고 각 아이템을 단어로 가정하고 Word2Vec을 적용 유저-아이템 관계를 사용하지 않으므로 유저를 별도로 식별하지는 않음 이 말은 다시 말하면 비로그인 상태의 추천인 세션단위 추천이 가능 Item2Vec의 목표는 SGNS 기반의 아이디어를 활용하여 아이템을 vector화하여 vector space에 배치하는 것 소비 아이템 집합을 생성하여 사용하기 때문에 공간/시간적 정보는 존재하지 않음\\[\\frac{1}{K}\\sum_{i=1}^{K}\\sum_{-n\\leq j &amp;lt; n, j \\neq 0} \\log p(w_{i+j} | w_{i}) \\quad \\rightarrow \\quad \\frac{1}{K}\\sum_{i=1}^{K}\\sum_{j \\neq i}^{K} \\log p(w_{j} | w_{i})\\] 집합관계를 사용하므로 동일 아이템 집합은 Positivie Sample로 설정됨 Skip-Gram은 앞뒤 window n개를 참고하지만 Item2Vec은 모든 아이템 쌍을 활용함 같은 아이템은 제외함 (center word와 center word가 굳이 관계를 구할 필요는….) 예시를 통한 이해 동일 세션 유저가 A, C, B 아이템을 소비한 상황 아이템 집합 {A, B, C}$\\rightarrow$ Word2Vec 데이터 [A, B, 1], [A, C, 1], [B, C, 1] , [B, A, 1] , [C, A, 1], [C, B, 1] 이렇게 생성된 테이블에 Neagtive Sampling 진행 SGNS처럼 동일 집합내의 임베딩 정보를 추출해서 Item2Vec에 넣어서 처리" }, { "title": "[BoostCamp AI Tech] Day34", "url": "/posts/day34/", "categories": "NAVER BoostCamp AI Tech, Review", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech", "date": "2022-03-10 23:00:00 +0900", "snippet": "Day34 Review당신은 오늘 하루 어떻게 살았나요? 추천 시스템 basic 3~4강 정리 추천시스템 기본과제 충분한 휴식오늘 하지 못한 것들 추천 Basic 5-6강 정리 심화과제내일은 어떤 것을 할까? 추천 Basic 5-6강 정리 심화과제 버킷플레이스 코테 이전 코테 준비 및 풀이이번주에 할 것들 추천 시스템 basic 내용 정리 Neural Graph CF 관련 논문 리딩 및 세미나 준비 삶의 지도 그려보기 + 변성윤 마스터님 두러두런 1회차 질문에 답해보기 코세라 MLOps Specialization 2주차 정리 추천 시스템 기본과제마무리 변성윤 마스터님께서 말씀하신 CV 작성법을 고려해서 역량을 강조한 CV를 작성했다. 생각보다 내가 일관성 있게 프로젝트에서 역할을 했다는 것을 알았다. 이번주 토요일에는 버킷플레이스 코테가 있는데 4학년 들어와서 첫 공채지원이다. 열심히 준비해야겠다. 다음주부터 본격적으로 1일 1백준을 시도할 예정이다. 문제 리스트업을 어떻게 할지 고민했는데, 아마도 solved class 도장깨기로 할 것 같다." }, { "title": "[BoostCamp AI Tech / RecSys] Day34 - Matrix Factorization", "url": "/posts/day34_recsysbasic6/", "categories": "NAVER BoostCamp AI Tech, Level 2 - 추천 시스템 이론", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech, Recommender System, Collaborative Filtering, CF, MF, Matrix Factorization", "date": "2022-03-10 14:22:00 +0900", "snippet": "RecSys : Matrix Factorization강의에서는 ALS와 BPR을 다뤘지만 두 내용은 논문을 직접 읽고 분석할 예정이므로 Paper Review 카테고리에 업데이트 될 예정입니다.Model Based CF 이전에 언급한 Neighborhood-Based CF는 Sparsity와 Scalability 문제가 존재했음 NBCF는 데이터를 메모리에 올려놓고 사용한다고해서 Memory-based CF라고도 함 이 문제를 해결하고자 모델 기반 협업 필터링(MBCF) 이 등장함 단순히 유사도를 비교하는 것보다 데이터 내부의 패턴분석을 통해 추천하는 방식 Parametric Machine Learning 방식 데이터 정보가 파라미터(데이터 패턴)로 모델에 적용되고 이 파라미터를 학습하는 것이 목표 장점 모델을 학습하여 압축된 형태로 저장함 기존 모델 서빙처럼 학습된 모델을 통해 추천하므로 서빙 속도가 빠흠 NBCF의 문제인 sparsity, scalability 한계 극복 Sparsity Ratio가 99.5%가 넘어도 좋은 성능을 보임 NBCF는 특정 K개의 이웃에 대해 추천하므로 오버피팅의 위험이 존재하지만 MBCF는 모든 데이터 패턴을 학습하므로 이를 방지할 수 있음 NBCF는 이론적으로는 그럴듯하지만 현실의 데이터에서는 일정 기준치를 넘는 유사도를 갖는 케이스를 찾기가 어려운 문제가 존재함 (Limited coverage) NBCF VS MBCF NBCF는 특정 무언가를 학습하는 것이 아닌 단순 저장된 데이터를 통해 계산하는 방식으로 추천 MBCF에서 등장하는 유저, 아이템 벡터는 모두 학습 과정을 통해 업데이트하는 파라미터FeedbackExplicit Feedback 영화 평점, 별점 등 유저가 아이템에 대한 직접적인 선호 강도를 표현한 데이터 데이터 추천에 있어서 가장 편리한 수단Implicit Feedback 클릭 여부, 시청 여부, 페이지 접속 등 아이템에 대한 유저의 간접적인 선호를 표현한 데이터 상호작용의 여부이므로 일반적으로는 binary 형태로 기록 현실적으로 가장 많이 존재하는 데이터Latent Factor Model Latent Factor Model이란 최근에는 Embedding이라고 부름 유저와 아이템의 관계는 매우 복잡한 요소들로 이루어져 있으나 이를 간단한 몇 개의 벡터 차원으로 embedding할 수 있다는 모델 컨셉 유저-아이템 행렬을 저차원 행렬로 분해하는 방법으로 작동 black box인 모델 특성상 latent factor가 어떤 패턴을 추출한지는 알 수 없음 핵심 아이디어는 유저와 아이템을 동일 벡터공간에 두고 유저와 아이템의 유사도를 동시에 판단SVD : Singular Value Decomposition\\[R = U \\Sigma V^{\\intercal}\\] SVD는 선형 대수학에서 차원 축소 기법 중 하나 유저가 책정한 rating이 담겨있는 rating matrix($R$)를 3개의 벡터로 분해 $U$ : 유저-잠재 행렬 (user-latent matrix) $\\Sigma$ : 잠재 대각행렬 (latent matrix) $RR^\\intercal$을 고유값 분해하여 나오는 값 $V$ : 아이템-잠재 행렬 (item-latent matrix) 모든 latent factor를 사용하는 것보다 가장 중요한 정보들만 살려서 연산을 하는 Truncated SVD를 사용하기도 함 정확하게 $R$로 복원할 수는 없지만 유사하고 근접한 $\\widehat{R}$로 복원 한계점 SVD는 non-sparse matrix에 한해서만 연산이 가능 하지만 현실 데이터는 sparse matrix 이를 해결하고자 결측치를 채우는 과정에서 데이터의 왜곡과 연산비용이 증가 SVD의 한계를 해결하고자 Matrix Factorization이 등장Matrix Factorization User-Item 행렬을 저차원의 User와 Item의 latent factor 행렬의 곱으로 분해하는 SVD 방식 개념을 활용 관측된 데이터만 활용할 수 있게 SVD를 변형하여 활용 Rating Matrix를 $P$와 $Q$로 분해하여 가장 유사한 $\\widehat{R}$을 추론MF 기본 원리\\[\\underset{P,Q}{\\min} \\sum_{\\text{observed } r_{u,i}} (r_{u,i} - p_{u}^\\intercal q_{i})^2\\] 실제 rating matrix ($R$)과 유사한 rating matrix인 ($\\widehat{R}$)의 오차가 최소가 되게 하는 것이 목적 실제 코드 구현에서는 마치 layer의 초기 weight가 랜덤으로 초기화된 것처럼 무작위의 형태로 $P$와 $Q$를 설정 구현적인 부분은 따로 포스팅을 할 예정 MF의 핵심적인 요소들을 알아보면서 원리를 깊게 알아보겠다.Objective Function\\[\\underset{P,Q}{\\min} \\sum_{\\text{observed } r_{u,i}} (r_{u,i} - p_{u}^\\intercal q_{i})^2 + \\lambda(\\lVert p_{u}\\rVert_2^2 + \\lVert q_{i} \\rVert_2^2)\\] 모델 학습에 사용되는 목적함수는 위의 식으로 구성되어 있음 2가지 항으로 구성 $\\underset{P,Q}{\\min} \\sum_{\\text{observed } r_{u,i}} (r_{u,i} - p_{u}^\\intercal q_{i})^2$ : 실제 목적함수 $\\lambda(\\lVert p_{u}\\rVert_2^2 + \\lVert q_{i} \\rVert_2^2)$ : Regularization 항 $p_u$, $q_i$는 유저와 아이템의 latent vector이고 목적함수를 통해 지속적으로 업데이트되는 matrix 형태의 파라미터 여기서 잘 생각해보면 이전에 언급한 matrix가 선형 대수학에서 어떤 의미인지와 연관성이 깊다고 볼 수도 있다. 우리가 모델에 layer를 쌓는 것은 weight matrix를 곱하여 vector space를 변화해주는 역할이었다. 물론 이렇게 vector space를 변화해주는 것인지는 확실치 않지만 결국 layer를 쌓은 것처럼 weight가 담긴 matrix의 weight를 학습하는 것과 유사한 원리이다. 뒤쪽의 항은 L2-regularization인 규제항 규제항의 역할은 학습 데이터의 과적합을 방지해주는 역할 Regularization\\[\\lambda(\\lVert p_{u}\\rVert_2^2 + \\lVert q_{i} \\rVert_2^2)\\] weight를 loss function에 넣어주면 weight가 너무 커지지 않도록 제한이 걸려서 overfitting이 되는 것을 방지함 $\\lambda$는 일반적으로 하이퍼파라미터로 결정되며 영향력을 결정함SGD\\[\\begin{aligned}\\text{Loss: } L = \\sum (r_{u,i}-p_u^\\intercal q_i)^2 + \\lambda(\\lVert p_{u}\\rVert_2^2 + \\lVert q_{i} \\rVert_2^2)\\end{aligned}\\] Original MF의 아이디어에서는 SGD를 통해 모델 파라미터를 학습 Loss function은 앞서 설명한 목적함수를 활용해서 계산\\[\\text{Gradient: } \\frac{\\partial L}{\\partial p_u} = \\frac{\\partial (r_{ui}-p_t^\\intercal q_i)^2}{\\partial p_u} + \\frac{\\partial \\lambda \\lVert p_{u}\\rVert_2^2}{\\partial p_u}\\] $p_u$와 $q_i$에 대해 각각 gradient를 계산해서 update 진행\\[\\begin{aligned}\\text{Error: } e_{ui} = r_{ui} - p_u^\\intercal q_i \\qquad\\qquad\\qquad\\qquad \\\\\\\\\\text{Gradient} = -2(r_{ui} - p_u^\\intercal q_i)q_i + 2\\lambda p_u = -2(e_{ui}q_i - \\lambda p_u)\\end{aligned}\\] 기존의 gradient update와 동일한 방식을 사용해서 반대방향으로 $p_u$, $q_i$ 업데이트\\[\\begin{aligned}p_u \\leftarrow p_u + \\eta\\cdot(e_{ui}q_i - \\lambda p_u)\\\\q_i \\leftarrow q_i + \\eta\\cdot(e_{ui}p_u - \\lambda q_i)\\end{aligned}\\] -2는 원래 적는게 맞지만 그냥 learning rate에 포함된 계산이라 생략된다.Matrix Factorization Techniques for Recommender Systems Netflix Prize 대회 우승 논문으로 대표적인 MF 추천 논문 오리지널 MF에 추가적인 테크닉을 적용하여 성능을 향상함 핵심은 다른 것을 수정하는 것이 아닌 목적함수를 수정Adding Biases\\[\\underset{P,Q}{\\min} \\sum_{\\text{observed } r_{u,i}} (r_{u,i} - {\\color{red}{\\mu - b_u - b_i}} - p_{u}^\\intercal q_{i})^2 + \\lambda(\\lVert p_{u}\\rVert_2^2 + \\lVert q_{i} \\rVert_2^2 + {\\color{red}{b_u^2 + b_i^2}})\\] 위에서 언급한 유저별로 평점의 분포가 다를 수 있다는 것을 고려한 편차를 적용하는 방법을 도입 전체 평균($\\mu$), 유저/아이템의 bias를 추가해서 이를 보정하는 새로운 목적함수를 설정 에러도 이에 맞춰 수정$e_{ui} = r_{ui} - \\mu - b_u - b_i - p_u^\\intercal q_i$\\[\\begin{aligned}&amp;amp; b_u \\leftarrow b_u + \\gamma\\cdot(e_{ui} - \\lambda b_u) \\\\&amp;amp; b_i \\leftarrow b_i + \\gamma\\cdot(e_{ui} - \\lambda b_i) \\\\&amp;amp; p_u \\leftarrow p_u + \\gamma\\cdot(e_{ui}q_i - \\lambda p_u) \\\\&amp;amp; q_i \\leftarrow q_i + \\gamma\\cdot(e_{ui}p_u - \\lambda q_i) \\\\\\end{aligned}\\] 항이 늘어난만큼 추가적인 gradient update도 진행 $\\gamma$는 learning rateAdding Confidence Level\\[\\underset{P,Q}{\\min} \\sum_{\\text{observed } r_{u,i}} {\\color{red}{c_{u,i}}}(r_{u,i} - {\\mu - b_u - b_i} - p_{u}^\\intercal q_{i})^2 + \\lambda(\\lVert p_{u}\\rVert_2^2 + \\lVert q_{i} \\rVert_2^2 + {b_u^2 + b_i^2})\\] 모든 평점 데이터가 동일한 신뢰도를 갖지 않는다는 문제를 보완 특정 아이템이 많이 노출되어 클릭률이 높은 경우 유저가 정확한 평점을 입력하지 않은 경우 각 rating에 대한 신뢰도인 $c_{u,i}$를 추가Adding Temporal Dynamics\\[\\begin{aligned}\\widehat{r_{ui}(t)} = \\mu + b_u(t)+b_i(t)+p_u^\\intercal q_i(t) \\\\\\\\b_u(t) = b_u + \\alpha_u\\cdot sign(t - t_u)\\cdot |t-t_u|^\\beta\\end{aligned}\\] 아이템은 시간이 흐름에 따라 인기도가 떨어지는 경향이 있음 유저는 새로운 아이템에 더 긍정적이고 오래된 아이템에 부정적인 경향이 나타나는 엄격해지는 경향성을 보임 이러한 시간의 흐름을 반영하는 모델을 설계" }, { "title": "[BoostCamp AI Tech] Day33", "url": "/posts/day33/", "categories": "NAVER BoostCamp AI Tech, Review", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech", "date": "2022-03-08 23:00:00 +0900", "snippet": "Day33 Review당신은 오늘 하루 어떻게 살았나요? 추천 시스템 basic 3~4강 캐글 EDA오늘 하지 못한 것들 기본과제1 Class-Based n-gram Models of Natural Language 리딩 및 정리내일은 어떤 것을 할까? 추천 시스템 basic 5~6강 Class-Based n-gram Models of Natural Language 리딩 및 정리 추천 시스템 basic 내용 정리이번주에 할 것들 추천 시스템 basic 내용 정리 Neural Graph CF 관련 논문 리딩 및 세미나 준비 삶의 지도 그려보기 + 변성윤 마스터님 두러두런 1회차 질문에 답해보기 코세라 MLOps Specialization 2주차 정리 추천 시스템 기본과제마무리 오늘 변성윤 마스터님 두런두런 시간을 보고 많은 생각이 들었다. 내가 기존에 작성하고 있던 이력서는 내가 자랑하기 위한 거였다는 생각이 많이 들었다. 물론 나를 어필하는 것이 이력서의 주 목적이지만 과연 담당자들은 내 이력서가 수많은 종이들 중 1개가 아닌 특별한 무언가로 볼까?라는 생각을 해보면 그렇지 않을 것 같다. 내 스스로 내 진로를 충분히 잘 알고 있다고 생각했는데, 오늘 멘토님 상담도 그렇고 마스터님 두런두런도 그렇고 생각보다 하는게 많고 막상 잡아놓은 느낌이 없어서 그런지 눈에 안 들어오는 것 같았다. 마스터님께서 말씀하신 삶의 지도를 주말동안 쭉 적어봐야겠다. 어떻게 보면 나에게는 굉장히 은인? 같은 유튜버를 발견했다. Karolina Sowinska라는 해외 데이터 엔지니어인데, 굉장히 도움이 많이 되고 있다. 영어공부도 되기도 하고 내용 자체도 내가 고민하는 것들, 내 방향성에 대한 고민을 잘 다루고 있는 느낌이다. 기존의 하루 review를 너무 일정관리 표처럼 사용했는데 마무리는 이런식으로 하루 회고 느낌으로 마무리 지어야겠다." }, { "title": "[BoostCamp AI Tech / RecSys] Day33 - Collaborative Filtering", "url": "/posts/day33_recsysbasic5/", "categories": "NAVER BoostCamp AI Tech, Level 2 - 추천 시스템 이론", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech, Recommender System, Collaborative Filtering, CF", "date": "2022-03-08 14:22:00 +0900", "snippet": "RecSys : Collaborative Filtering협업필터링 : Collaborative Filtering 이름의 의미대로 다른 유저들의 정보를 활용하여 특정 유저의 관심사를 예측 많은 유저/아이템 데이터가 축적될수록 높은 정확도를 보여줄 것이라는 아이디어 DL에서 학습 데이터 양이 많아지면 성능이 좋아진다는 개념에서 착안 유저 A와 비슷한 취향의 유저들이 선호/비선호하는 정보를 활용하여 유저 A에게 추천하는 원리Neighborhood-based CF 특정 유저가 선택하지 않은 아이템 i에 어떤 평가를 내릴지를 예측하는 것이 목표 직관적인 아이디어이므로 구현이 간단하고 이해하기 쉽다는 장점이 있음 문제점 Scalability 아이템이나 유저가 증가하면 확장성이 떨어짐 메모리에 로드할 수 있는 테이블의 크기 한도가 존재함 Sparsity 추천 시스템의 근본적인 문제인 sparse matrix에 취약함 NBCF는 sparsity ratio(전체 행렬 중 비어있는 원소 비율)가 99.5%를 넘지 않는 것이 좋음 두가지 문제점을 해결하고자 Model-based CF인 Matrix Factorization이 등장함유저 기반 협업 필터링 : User-based CF 두 유저가 얼마나 유사한 아이템을 선호하는가? 가 기준 유저간의 유사도를 활용하여 유사도가 특정 유저와 유사도가 높은 유저들의 선호 아이템을 추천 위의 테이블에서는 User A가 유사도가 높은데 이런 경우 User B도 5.0을 평점으로 줄 확률이 높다고 본다.아이템 기반 렵업 필터링 : Item-based CF 두 아이템이 유저들에게 얼마나 유사한 평가를 받았는가?를 기준 아이템간의 유사도를 활용하여 유사도가 높은 아이템을 기반으로 타겟 아이템을 추천K-Nearest Neighbors CF (KNN CF) NBCF는 모든 유저와의 유사도를 구해야 하는 시간적, 자원적 문제가 존재 유저가 많아지는 경우 연산이 늘어나고 성능이 감소함 유사도가 높은 특정 K명만 추출해서 활용하자는 KNN 아이디어에서 착안 여기서 K는 일반적으로 25~50을 많이 사용하지만 조정하는 하이퍼파라미터Mean Squared Difference Similarity\\[\\begin{aligned}msd(u, v) = \\frac{1}{|I_{uv}|}\\cdot \\sum_{i\\in I_{uv}}(r_{ui}-r_{vi})^2 , \\qquad msd_{sim(u, v)} = \\frac{1}{msd(u,v) \\color{red}{+ 1}} \\\\\\\\msd(i, j) = \\frac{1}{|U_{ij}|}\\cdot \\sum_{u\\in U_{ij}}(r_{ui}-r_{uj})^2, \\qquad msd_{sim(i,j)} = \\frac{1}{msd(i,j) \\color{red}{+ 1}}\\end{aligned}\\] mean squared의 특징에 따라 유클리드 거리에 반비례하는 특성을 보임 거리가 멀수록 유사도는 떨어진다는 아이디어 Cosine Similarity\\[\\cos(\\theta) = \\cos(X, Y) = \\frac{X \\cdot Y}{|X| |Y|} = \\frac{\\sum_{i=1}^{N} X_iY_i}{\\sqrt{\\sum_{i=1}^{N} X_{i}^2} \\sqrt{\\sum_{i=1}^{N}Y_i^2}}\\] MSD의 문제점인 두 벡터의 크기가 유사도에 영향을 준다는 것을 해결하고자 등장 cosine은 단순 각도의 개념만 갖게되므로 크기가 영향을 주지 않음 두 벡터의 방향성이 곧 유사도의 지표 가장 많이 사용하는 유사도 지표 자세한 설명은 링크 참조Pearson Similarity (Pearson Correlation)\\[\\text{pearson\\_sim}(X, Y) = \\frac{\\sum_{i=1}^{N} (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sqrt{\\sum_{i=1}^{N} (X_i - \\bar{X})^2}\\sqrt{\\sum_{i=1}^{N} (Y_i - \\bar{Y})^2}}\\] 각 벡터의 표본평균으로 정규화하고 코사인 유사도를 구한 것 정규화하는 과정을 통해 유저의 평가 기준을 통일시킬 수 있음 특정 유저는 평가범위가 1 ~ 3, 다른 유저는 3 ~ 5일 수 있음 각 vector rating의 크기차를 고려 가능 Jacard Similarity\\[J(A, B) = \\frac{|A\\cap B|}{|A\\cup B|} = \\frac{|A \\cap B|}{|A| + |B| - |A\\cap B|}\\] 앞의 유사도들과 다르게 벡터가 아닌 집합에 대한 유사도 연산 집합의 유사도를 계산하는 특성상 다른 유사도들과 달리 차원이 달라도 이론적 유사도가 계산이 가능 두 집합의 아이템의 공유 정도를 의미하는 유사도PredictionAverage\\[\\hat{r}(u, i) = \\frac{\\sum_{u&#39;\\in\\Omega_{i}}r(u&#39;,i)}{|\\Omega_i|}\\] 가장 일반적으로 많이 하는 방법 다른 유저들의 평점의 평균치를 활용하는 것 자신과 성향이 다른 유저도 동일한 비율로 적용된다는 문제 점이 존재Weighted Average\\[\\hat{r}(u, i) = \\frac{\\sum_{u&#39;\\in\\Omega_i}sim(u, u&#39;)r(u&#39;, i)}{\\sum_{u&#39;\\in\\Omega_i}sim(u, u&#39;)}\\] 유저간의 유사도를 가중치로 활용 rating의 평균을 활용하는 것은 동일하지만 가중치를 활용한 가중평균을 사용 유저의 성향을 반영한 정보 활용 단순히 평점의 수치를 그대로 활용하므로 각 유저의 평점 분포를 반영하지 못한다는 단점 존재Relative Rating\\[\\begin{aligned}dev(u, i) = r(u,i) - \\bar{r_u} \\quad for \\; known \\; rating \\quad \\\\\\\\\\widehat{dev}(u, i) = \\frac{\\sum_{u&#39;\\in\\Omega_i}dev(u&#39;, i)}{|\\Omega_i|} = \\frac{\\sum_{u&#39;\\in\\Omega_i}r(u&#39;, i) - \\bar{r_{u&#39;}}}{|\\Omega_i|} \\\\\\\\\\hat{r}(u, i) = \\bar{r_u} + \\widehat{dev}(u, i) \\qquad\\qquad\\qquad\\end{aligned}\\] Average나 Weighted Average 같은 방식은 Absolute Rating방식으로 유저의 평점을 그대로 사용하는 방식 이런 Absolute Rating 방식은 유저의 평점 분포를 고려하지 않는다는 문제점이 존재함 이를 해결하고자 유저의 평점 편차( $dev(u,i)$ )를 활용한 상대적 평점이 등장 특정 유저의 평균 평점에 다른 유저들의 예측 아이템에 대한 평점 편차를 더해서 유저 평점을 보정하는 방식 추가적으로 weighted average 방식도 함께 고려한 최종 식은 아래와 같음\\[\\hat{r}(u, i) = \\bar{r_u} + \\frac{\\sum_{u&#39;\\in\\Omega_i}sim(u, u&#39;)\\{r(u&#39;,i) - \\bar{r_{u&#39;}}\\}}{\\sum_{u&#39;\\in\\Omega_i}sim(u,u&#39;)}\\]" }, { "title": "[BoostCamp AI Tech] Day32", "url": "/posts/day32/", "categories": "NAVER BoostCamp AI Tech, Review", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech", "date": "2022-03-07 23:00:00 +0900", "snippet": "Day32 Review당신은 오늘 하루 어떻게 살았나요? 첫 추천 시스템 강의 수강오늘 하지 못한 것들 Kaggle EDA 논문 리딩내일은 어떤 것을 할까? Kaggle Data EDA 강의 3~4강 기본과제1 Class-Based n-gram Models of Natural Language 리딩 및 정리마무리 이번주부터 본격적으로 추천트랙 시작!!! 컨디션 관리, 시간관리 잘 하자 내용 정리는 핵심을 잘 뽑아내는 걸로 하자. 첫날에 역시나 분량 조절 실패 주말에 반드시 MLOps 내용정리" }, { "title": "[BoostCamp AI Tech / RecSys] Day32 - Content Based Recommendation", "url": "/posts/day32_recsysbasic4/", "categories": "NAVER BoostCamp AI Tech, Level 2 - 추천 시스템 이론", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech, Recommender System", "date": "2022-03-07 19:22:00 +0900", "snippet": "RecSys : Content Based RecommendationContent-based Recommendation 유저가 과거에 선호한 아이템을 기반으로 비슷한 아이템을 유저에게 추천 장점 다른 유저의 데이터가 필요하지 않으므로 서비스 초반에 활용하기 좋음 새로운 아이템 또는 인기도가 낮은 아이템 추천 가능 (long-tail problem 극복가능) 추천 아이템이 explainable해짐 $\\rightarrow$ 아이템의 특성이 곧 추천의 이유 단점 아이템의 적합한 feature를 찾기 어려움 특정 concept의 추천 결과만 나올 수 있음 (overspecialization) 다른 유저의 데이터를 활용할 수 없음 Item Profile 추천을 위해서는 item의 특성을 활용하므로 item의 feature로 구성된 profile을 만들어야 함 각 item의 특성을 살리는 것이 중요 (ex. 영화 장르, 음악 장르 등…) 일반적으로 아이템 feature는 vector로 표현TF-IDF\\[\\begin{aligned}TF(w, d) = freq_{w,d}\\quad \\text{ or }\\quad \\frac{freq_{w, d}}{\\underset{k}{max}(freq_{k,d})}\\qquad\\qquad \\\\IDF(w) = \\log\\frac{N}{n_w} \\quad \\text{(N: 전체 문서 수, n_w: w가 등장한 문서 수)}\\end{aligned}\\]\\[TF-IDF(w,d) = TF(w,d) \\cdot IDF(w)\\] TF는 단어 w가 문서 d에 등장하는 횟수 TF의 경우 특정 단어w가 문서 d에 얼마나 많이 등장하는가? 를 의미 IDF는 전체 문서 중 단어 w가 등장한 비율의 역수 단순히 TF만 처리를 하면 영어는 is, a, the 같은 관사나 be동사가 극단적으로 많이 나타나고, 한국어는 조사가 많이 등장하는 문제가 발생 이를 해결하고자 IDF를 도입하는데, IDF는 특정 단어 w가 전체 문서 D에서 얼마나 적게 등장하는가? 를 의미함 smoothing을 위해 log 연산 처리 TF-IDF의 연산 특성상 단어 w가 문서 d에서는 많이 등장하면서 전체 문서 D에서는 적게 나타날수록 곱연산의 값이 커지게 됨$\\rightarrow$ 단어 w가 특정 문서 d를 잘 표현한다고 볼 수 있음 대략적으로 d1에 대한 각 단어들의 TF-IDF를 계산하면 이 값은 d1의 vector$\\rightarrow v_{d1}=(0.6, 1.2, 0.12, 0, 0, 0)$User Profile 기반 추천 item profile이 모두 구축되었다면 user profile을 구축해서 유저의 선호 리스트를 파악해야함 각 유저의 과저 item list의 item vector들을 통합하면 user profile이 됨 단순한 방법 : item vector의 평균계산 유저의 평점이 존재하면 평점을 가중치로 정규화한 평균을 활용 만약 위의 item profile에서 유저가 d1과 d3를 선택하였고 rating이 각각 3, 5로 평가했다고 하면 다음과 같이 user profile을 생성 가능\\[\\begin{aligned}\\text{simple vector} = \\frac{v_{d1}+v_{d3}}{2} \\quad\\quad \\\\\\text{variant vector} = \\frac{r_{d1}v_{d1} + r_{d3}v_{d3}}{r_{d1}+r_{d3}}\\end{aligned}\\] 이렇게 구한 User profile과 item vector 간의 유사도를 분석해서 item을 추천함Cosine Similarity\\[\\cos(\\theta) = \\cos(X, Y) = \\frac{X \\cdot Y}{|X| |Y|} = \\frac{\\sum_{i=1}^{N} X_iY_i}{\\sqrt{\\sum_{i=1}^{N} X_{i}^2} \\sqrt{\\sum_{i=1}^{N}Y_i^2}}\\] 두 벡터의 내적원리를 이용하여 두 벡터의 cos 거리를 계산함 cosine 함수의 성질에 따라 두 벡터의 방향이 같을수록 1에 가깝고 다를수록 -1에 가까움 이를 활용하여 user와 item의 점수를 환산해서 아이템을 추천\\[score(u, i) = \\cos(u, i) = \\frac{u \\cdot i}{|u| \\cdot |i|}\\]User Rating 예측\\[\\begin{aligned}sim(i, i&#39;) = \\cos(v_i, v_{i&#39;}) \\qquad\\quad \\\\prediction(i&#39;) = \\frac{\\sum_{i=1}^{N} sim(i, i&#39;)\\cdot r_{u,i}}{\\sum_{i=1}^{N} sim(i, i&#39;)}\\end{aligned}\\] 새로운 아이템 $i’$이 주어졌을 때 해당 아이템의 평점을 예측할 수 있음 variant vector와 유사하게 유저가 매긴 평점이 가중치로 들어간 가중평균처리가 됨 이 경우 m4 = (0.4, 1.4, 3.1, 1.0)인 새로운 아이템이 도입될 경우 유사도를 구한 것이다. 각 아이템은 (3.0, 2.5, 4.0)으로 평점을 유저가 준 경우 새로운 아이템 m4의 예측 평점은 다음과 같다.def cal_cosine(x, new): val1 = np.dot(x, new) val2 = norm(x) * norm(new) return round(val1/val2, 2)data = { &quot;v1&quot;:[0.2, 0.4, 0.3], &quot;v2&quot;:[0.4, 0.7, 1.2], &quot;v3&quot;:[1.2, 0.3, 1.0], &quot;v4&quot;:[1.5, 0.5, 1.0],}df = pd.DataFrame(data, index=[&#39;m1&#39;, &#39;m2&#39;, &#39;m3&#39;])m4 = np.array([0.4, 1.4, 3.1, 1.0])df[&#39;cos&#39;] = df.apply(lambda x: cal_cosine(x, m4), axis=1)sim = df[&#39;cos&#39;].valuesrating = np.array([3.0, 2.5, 4.0])print(f&quot;M4&#39;s rating: {((sim@rating)/np.sum(sim))}&quot;)" }, { "title": "[BoostCamp AI Tech / RecSys] Day32 - 추천 시스템 종류와 연관 분석", "url": "/posts/day32_recsysbasic3/", "categories": "NAVER BoostCamp AI Tech, Level 2 - 추천 시스템 이론", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech, Recommender System", "date": "2022-03-07 17:22:00 +0900", "snippet": "RecSys : 추천 시스템 종류와 연관 분석추천 시스템 종류 Simple Aggregation (popularity, average score, recent) Association Analysis Content-based Recommendation Collaborative Filtering Item2Vec Recommendationn and ANN Deep Learning-based Recommendation Contex-aware Recommendation Multi-Armed Bandit(MAB)-based Recommnedation 여기서 1~4는 일반적으로 classic method고 5~7은 DL을 사용하는 방식인데 현재 추천 시스템에서는 DL이 드라마틱한 성능을 보이지는 않고 있음 또한 추천이라는 컨셉상 inference time이 길면 오히려 만족도가 떨어질 수 있으므로 DL을 자주 사용하진 않음연관 분석 (association analysis) Transaction Items 1 {맥주, 우유} 2 {빵, 기저귀, 맥주, 계란, 우유} 3 {우유, 기저귀, 맥주, 콜라} 4 {빵, 우유, 기저귀, 맥주} 5 {빵, 우유, 기저귀, 콜라, 계란} 연관 분석이란 장바구니 분석 혹은 서열 분석이라 함 상품 구매, 조회 등 연속적인 거래에서 규칙을 발견하기 위해 적용 위의 거래 표를 기준으로 아래 예시들을 이해하면 좋음연관 규칙 분석\\[X \\rightarrow Y \\text{가 존재할 때, (X, Y: itemset, N: 전체 transaction 수)}\\] 연관 규칙 주어진 transaction에서 하나의 상품이 등장시 다른 상품의 등장 규칙을 찾는 것 규칙 (rule)의 구조IF (condition) THEN (result) : {condition} $\\rightarrow$ {result} 연관 규칙 (association rule)의 구조IF (antecendent) THEN (consequent)특정 사건이 발생했을 때 함께 빈번하게 발생하는 또 다른 사건 규칙을 의미 Itemset antecedent와 consequent 각각을 구성하는 상품들의 집합 antecedent와 consequent는 서로소를 만족함ex) antecedent: {빵, 버터}, consequent: {우유} support count($\\sigma$) 전체 transaction data에서 itemset이 등장하는 횟수 support itemset이 전체 transaction data에서 등장하는 비율 공식은 일반적으로 $\\text{support count}/\\text{# of transaction}$ 빈발 집합 (Frequent Itemset) 유저가 지정한 minimum support (threshold) 이상의 itemset을 의미 반대 성향은 infrequent itemset이라 함 Support\\[\\begin{aligned} s(X) = \\frac{n(X)}{N}=P(X) \\geq s(X \\rightarrow Y) = \\frac{n(X \\cup Y)}{N} = P(X \\cap Y)\\end{aligned}\\] 연관 규칙에서 자주 사용되는 척도 중 하나 두 itemset X, Y를 모두 포함하는 transaction의 비율= 전체 transaction에 대한 itemset의 확률값 좋은 규칙을 찾거나, 불필요한 연산을 줄일 때 사용 Support에 나타나는 $n(X \\cup Y)/N$은 확률의 교집합과 동일한 개념 집합의 관점으로 바라보면 이 의미는 X가 있는 동시에 Y가 있는 집합이라는 의미 Confidence\\[c(X \\rightarrow Y) = \\frac{n(X \\cup Y)}{n(X)} = \\frac{s(X\\rightarrow Y)}{s(X)} = \\frac{P(X \\cap Y)}{P(X)} = P(Y|X)\\] X가 포함된 transaction 중 Y도 초함하는 transaction 비율 confidence가 높을수록 유용한 규칙Lift\\[l(X \\rightarrow Y) = \\frac{P(Y|X)}{P(Y)} = \\frac{P(X \\cap Y)}{P(X)P(Y)} = \\frac{s(X\\rightarrow Y)}{s(X)s(Y)} = \\frac{c(X \\rightarrow Y)}{s(Y)}\\] support와 confidence와 다르게 1을 기준으로 비교 lift = 1 : X, Y는 독립 lift가 1보다 크면 양의 상관관계, 작으면 음의 상관관계연관 규칙 사용 item 수가 많아질수록 rule의 수가 너무 많아지므로 유의미한 rule만 사용 minimum support, minimum confidence로 의미 없는 rule filtering lift 값의 내림차순으로 의미있는 rule 평가 이는 lift가 antencedent와 consequent의 연관분석 값을 의미하므로 큰 값일수록 둘의 상관관계가 높음 lift는 user의 질적 만족도와 관련성이 높음 연관 규칙 탐색 transaction이 주어진 경우에 가능한 연관 규칙을 찾는 방법 Brute-force approach 가능한 모든 연관 규칙에 대해 support와 confidence를 계산 모든 case를 탐색하므로 계산량이 상당함 $Complexity \\sim O(NW M), M = 2^d \\text{(d: # of unique items)}$ brute-force 방식의 문제점을 해결하고자 다양한 방법이 도입 rule mining 과정에서 많은 cost가 들어가는 부분은 minimum support 이상의 모든 itemset을 생성하는 것이므로 이 부분의 cost를 줄일 필요가 있음 Apriori 알고리즘 : 가지치기를 활용하여 탐색하는 M을 줄임 Direct Hashing &amp;amp; Pruning (DHP) 알고리즘 : itemset 크기가 커지면 전체 N개 transaction보다 적은 개수 탐색 FP-Growth 알고리즘 : 호율적 자료구조를 활용하여 후보 Itemset과 transaction 저장 " }, { "title": "[BoostCamp AI Tech / RecSys] Day32 - 추천 시스템 평가 지표와 인기도 추천", "url": "/posts/day32_recsysbasic2/", "categories": "NAVER BoostCamp AI Tech, Level 2 - 추천 시스템 이론", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech, Recommender System", "date": "2022-03-07 15:22:00 +0900", "snippet": "RecSys : 추천 시스템 평가 지표와 인기도 추천추천 시스템의 목표 추천 시스템의 목표는 유저에게 필요한 아이템을 추천하는 것 이를 위해서는 유저와 관련된 아이템들의 랭킹을 산정하거나 아이템 선택을 예측할 필요가 있음 어떤 케이스의 문제를 처리하느냐에 따라 사용하는 평가 지표가 다름 추천 모델의 성능 평가는 단순히 모델적 지표만 사용하기엔 무리가 있기 때문에 비즈니스/서비스적 관점의 지표를 더 중요하게 확인함 매출, PV(Page View)증가 유저의 CTR(Click Through Rate) 상승 품질적인 관점의 지표도 많이 고려함 연관성 : 추천 아이템과 유저의 연관성 다양성 : Top-K의 아이템이 얼마나 다양한 아이템인가? 새로움 : 얼마나 새로운 아이템이 추천되는가? (반복적인 아이템 추천은 유저 만족도 감소) 참신함 : 유저가 생각하지 못하거나 기대하지 않은 뜻밖의 아이템이 추천되는가? (연관성과 상충되는 요소가 있을 수 있음) 오프라인 테스트 오프라인 테스트(Offline Test)는 새로운 추천 모델을 검증할 때 가장 우선적으로 수행되는 단계 수집 데이터를 train/valid/test로 나누어 성능 지표 판단 offline test에서 좋은 성능을 보여야 서빙에 투입되지만 실제 서비스에서는 다양한 상황이 나타나는 serving bias가 발생 랭킹과 예측문제에 따라 평가 지표가 다름랭킹 문제 Precision / Recall @K\\[\\begin{aligned} \\text{Precision@K} = \\frac{\\text{# of user preferred items in recom list}}{K} \\\\ \\text{Recall@K} = \\frac{\\text{# of user preferred items in recom list}}{\\text{# of total user preferred items}} \\end{aligned}\\] Precision@K : 추천한 K개 아이템 중 실제 유저가 관심있는 아이템의 비율 Recall@K : 유저가 관심있는 전체 아이템 중 추천한 아이템의 비율 Mean Average Precision(MAP)@K\\[\\begin{aligned}\\text{AP@K} = \\frac{1}{m}\\sum_{i=1}^{K}\\text{Precision@}i \\\\\\text{MAP@K} = \\frac{1}{|U|}\\sum_{u=1}^{|U|}(\\text{AP@K})_{u}\\end{aligned}\\] AP@K는 Precision@1부터 Precision@K까지의 평균값 관련 아이템을 더 높은 순위에 추천할수록 점수 상승 MAP@K는 모든 유저에 대한 AP@K를 평균한 것 Normalized Discounted Cumulative Gain(NDCG)\\[\\begin{aligned} \\text{CG}_{K} = \\sum_{i=1}^{K} rel_{i} \\qquad \\\\ \\text{DCG}_{K} = \\sum_{i=1}^{K}\\frac{rel_i}{\\log_2(i+1)} \\\\ \\text{IDCG} = \\sum_{i=1}^{K}\\frac{rel_{i}^\\text{opt}}{\\log_{2}{(i+1)}} \\\\ \\text{NDCG} = \\frac{\\text{DCG}}{\\text{IDCG}} \\qquad\\end{aligned}\\] Cumulative Gain (CG) : 상위 K개 아이템에 대한 관련도를 합한 것 순서에 따른 discount는 적용하지 않음 Discounted CG (DCG) : 순서에 따라 CG를 discount Idea DCG (IDCG) : 이상적인 추천이 발생했을 시 DCG값, 가능한 DCG 중 제일 큰 값 Normalized DCG (NDCG) : 추천 결과에 따라 구해진 DCG를 IDCG로 나눈 값 Online A/B Test offline test에서 검증된 가설이나 모델을 이용해서 실제 추천 결과를 서빙하는 단계 적용 전후를 비교하는 것이 아닌 동시에 비교군과 대조군을 적용해서 성능을 평가 측정하고자하는 부분 제외하고 다른 환경은 최대한 동일하게 유지해야함 현업에서는 결국 의사결정시 모델 성능보다 매출, CTR등 비즈니스/서비스 지표를 활용함인기도 기반 추천 시스템 이름 그대로 인기있는 아이템을 추천 인기도의 척도는 갖고있는 아이템이 무엇인가에 따라 결정됨 인기도 척도를 선정하는 과정에선 다음을 고려 Most Popular (많은 조회수) Highly Rated (높은 평점) Recently (최신성) 뉴스와 같은 경우 가장 중요한 것은 최신성이므로 최신 정보를 잘 처리하는 것이 좋음Hacker News Fromula\\[score = \\frac{pageviews - 1}{(age+2)^{gravity}}\\] 뉴스는 많은 조회와 최신 정보라는 2가지가 모두 고려되어서 추천해야 함 기본적으로 오래된 뉴스는 높은 조회수를 기록할 수 밖에 없으므로 이를 고려해서 시간을 discount하는 요소로 활용해야 함 시간에 따라 줄어들게 score를 조정하고자 gravity 상수를 사용Reddit Formula\\[score = \\log_{10}(\\max(ups - downs, 1)) + \\frac{sign(ups-downs)\\cdot seconds}{45000}\\] 실제 reddit에서 사용하는 포스트 스코어 2개의 term으로 이루어진 공식으로 첫번째 term은 popularity, 두번째 term은 포스팅 게시된 절대 시간 나중에 게시된 포스팅일수록 절대시간이 크므로 높은 score를 가짐 첫항의 log term에 의해 초반부 vote가 높은 가치를 갖게됨 오래된 포스트일수록 아주 많은 vote가 있어야 높은 score가 생성 첫항에서 down vote가 더 많을 때를 방지하고자 down vote가 더 많으면 1로 세팅 참고링크 Steam Rating Formula\\[\\begin{aligned}\\text{avg_rating} = \\frac{\\text{# of positive reviews}}{\\text{# of reviews}} \\qquad\\qquad \\\\score = \\text{avg_rating}-(\\text{avg_rating - 0.5})\\cdot 2^{-\\log(\\text{# of reviews})}\\end{aligned}\\] rating의 평균치를 사용하되, 전체 review 수에 맞춰 rating 보정 review가 너무 적으면 평균 rating에서 수치를 보정함 review 개수가 많을 경우 $2^{\\log}$ 항이 0에 가까워지므로 보정값의 영향이 없어져서 평균 rating과 유사해짐" }, { "title": "[BoostCamp AI Tech / RecSys] Day32 - 추천 시스템 개념", "url": "/posts/day32_recsysbasic1/", "categories": "NAVER BoostCamp AI Tech, Level 2 - 추천 시스템 이론", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech, Recommender System", "date": "2022-03-07 14:40:00 +0900", "snippet": "RecSys : 추천 시스템 개념추천 시스템이란? 현재 많은 서비스들은 다양하고 많은 item으로 구성 item을 소비자에게 제공하는 것이 핵심이고 제공 방법은 크게 2가지 방식이 존재 pull 방식 : 소비자가 직접 서비스에 searching과 같은 방식을 통해 자신의 의도를 전달하여 item을 탐색 push 방식 : 서비스가 소비자에게 어울리는 item을 추천 및 제공하는 방식 과거에 비해 현재는 정보가 과다하게 많은 시대라 사용자가 직접 item을 탐색하는 방식은 시간이 오래 걸림$\\rightarrow$ 사용자의 만족도 감소Long tail phenomenon 많은 정보 속에 인기있는 소수의 item만 선택적으로 소비되기 때문에 Long tail phenomenon이 발생하고 이는 item의 수가 늘어날 수록 꼬리의 길이는 길어짐 이런 long-tail 부분의 item을 추천해주는 것이 추천 시스템의 본질이자 개인화 추천의 핵심 목적 (long-tail recommendation) long-tail recommendation이 잘 될수록 소비자는 개인화 추천이 잘 되었다고 느끼고 높은 만족도를 느낌ex1) 알 수 없는 유튜브 알고리즘이 적은 조회수의 영상을 추천해주는 경우ex2) SNS는 유명인을 추천해주는 것도 좋지만 자신과 실제로 관련있는 사람들만 추천해주는 것이 더 만족도가 높음추천 시스템의 데이터 구조유저 관련 정보 유저 profiling : 유저에 관련된 정보를 구축하여 개별 혹은 그룹으로 추천 제공 식별자 (identifier) : 유저 ID, device ID (광고추천), 브라우저 쿠키 (비로그인 상태) 데모그래픽 정보 : 성별, 연령, 지역, 관심사 등 유저에 직접적으로 관련이 깊은 정보 직접 수집을 하는 것이 가장 좋지만 최근에는 개인정보 issue로 수집이 어려움 성별, 연령은 다른 데이터를 통해 추정값으로 사용하는 경우도 있음 유저 행동 정보 페이지 방문 기록, 아이템 평가, 구매 등의 feedback 기록 아이템 관련 정보 추천 아이템 종류 : 포탈, 광고, 미디어 등 사용되는 환경에 따라 아이템의 종류는 달라짐 아이템 profiling : 아이템도 유저처럼 profiling을 진행 Item ID : 아이템 구분을 위한 식별자 Item metadata : 아이템의 고유 정보이며 영화 장르, 상품 카테고리 등 해당하는 아이템이 갖고 있는 고유한 특성과 정보를 의미 유저-아이템 상호작용 정보 (Feedback) Explicit Feedback 유저가 직접 아이템에 대한 평가(만족도)를 기록한 경우ex) 평점 Implicit Feedback 유저가 직접적으로 아이템에 대한 선호 강도를 표시하진 않았지만 아이템을 클릭, 접속 시간, 구매와 같이 선호와 관련된 행동을 보여준 경우ex) 유저의 상품 구매 여부가 Y로 설정되는 경우 일반적으로 explicit보다 implicit data가 더 많기 때문에 implicit data에 대한 연구가 많이 이뤄지고 있음" }, { "title": "[BoostCamp AI Tech / P Stage 1] Day31 - Project Day Final", "url": "/posts/day31_pstage_10/", "categories": "NAVER BoostCamp AI Tech, P Stage", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech, Image Classification, Project", "date": "2022-03-04 22:00:00 +0900", "snippet": "P Stage 1 : Project Day FinalDay Final list 대회 종료 데이터 분포 변경 및 앙상블대회종료드디어! 2주간 진행된 대회가 종료되었습니다. 저희 팀은 나름 기적?을 보여줬다고 생각합니다.대회 종료 2시간 전까지만 해도 48개 팀 중 47등이었는데…. 최종 10등으로 마무리했습니다. (private 13등) 2시간만에 37계단 상승을 보여줬습니다.방법은 생각보다 간단했습니다. 자세한 내용은 2번에서 설명드리겠습니다.데이터 분포 및 앙상블사실 제일 큰 문제는 데이터의 분포였습니다. 제가 마지막으로 제출한 모델의 결과가 0.7000으로 나왔고 다른 팀원이 제출한 모델은 0.6 후반대가 나왔습니다. 왜 결과가 향상이 안되는지가 저희 팀의 가장 큰 문제점이었습니다. 모델변경, 데이터 증강 등 해볼 수 있는 대부분의 방법은 다 시도해봤기 때문입니다.그러던 중 팀원이 상위 2개 제출의 결과를 실제 테스트와 비교를 해봤을 때, 제 모델은 연령을 제외한 마스크, 성별은 거의 다 맞추는 반면, 다른 팀원의 결과는 연령은 매우 잘 맞추고 있었습니다. 그래서 역연산을 통해 성별, 마스크, 연령을 각 모델에서 추출하여 재연산을 통해 결과를 도출했더니 점수 향상이 매우 높게 나타났습니다.또한 60세이상의 데이터가 너무 적은 것이 문제이고 잘 맞추지 못한다는 문제가 나타나서 60세이상의 분포는 58세까지로 분포를 변경했더니 점수 향상이 굉장히 높게 나타났습니다. (F1 score 0.0464 향상)추가로 재분류한 60세 이상 데이터를 GaussianBlur, HorizontalFlip, GridDistortion을 활용하여 증강을 진행했고 이 방법도 점수 향상이 나타났습니다. (F1 score 0.0401 향상)최종 대회 코드 모음대회 GitHub repository" }, { "title": "[Life] 코세라 MLOps 특화과정 수료!", "url": "/posts/MLOps_Special/", "categories": "Life, 일상", "tags": "coursera, MLOps, Data Engineering", "date": "2022-03-04 15:00:00 +0900", "snippet": "MLOps 특화과정부캠 시작과 거의 동시에 코세라의 MLOps 특화과정을 재정지원 신청을 통해 수강하고 있었습니다. 솔직히 같이 진행할 수 있을거라 생각했는데 제가 정리하는 걸 너무 꼼꼼하게 하다보니 부캠 정리하는데에도 시간이 엄청 걸리더라구요 ㅠㅠ하도 이게 시간을 너무 잡아먹으니 정리하는 스타일도 좀 더 간소화하고 핵심적으로 적는 형식으로 바꾸기도 했고 그러면서 주말에 몰아 듣고 하는 방식으로 했습니다.더 힘든건 이거 재정지원을 받으면 그때부터 바로 강의가 시작이라 실수로 특화과정 1~4를 동시에 수강해야하는 불상사가 발생해버렸습니다.솔직히 이걸 들어면서 다 정리하자!라는 건 1주차가 끝나고 욕심이란걸 깨닫고 일단 강의를 다 들으면서 필기로 정리를 하고 블로그 정리는 나중에 하자!라는 방식으로 바꿨습니다. 일단 그렇게 하니까 강의 몰입도도 높아지고 정리 자체도 집중하게 될 수 있었습니다. 무엇보다 힘들었던건 강의 자체가 영어라 영어자막을 보면서 진행했는데, 이게 음성인식으로 자막이 만들어진건지 발음이 살짝 애매한 부분은 완전히 다르게 나오더군요…;;예를 들면 문맥상 보면 parallelize인데 paralyze라고 한다던가…. (그 해석이 안되서 번역기를 돌렸더니 자꾸 뭘 마비시킨다고해서 다시 들어보니까 parallelize였던….)MLOps Specialization 수료4~5달을 들어야하는 특화과정을 저의 실수로 그만 한달만에 끝내버리는 기염을 보여줬습니다. 내용자체가 엄청 어려운 내용이 있었다기보단 좀 더 스스로 해보면 좋을 내용들이 많았습니다.블로그에 정리하면서 하기에는 일정이 너무 빡빡해서 1주차 정도만 올려놨는데 이제 슬슬 주말에 한주차씩 정리해서 올려야할 듯 합니다. 다음 coursera specialization은 Learn SQL Basics for Data Science를 들어볼까 생각중입니다. SQL 자체를 못하는건 아니지만 자유자재로 쓸 정도로 잘한다고 보기도 어렵기 때문에 배워두면 충분히 좋을 것으로 생각됩니다." }, { "title": "[BoostCamp AI Tech / P Stage 1] Day28 - Project Day 9", "url": "/posts/day29_pstage9/", "categories": "NAVER BoostCamp AI Tech, P Stage", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech, Image Classification, Project", "date": "2022-03-02 22:00:00 +0900", "snippet": "P Stage 1 : Project Day 9Day 9 list K-Fold 원본 이미지 학습 K-Fold 원본을 400, 200 resize원본 이미지 400 x 200 - resize1번 모델 학습 진행하는데 너무 오랜시간이 걸려서 2번 모델을 겨우겨우 하루 지나기전에 처리했는데 확실히 성능향상이 있었습니다.근데 저게 뭔 말이냐….간단히 설명하면 저희에게 주어진 이미지가 불필요한 배경도 포함하고 있다보니 모델 학습에 영향을 주는 것으로 판단했습니다. 따라서 주변 환경을 최대한 배제하는 방향으로 이미지를 CenterCrop했습니다. 그리고 CenterCrop이전에 이미지 비율을 절반으로 줄여서 진행하였습니다.그리고 learning rate를 기존에는 0.001(1e-3) ~ 0.0001(1e-4)로 진행했던 것을 좀 더 낮춰보자는 의견이 있어서 1e-5로 수행하였습니다.또한 Clova에서 개발한 AdamP를 활용해서 optimizer 수정을 진행하였습니다.다행히도 이전 제출 0.6693에서 0.6869로 점수 향상이 있었습니다.현재 진행하고 있는것은 이미지를 원본비율로 조정해서 위 방식을 도입하는 것으로 실험을 진행 중이며 wandb를 통해 확인되는 것은 전반적인 앙상블(fold)에서 높은 validataion f1 score와 낮은 validation loss가 관찰되고 있습니다. 이전 모델들에서는 첫번째 fold에서 모델이 좋지 못한 성능을 내고 있었는데 이번에는 첫번째 폴드부터 좋은 성능을 보이고 있습니다.다음 진행할 것은 ImbalancedDatasetSampler로 샘플링을 좀 조정할 생각입니다. 문제는 이걸 적용하려면 여러가지 수정을 해야하는데…. 잘 적용될지 모르겠습니다 ㅠㅠ" }, { "title": "[BoostCamp AI Tech / P Stage 1] Day28 - Project Day 8", "url": "/posts/day28_pstage6/", "categories": "NAVER BoostCamp AI Tech, P Stage", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech, Image Classification, Project", "date": "2022-02-28 22:00:00 +0900", "snippet": "P Stage 1 : Project Day 8Day 8 list K fold 5개로 각 250 epoch으로 앙상블 진행 mask, age, gender 각각 훈련 모델 코딩1번은 큰 내용 없으므로 2번으로 넘어갑니다.Mask, Age, Gender 각각 훈련 모델이전에 Day 4에서 언급한 모델의 각각 분리 형태 분류를 시도해봤으나 성과가 그렇게 좋지는 못했습니다.그러던 중 떠오른게 그냥 각 모델이 csv파일 내보내면 그걸 합치면 되지 않을까?라는 생각으로 각각으로 k-fold 학습을 하는 앙상블 모델을 코딩했습니다.위 그림을 1개의 모델에서 처리를 했더니 모델 자체의 부담도 너무 크고 여러가지로 속도면에서도 메리트가 없어서 각각 결과물을 도출하게 코딩을 했습니다.문제는 일요일에 사진을 wandb로 전달해서 제대로 맞추지 못하는 유형을 파악한 결과 연령을 잘 맞추지 못하는 경향이 강했습니다.특히 연령대 구분인 30대 부근과 60대 부근의 이미지가 들어오면 정확도가 낮아졌으므로 나이를 조금 더 세분화할 필요가 있다고 생각했습니다.새롭게 설계한 모델은 연령 구간을 총 10구간으로 세분화하여 모델이 학습하고 세분화된 구간을 다시 큰 구간의 원본 구간으로 변환하는 것입니다. 이렇게하면 분절된 구간의 값을 좀 더 세분화하게 분석이 가능할 것으로 보입니다.현재 큰 학습을 진행중이라 현재 진행중인 학습 종료 후 테스트 가능할 것으로 보입니다." }, { "title": "[BoostCamp AI Tech / P Stage 1] Day27 - Project Day 5-6", "url": "/posts/day27_pstage5/", "categories": "NAVER BoostCamp AI Tech, P Stage", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech, Image Classification, Project", "date": "2022-02-26 22:00:00 +0900", "snippet": "P Stage 1 : Project Day 5-6Day 5-6 list Baseline code로 좋은 나만의 코드 템플릿 만들기Baseline code 분석dotenv 서버와 관련된 접속 정보를 숨길 때 사용 서버상에 환경변수로 설정하거나 설정된 값을 가져올 때 사용 python 뿐 아니라 javascript에서도 사용가능 일반적으로 많이 쓰는 방법은 .env 파일에 다양한 환경변수를 설정하고 .gitignore에 파일 추가해서 깃허브 업로드는 막지만 로컬에서는 동작할 수 있게 하는듯 개인적으로 프로젝트 진행할 때마다 이 부분에 대한 고민이 많았는데 이런 방법이 있었군….argument 대한 고찰 파이썬 머신러닝 코드는 argparse를 통해 argument를 받아서 실행함 코드가 생각보다 길어지지만 불필요한 내용이므로 utils.py에 따로 빼서 관리하는 것이 좋아보임 개인적으로 가끔 사용하는 경우인데 그때그때 생각나는 argument가 있을때마다 새로 추가하는 경우도 많아서 사용처에 따라 묶어서 적어두는 것이 좋다고 생각 예를 들면 모델, 데이터는 같이 적어두고 모델의 hyperparameter 세팅끼리 묶어두는 것이 좋다고 생각$\\rightarrow$ 이후 코드를 받아보는 다른 사람이 읽기 쉬울듯 lr_scheduler에서 사용되는 learning rate decay 값은 lambda로 지정되어 있는데 이는 lr_decay랑 lambda 중 개인적으로는 lambda가 좋다고 생각하는데 겹치는 문제가 있을 수 있음 argument에 리스트를 입력하는 경우가 있을 수 있는데, 이때는 인자를 다음과 같이 주면 된다.parser.add_argument(&quot;--resize&quot;, nargs=&quot;+&quot;, type=int, default=[512, 384]) argument에서 boolean type은 잘 안 받아지는데 이때는 다음과 같이 작성하자from disutils.util import strtoboolparser.add_argument(&#39;--record&#39; type=lambda x: bool(strtobool(x)), defulat=False)docstring과 typing, annotation docstring을 사용하는 것이 좋아보임. 나 혼자 쓰거나 많은 사람들이 쓰진 않아도 버릇을 미리 들여놓는게 좋을듯 부캠에서 프로덕트 서빙 조교님께서 typing 모듈과 annotation의 활용을 버릇화 하는게 좋다고 하셨다. 참고해서 연습해야겠음 파이참은 이걸 자동 지원해줘서 넘 좋다 ㅎㅎ 난 구글 갈 거니까 Google Style로 했음Dataset과 DataLoader에 대한 구조적 이해 Dataset의 인자로는 data_dir, mean, std, val_ratio가 존재 data_dir은 데이터(이미지)가 존재하는 최상위 폴더 데이터 파일을 다루는 부분에 있어서 os 라이브러리를 잘 활용하는 쪽으로 가는 것이 좋을 것 같음. 단순 string 연산으로는 좀 무리가 있을 듯 os.path.join os.path.splittext class에 Enum 타입을 활용하는 경우가 있음 annotation을 사용하는 경우가 많음 @classmethod : 클래스 내에 공유되는 변수와 class method, static method에 접근 가능하게 해줌 클래스 내부에 있을 필요는 없지만 연관성에 대한 가독성을 높이고자 클래스 내부에 작성 @staticmethod를 활용해서 클래스의 encoding, decoding을 해주는 메서드를 구현함$\\rightarrow$ 이건 잘 기억해두자 : 모든걸 외부 함수로 할 필요없이 클래스 내부에 메서드로 만들어도된다. 나는 데이터 셋에 모든 데이터를 데이터 프레임가 같은 통합적 형태로 다룰려고 했는데 baseline을 보면 ‘이미지 경로’, ‘마스크 라벨’, ‘성별 라벨’, ‘연령 라벨’로 따로 분리해서 핸들링하고 있다는 것을 알 수 있음 사실상 순서만 같으면 무관하므로 이렇게 관리하는게 더 좋을듯 네이밍 컨벤션은 보통 데이터는 단수, 데이터를 담는 리스트는 복수로 작성하는 것이 좋음 옛날엔 훈련과정이랑 validation 과정에서 동일한 변수명을 썼는데 train, val은 구분해서 변수를 해주는 것이 좋을 것 같음모듈 관리 모듈을 관리하는 과정에서 getattr을 활용한 importlib의 import_module을 자주 활용함 argument를 다량으로 받아올 수 있게 설정하고 다양한 optimizer, loss function, scheduler를 실험자가 마음대로 커스터마이징이 가능하게 함 결과적으로 코드를 일반화하기에 아주 좋은 세팅임 개인적으로 이번 베이스라인에서 가장 좋은 코드 일반화 소득Scheduler Hyperparameter 모델이랑 optimizer는 사용자가 실행시 argument로 자유롭게 설정이 가능함 Scheduler는 그렇지 않아서 이것도 가능하게 세팅 진행 진행과정 문제는 scheduler별로 가지고 있는 parameter가 다르다는 것 처음에는 argparse에 모든 가능한 argument를 추가하려 했으나 이는 사실상 말이 안되는 과정 $\\rightarrow$ 찾아보니까 내부 파라미터는 너무 많다. 해결방법 어차피 해당 스케쥴러 사용자는 들어가는 파라미터를 알고 있을 것이므로 사용자가 직접 파라미터를 입력하게 하면된다. 따라서 argparse에는 딱 한줄만 추가 parser.add_argument(&#39;--sch_params&#39;, dest=&#39;sch_params&#39;, action=StoreDictKeyPair, metavar=&quot;KEY1=VAL1,KEY2=VAL2&quot;, help=&#39;Your scheduler parameters&#39;, required=True) 여기서 사용된 StoreDictKeyPair는 커스텀 클래스로 코드는 다음과 같음 class StoreDictKeyPair(argparse.Action): def __call__(self, parser, namespace, values, option_string=None): my_dict = {} for kv in values.split(&quot;,&quot;): key, value = kv.split(&quot;=&quot;) for t in (int, float): try: value = t(value) except ValueError: pass else: break my_dict[key] = value setattr(namespace, self.dest, my_dict) StoreDictKeyPair에서 문제는 argparse의 특성상 내부 인자를 문자열로 받아오는 것이었음 auto typcasting이 있었다면… 좋았겠지만 솔직히 어떤 언어가 문자열로 된 숫자가 int인지 float인지 구분 하는지… 따라서 직접 type check를 통해 처리했음 int는 float보다 훨씬 엄격하게 타입 체크가 들어가므로 우선 int 통과여부를 확인했고 만약 둘 중 1개의 타입이라도 통과된다면 바로 type casting을 진행하고 멈춤 만약 모든 타입 체크에 실패하면 무조건 문자열이므로 그대로 적용 학습경과 출력 문구 (f-string) 원래도 f-string을 자주 썼지만 좀 더 잘 활용하고자 f-string을 추가로 알아봄 문자정렬 $\\rightarrow$ 각 옵션 넣고 자릿수 설정 좌측 정렬 : f&quot;{s1:&amp;lt;10}&quot; 가운데 정렬 : f&quot;{s1:^10}&quot; 우측 정렬 : f&quot;{s1:&amp;gt;10}&quot; :은 최소문자 폭을 의미하고 열을 줄 맞춤할 때 편리" }, { "title": "[BoostCamp AI Tech] Day26", "url": "/posts/day26/", "categories": "NAVER BoostCamp AI Tech, Review", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech", "date": "2022-02-24 23:00:00 +0900", "snippet": "Day26 ReviewDay26 ~ Day 31 까지는 P Stage 카테고리를 확인해주세요!" }, { "title": "[BoostCamp AI Tech / P Stage 1] Day26 - Project Day 4", "url": "/posts/day26_pstage4/", "categories": "NAVER BoostCamp AI Tech, P Stage", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech, Image Classification, Project", "date": "2022-02-24 22:00:00 +0900", "snippet": "P Stage 1 : Project Day 4Day 4 list wandb 연결 train - validation 분리 pre transform 적용으로 데이터 로더 속도 증가 SENet 사용 Multi-head classifier 구현 CutMix 적용wandb 연결CLI 창에 뜨는 값으로만 확인하기엔…. 너무 힘들기도 하고 영 눈에 안들어오는 거 같아서 wandb에 연결했습니다. 생각보다 어렵지 않았습니다.def train(args): if args.wandb == &quot;true&quot;: config = { ... } wandb.init(project=&quot;project&quot;, name=args.name, config=cofing, entity=&quot;entity&quot;) ... # validation part ... if args.wandb == &quot;true&quot;: wandb.log({&quot;Loss&quot;: {&quot;train loss&quot;: train_loss, &quot;val loss&quot;: val_loss},&quot;F1 Score&quot;: {&quot;train f1&quot;: train_f1, &quot;val f1&quot;: val_f1}})위처럼 코드 작성했고 물론 1에포크당 validation check를 하므로 1에포크의 평균치로 기록됩니다. wandb.init에 들어가는 name은 사용자 입력으로 받는 모델 이름으로 넘겨주어서 wandb 사이트에서 어떤 모델이 어떤 지표를 나타내는지 보기 쉽게하였습니다.train-valid 분리이번 대회에서는 이미지에 대한 데이터프레임을 만들고 sklearn의 train_test_split을 쓰면 안되는 구조로 되어있습니다. 일반적으로 validation check를 위해서 데이터 스플릿을 진행하는데, 데이터가 1명에 대해 다양한 클래스로 나눠져있어서 사람별로 스플릿하지 않으면 validation check 과정에서 cheating이 발생합니다.따라서 train과 valid를 일단 사람별로 인덱스로 구분하고 그 인덱스를 Dataset에 넘겨주어 처리하는 방식을 선택했습니다.class CustomTrainDataset(Dataset): def __init__(self, df: pd.DataFrame, pre_transforms: T = None, transforms: T = None, train: bool = True): self.base_url = &#39;/opt/ml&#39; self.img_url = &#39;/input/data/train/images&#39; self.transforms = transforms self.train = train self.images = [] self.df = self.__make_dataframe(df) for path in self.df[&#39;path&#39;].values: image = Image.open(path) if pre_transforms: image = pre_transforms(image) self.images.append(image) def __make_dataframe(self, df: pd.DataFrame) -&amp;gt; pd.DataFrame: if self.train: save_name = &#39;train_df.csv&#39; else: save_name = &#39;valid_df.csv&#39; ret_df = pd.DataFrame(columns=[&#39;id&#39;, &#39;gender&#39;, &#39;age&#39;, &#39;age_group&#39;, &#39;mask&#39;, &#39;path&#39;, &#39;class&#39;]) if save_name in list(os.listdir(&#39;/opt/ml/outputs&#39;)): print(f&quot;File {save_name} is Exist!&quot;) ret_df = pd.read_csv(&#39;/opt/ml/outputs/&#39;+save_name) else: for row in df.iloc: for file in list(os.listdir(os.path.join(self.base_url+&#39;/&#39;+self.img_url, row.path))): # 보안상 생략 data = { # 보안상 생략 } ret_df = ret_df.append(data, ignore_index=True) ret_df.to_csv(&#39;/opt/ml/outputs/&#39;+save_name, index=False) return ret_df def __getitem__(self, idx): image = self.images[idx] classes = self.df.iloc[idx] if self.transforms: image = self.transforms(image) return image, classes[&#39;class&#39;] def __len__(self): return len(self.df)위 방식처럼 특정 데이터프레임을 받아오면 그 데이터프레임에 대해 처리를 전반적으로 진행합니다. 물론 데이터셋을 생성하는 과정에서 오래 걸리는 것이 문제라 이 부분을 해결하기위해 제공된 베이스라인을 분석 및 변형할 예정입니다.–pre transform전반적인 학습속도가 너무 오래 걸리는 것이 문제였습니다. 이유는 DataLoader가 배치 1개를 불러올때 속도가 오래 걸리는 것이 문제였습니다. 따라서 전반적인 학습 속도의 향상을 위해 GPU 메모리의 사용률을 높이는 방향으로 갔습니다.pre_transform = transforms.Compose([ transforms.Resize((512//4, 384//4)), ])train_dataset = CustomTrainDataset(train_df, pre_transforms=pre_transform, transforms=transform, train=True)이렇게 pre_transform을 통해 이미지의 전체 비율을 줄입니다. 이 과정을 거치면 1개의 이미지 용량이 줄어들게 됩니다. 그리고 pre_transform이 적용되는 과정을 보면 다음과 같습니다.class CustomTrainDataset(Dataset): def __init__(self, df: pd.DataFrame, pre_transforms: T = None, transforms: T = None, train: bool = True): self.base_url = &#39;/opt/ml&#39; self.img_url = &#39;/input/data/train/images&#39; self.transforms = transforms self.train = train self.images = [] self.df = self.__make_dataframe(df) for path in self.df[&#39;path&#39;].values: image = Image.open(path) if pre_transforms: image = pre_transforms(image) self.images.append(image) ...데이터셋을 구성할 때 모든 이미지를 불러와서 메모리에 올리는 방식을 사용했습니다. 이 방법을 사용하면 데이터셋 구성에서 시간이 오래 걸리지만 1번의 과정만 시간을 소모하면 되므로 학습 시간에서 향상이 나타났습니다. 하지만 이 자체가 그렇게 좋은 것은 아니라 생각되므로 추가적인 방법을 고려해서 데이터셋 로드 시간 자체도 줄여야겠습니다.SENet 사용, Multi-head classifier 구현일단 이미지 분류 대회다보니 ILSVRC에서 우승한 모델들 위주로 찾아봤습니다. 그 중 SENet이 성능이 좋은 것으로 나타났단 것을 보고 관련 내용 구현코드를 활용했습니다.자세한 코드 설명은 하지 않겠습니다. 일단 torchvision의 resnet18보다 성능이 좋지 않기 때문에 이를 그대로 사용할 것입니다.마찬가지로 어제 구상한 multi-head classifier를 구현해봤습니다. 기존 resnet의 out_feature 1000을 유지하고 1000에다가 마지막에 총 3개의 nn.Linear를 추가했습니다. 이를 총 18개의 클래스 위치에 값을 연산하여 forward 처리를 해주는 모델을 구상했습니다. 이 코드도 딱히 의미가 있지 않아서 첨부하진 않겠습니다.이후 공개되는 깃허브로 공유하겠습니다.성능 향상은 크게 없었습니다.CutMixdata augmentation 방법 중 cut mix가 효과가 좋다고 알고 있습니다. 해당 값을 적용했더니 validation loss가 많이 줄어드는 것을 발견했으나 정작 test data에서 좋은 성능이 나타나지 않습니다….진짜 모르겠습니다 ㅠㅠㅠㅠㅠㅠㅠㅠㅠ 내일 팀원들이랑 상의를 해봐야겠습니다." }, { "title": "[BoostCamp AI Tech] Day25", "url": "/posts/day25/", "categories": "NAVER BoostCamp AI Tech, Review", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech", "date": "2022-02-23 23:00:00 +0900", "snippet": "Day25 Review당신은 오늘 하루 어떻게 살았나요? 프로젝트 모델 코드 수정 이미지 center crop으로 데이터로더 부담 줄이기 -&amp;gt; 속도 향상 오늘 하지 못한 것들 MLOps Chapter4 Week2 (44분) MLOps Chapter1 Week3 (2시간 7분)내일은 어떤 것을 할까? MLOps Chapter4 Week2 (44분) MLOps Chapter1 Week3 (2시간 7분) MLOps Chapter2 Week3마무리 이번주 큰 일정 (우선순위순) 목요일 MLOps Chapter3 Week3 부캠 서빙 특강 - 내가 만든 모델 합법, 불법 정리 금요일 MLOps Chapter4 Week3 주말 MLOps 2주차 강의 정리 MLOps 3주차 강의 정리 열심히 하자… 모델…하… 힘들긴한데 그래도 재밌다. 직접 코드 구현은 진짜 최고야~ " }, { "title": "[BoostCamp AI Tech / P Stage 1] Day25 - Project Day 3", "url": "/posts/day25_pstage3/", "categories": "NAVER BoostCamp AI Tech, P Stage", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech, Image Classification, Project", "date": "2022-02-23 22:00:00 +0900", "snippet": "P Stage 1 : Project Day 3Day 3 list 신규 모델 구상신규 모델 아이디어 구상요약 ResNet18 기준 0.5정도 나옴 성능 올리고자 data augmentation 이전에 모델 구성적 접근 구상 자세한 구상은 자세한 이야기에…자세한 이야기일단 이번 대회의 목표는 총 18개의 클래스를 분류하는 문제였습니다. 클래스의 분류는 다음과 같이 나눠져서 총 18개의 조합이 존재합니다. 마스크 착용 normal : 마스크 미착용 incorrect : 마스크 비정상 착용 mask : 마스크 착용 성별 남 여 연령대 30세 미만 30세 이상 60세 미만 60세 이상 이미지를 넣어서 총 18개의 클래스로 분류하는 것이 개인적인 생각으로는 이미지의 feature를 찾는 데에 부담이 될 것으로 생각됩니다.이처럼 1개의 batch 단위의 이미지가 들어오면 모델은 성별, 마스크, 연령을 모두 고려해서 이미지를 처리해야 합니다. 개인적인 생각이지만 이를 처리하려면 전반적으로 모델이 분류해야하는 특징점이 너무 세분화되는 것입니다.만약 이렇게 세분화되는 과정이 있다면 모델의 학습에 부담이 될 것이고 성능에 영향을 줄 수 있을 것이라 생각해서 다음과 같이 모델을 구성할 수 있지 않을까 생각합니다. (물론 수학적 근거가 있지는 않습니다. 머릿속 구상이라…)이를 해결하고자 앙상블 머신의 아이디어를 착안했습니다. 1개의 모델은 1개의 서브 클래스 부류에만 집중하는 것이 모델 자체의 성능 향상에 기여할 수 있을 것이라 생각합니다. 앙상블은 입력에 대해 여러 모델의 결과를 병합하는 것이지만 제가 생각한 것은 이와 유사하게 각 모델이 역할에 맞는 특징을 추출해주면 이를 조합해서 정답 클래스로 연결시키는 것입니다.만약 모델이 보는 이미지의 특징이 조금이라도 줄어들면 정답을 처리하는데 충분히 도움이 될 것이라 생각합니다.일단 이는 구상이고 내일 멘토님이랑 얘기도 해보고 관련 자료도 찾아봐야겠습니다." }, { "title": "[BoostCamp AI Tech] Day24", "url": "/posts/day24/", "categories": "NAVER BoostCamp AI Tech, Review", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech", "date": "2022-02-22 23:00:00 +0900", "snippet": "Day24 Review당신은 오늘 하루 어떻게 살았나요? 프로젝트 모델 코드 수정 실험 기록 자동화 코드 추가 F1Score 측정 코드 추가 + average=macro pretrained=True 세팅 MLOps Chapter3 Week2오늘 하지 못한 것들 MLOps Chapter4 Week2 Kaggle House Price 분석해보기 MLOps Chapter1 Week3내일은 어떤 것을 할까? MLOps Chapter4 Week2 (44분) Day23, 24 프로젝트 진행 일지 작성 MLOps Chapter1 Week3 (2시간 7분)마무리 이번주 큰 일정 (우선순위순) 수요일 MLOps Chapter2 Week3 부캠 서빙 특강 - 내가 만든 모델 합법, 불법 정리 목요일 MLOps Chapter3 Week3 금요일 MLOps Chapter4 Week3 주말 MLOps 2주차 강의 정리 MLOps 3주차 강의 정리 " }, { "title": "[BoostCamp AI Tech / P Stage 1] Day24 - Project Day 2", "url": "/posts/day24_pstage2/", "categories": "NAVER BoostCamp AI Tech, P Stage", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech, Image Classification, Project", "date": "2022-02-22 19:00:00 +0900", "snippet": "P Stage 1 : Project Day 2Day 2 list 깃허브 README 실험기록 자동화 코드 수행 F1Score scoring 코드 추가 baseline model 기본 하이퍼파라미터 제출깃허브 README 자동화요약 부캠에서 제공해준 팀 프로젝트용 깃허브에 각자 폴더를 만들어서 실험 진행하기로 함 하나하나 기록값을 README에 적는 것은 효율 떨어진다고 판단 실험 종료 후 자동 기록 코드를 설정하여 실험 기록 자동화 진행 자세한 이야기def record_expr(model, model_name, best_train_loss, avg_val_loss, avg_val_score, best_val_f1, args): # | Date | model_name | best_loss | avg val loss | avg val f1 score | best f1 | Hyperparameters | current_time = datetime.now(pytz.timezone(&#39;Asia/Seoul&#39;)) base_url = &#39;/opt/ml&#39; model_state_save_path = base_url+f&#39;/level1-image-classification-level1-recsys-12/polar/model_state/&#39; \\ f&#39;{model_name}_{current_time.month}{current_time.day}_&#39; \\ f&#39;{current_time.hour}{current_time.minute}.pt&#39; markdown_path = base_url+&#39;/level1-image-classification-level1-recsys-12/polar/README.md&#39; non_list = [&#39;name&#39;, &#39;mode&#39;, &#39;save&#39;] hypers = {k: v for k, v in vars(args).items() if k not in non_list} current_time = current_time.strftime(&quot;%Y-%m-%d %H:%M:%S&quot;) os.system(f&quot;echo &#39;|{current_time}|{model_name}|{best_train_loss:.3g}|{avg_val_loss:.3g}|{avg_val_loss:.3g}|\\ {avg_val_score:.3g}|{best_val_f1:.3g}|{str(hypers)}|&#39; &amp;gt;&amp;gt; {markdown_path}&quot;) print(&quot;Save the experiment complete!&quot;)이전에 학교 문해기 수업에서 매번 주어진 테스트 케이스 확인하기 귀찮아서 풀이 C++ 파일이름을 양식에 맞춰서 argument에 입력하면 채점해주는 프로그램을 만든 적이 있습니다.당시에 os.system을 사용하면 파이썬 코드에서 콘솔로 명령어를 수행할 수 있다는 것을 배웠는데, 이를 활용해서 특정 출력을 README에 기록하는 방식으로 코드를 작성했습니다.우선 대부분의 파일은 utils.py로 따로 빼서 관리를 진행했기 때문에 utils.py에 관련 메서드를 작성했습니다. (위 코드)그림처럼 표로 작성하는 것이 좋을 것 같아서 상위 컬럼틀만 만들고 최하단에 표 내용을 채우는 방식을 사용했습니다.def main(args): ... if args.save.lower() == &quot;true&quot;: record_expr(best_model, args.name, prev_loss, val_loss, val_f1, f1_best, args) if args.mode == &quot;test&quot;: test(best_model, device)대충 훈련이 다 돌아가고 인자로 받는 값 중 -s 또는 --save가 true면 관련 함수를 동작하여 값을 자동기록합니다.F1Score scoring 코드 추가요약 이번 대회의 평가지표는 macro F1 score 일반적으로 sci-kit learn의 f1_score을 사용하지만 번거로우므로 다른거 찾아봄 torchmetrics를 알게되어 설치해서 세팅함 자세한 이야기def main(args): ... f1_score = F1Score(num_classes=18, average=&quot;macro&quot;).to(device) ...이번 대회에서는 multi-class에 대한 F1 score가 평가지표입니다. 따라서 단순 accuracy를 사용하면 안됩니다. 이에 f1 score 기록 방법을 찾다가 sci-kit learn보다는 pytorch와 관련된 형식으로 일치하기로 결정했습니다.자료를 찾던 중 torchmetics를 발견해서 사용했는데 매우 편했습니다. 나름 토치쓰면 강추baseline model 기본 하이퍼파라미터 제출요약 Day 1에 작성한 ResNet18 베이스라인 코드 제출해봄 기대를 안하긴했는데 역시나 그렇게 좋진않았음 valid f1 0.78 이었는데 실제 test f1은 0.5정도로 나옴 대략 validation과 오차 0.2정도 차이 Day 2 분석 일단 아무것도 처리하지 않은 상태에서 진행했으므로 좋은 성능이 안 나올 것은 예상 EDA 결과 imbalance가 심해서 관련 data augmentation 진행해야 할 듯 내 EDA 짱임~ (Upvote 많음! - 이러니까 캐글도 EDA 많이 해보고프다) 이미지도 PCA나 임베딩으로 클러스터링 가능하진 확인해보고 싶음 다른 pre-trained를 써보고 결과 확인하기 GPU 사용이 좀 떨어지는거 같은데 GPU 사용율 체크 해보기" }, { "title": "[BoostCamp AI Tech] Day23", "url": "/posts/day23/", "categories": "NAVER BoostCamp AI Tech, Review", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech", "date": "2022-02-21 23:00:00 +0900", "snippet": "Day23 Review당신은 오늘 하루 어떻게 살았나요? Image Classification EDA Image Classification Baseline Model 만들기 MLOps Chapter4 Week2 과제 제출오늘 하지 못한 것들 MLOps Chapter3 Week2 MLOps Chapter4 Week2내일은 어떤 것을 할까? MLOps Chapter3 Week2 MLOps Chapter4 Week2마무리 이번주 큰 일정 (우선순위순) 화요일 MLOps Week2 끝내기 Kaggle House Price 분석해보기 MLOps Chapter1 Week3 수요일 MLOps Chapter2 Week3 부캠 서빙 특강 - 내가 만든 모델 합법, 불법 정리 목요일 MLOps Chapter3 Week3 금요일 MLOps Chapter4 Week3 주말 MLOps 2주차 강의 정리 MLOps 3주차 강의 정리 " }, { "title": "[BoostCamp AI Tech / P Stage 1] Day23 - Project Day 1", "url": "/posts/day23_pstage1/", "categories": "NAVER BoostCamp AI Tech, P Stage", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech, Image Classification, Project", "date": "2022-02-21 19:00:00 +0900", "snippet": "P Stage 1 : Project Day 1Day 1 list 대회 데이터 EDA pre-trained model인 ResNet18 기준 베이스라인 전체 코드 작성데이터 EDA요약 지금 이거 작성을 첫날에 못해서 3일째에 작성중인데 up vote 짱 많음~ 추가적인 EDA 요소 찾아보면 좋을듯게시글 top vote 2ResNet 기반 베이스라인 코드 작성요약 전체 통합 파이프라인 파일 구축 ResNet18 기준으로 구축함 기본 베이스 하이퍼 파라미터 learning rate : 0.01 batch_size : 32 epoch : 10 코드는 지금 올려도 되는지 몰라서… 어차피 후에 private repoository public으로 된다고 했으니 그때 코드 링크 다시 걸겠습니다." }, { "title": "[MLOps Specialization / Step4] Introduction to Model Serving Infrastructure", "url": "/posts/coursera4_2/", "categories": "Data Engineering, MLOps", "tags": "Data Engineering, MLOps, Coursera", "date": "2022-02-19 07:53:00 +0900", "snippet": "Introduction to Model Serving Infrastructure이 포스트는 Coursera의 MLOps Specialization 강의 내용을 기반으로 작성되었습니다.Introduction to Model Serving Infrastructure 모델의 복잡도가 증가하면 같이 증가하는 것들 모델 크기 Complex functons 예측 지연율 예측 정확도 비용 (HW / SW 모두) 비용과 모델의 복잡도가 균형을 이뤄야 함 (단순히 연구가 아닌 배포가 목적이므로) 예측 효율과 지연율 속도간에는 trade off가 발생 Maintaining Input Feature Lookup 모든 feature가 예측에 활용되는 것은 아님 예를 들어 음식 배달관련으로는 주문이 들어온 시간과 과거에 분당 취소된 주문 수 정도만 필요 추가적은 pre-computed or aggregated feature들은 실시간으로 데이터 저장소로 부터 읽혀야 함 이 과정에서 비용이 발생Deployment Options Huge data centers Model 효율과 비용이 모두 중요 리소스 활용 최적화 비용의 최소화 Embedded devices 모바일 핸드폰에 들어가는 경우 고려해야할 것들이 꽤 있음 평균적인 GPU 메모리는 4기가미만 평균 앱 크기는 11메가 사용자는 너무 많은 device resource 소모를 좋아하지 않음$\\rightarrow$ 설치하지 않는 결과로 연결 Edge device의 제한된 환경에 대한 해결책으로 REST API를 활용한 서버와의 통신을 통한 예측을 활용 모델 자체는 서버에 두고 API 통신으로 예측을 수행 Improving Prediction Latency and Reducing Resource Costs Profile and Benchmark 적합한 후보 모델을 선택하거나 구축한 후 모델을 profiling하고 벤치마킹 Optimize Operators TF Lite 등을 활용해 통계 지표를 확인 가능 이를 활용해 성능 병목 현상 확인 연산 시간에 지배적인 operator 식별 가능 Optimize Model 모바일 기기에서 중요 Tweak Threads thread 수를 늘려 앱 실행속도 증가 하지만 energy 사용량이 증가하므로 효율성을 고려해야함 " }, { "title": "[MLOps Specialization / Step4] Introduction to Model Serving", "url": "/posts/coursera4_1/", "categories": "Data Engineering, MLOps", "tags": "Data Engineering, MLOps, Coursera", "date": "2022-02-19 07:52:00 +0900", "snippet": "Introduction to Model Serving이 포스트는 Coursera의 MLOps Specialization 강의 내용을 기반으로 작성되었습니다.Introduction to Model Serving 모델 서빙 패턴 모델 interpreter and execution 입력 데이터 위 3가지 과정은 추론에 활용 ML workflows 모델 학습 모델 예측 위 과정은 batch inference나 tlftlrks inference에 활용 주요한 지표 Latency Throughput Cost Latency user의 행동과 그에 따른 어플리케이션의 반응까지의 딜레이 Latency는 전체 프로세스, 데이터에서 서버로의 전송시기, 추론 과정 등 모든 과정에서 발생 가능 적은 latency는 고객 만족도 유지의 핵심 필수요건Throughput 1초에 성공적인 수행이 가능한 요청 일부 어플리케이션은 지연율보다 처리율만 중요한 경우도 있음Cost 각 inference와 관련된 cost는 최소화되어야 함 중요한 Infrastructure 요규사항은 비용이 비쌈 CPU GPU같은 가속 HW Caching infra 결과적으로 위의 3가지 요소가 조화를 이루는 것이 가장 좋습니다. 지연율은 줄이고 처리율은 높이는 것의 방향과 비용, 지연율, 처리율이 균형을 이루는 것 등 여러 관점으로 바라봐야 합니다." }, { "title": "[BoostCamp AI Tech / 심화포스팅] 확률분포와 검정", "url": "/posts/testing/", "categories": "NAVER BoostCamp AI Tech, Level 1 - AI Math", "tags": "NAVER, BoostCamp, AI Tech, 통계학, 검정", "date": "2022-02-18 14:00:00 +0900", "snippet": "심화포스팅 : 확률분포와 검정확률분포‘우리가 분석할 데이터는 어떤 확률변수로부터 실현된 표본’이라는 가정이 데이터 분석의 첫번째 가정입니다.즉 이 데이터들은 어떤 확률변수의 분포를 통해 만들어진 것이라는 의미입니다. 따라서 우리는 데이터 즉 표본으로부터 확률변수의 분포를 알아내야 합니다.대표적인 확률분포대표적인 확률분포로는 다음과 같은 것들이 있습니다. 베르누이분포 : 데이터가 0 또는 1인 경우 카테고리분포 : 데이터가 카테고리 값인 경우 베타분포 : 데이터가 0과 1 사이의 실수 값인 경우 로그정규분포, 감마분포, F분포, 카이제곱분포, 지수분포 등… : 데이터가 항상 0또는 양수인 경우 정규분포, 스튜던트 t분포, 코시분포, 라플라스분포 등… : 데이터가 크기 제한이 없는 실수인 경우그렇다면 이 분포들을 파이썬으로 사용하는 방법이 있을까요?scipy (사이파이) 패키지의 stats를 사용하면 이 분포들의 값을 얻어낼 수 있습니다.np.random.seed(0)plt.figure(figsize=(8, 6))# 베타분포 샘플링x = sp.stats.beta(15, 12).rvs(10000)sns.distplot(x, kde=False, norm_hist=True)plt.title(&quot;Beta dist&#39;n histrogram&quot;)plt.show()이런 분포를 활용하면 역으로 데이터의 모수 추정 방법이 유효하다는 것을 입증할 수도 있습니다.def estimate_beta(x): x_bar = x.mean() s2 = x.var() a = x_bar * (x_bar * (1 - x_bar) / s2 - 1) b = (1 - x_bar) * (x_bar * (1 - x_bar) /s2 - 1) return a, bparams = estimate_beta(x)print(params)(15.346682046700685, 12.2121537049535)plt.figure(figsize=(8, 6))xx = np.linspace(0, 1, 1000)sns.distplot(x, kde=False, norm_hist=True)plt.plot(xx, sp.stats.beta(params[0], params[1]).pdf(xx))plt.xlim(0, 1)plt.title(&quot;Beta dist&#39;n histrogram and Estimation&quot;)plt.show()scipy에 있는 확률분포 클래스 종류는 다음과 같습니다. 이산형 베르누이분포 (sp.stats.bernoulli) 이항분포 (sp.stats.binom) 연속형 다향분포 (sp.stats.multinomial) 균일분포 (sp.stats.uniform) 정규분포 (sp.stats.norm) 베타분포 (sp.stats.beta) 감마분포 (sp.stats.gamma) 스튜던트 t분포 (sp.stats.t) 카이 제곱분포 (sp.stats.chi2) F분포 (sp.stats.f) 디리클레분포 (sp.stats.dirichlet) 다변수 정규분포 (sp.stats.multivariate_normal) 각 클래스들에는 다음과 같은 메서드들이 있습니다. 대부분 각 분포들의 함수 형태와 관련된 것들입니다. pmf : 확률질량함수 pdf : 확률밀도함수 cdf : 누적분포함수 ppf : 누적분포함수의 역함수 sf : 생존함수 = 1-누적분포함수 isf : 생존함수의 역함수 rvs : 랜점 표본 생성검정과 유의확률검정은 데이터에 숨어 있는 확률변수의 분포에 대한 가설이 맞는지 틀리는지 정량적으로 증명하는 과정입니다.예를 들어 다음과 같은 질문이 있습니다. 어떤 동전을 15번 던졌더니 12번 앞면이 나왔다. 이 동전은 조작되지 않은 공정한 동전일까?동전은 0과 1이이 있는 베르누이분포이므로 베르누이분포 확률변수로 모형화하겠습니다.모수를 추정하면 $p = \\frac{12}{15} = 0.8$로 공정성과는 멀어보입니다.근데 이것은 단순히 추정에 불과합니다. 정말 어쩌다가 우연하게 결과가 나올 수도 있으니까요.그렇다면 우리는 이 주장을 어떻게 입증할 수 있을까요?가설과 검정우리는 우선 주어진 확률분포에 대한 가설($H$) 을 설립해야합니다. 이 가설을 증명하는 행위를 통계적 가설검정 즉, 검정이라고 합니다.특히 모숫값이 특정값을 갖는다는 가설을 검정하는 경우 모수 검정이라고 합니다.귀무가설과 대립가설확률분포의 모수에 대한 가설을 귀무가설($H_0)$ 이라고 합니다.귀무가설은 확률분포를 특정한 상태로 고정하므로 반드시 등식으로 표현해야합니다. 즉, 귀무가설은 어떤 기준점의 상태입니다.그렇다면 이 기준점 가설이 우리가 입증하려는 가설이면 괜찮은데, 아닌경우도 있습니다.우리가 이 가설을 반박하고 싶은 경우도 있는데, 이런 경우 대립가설($H_1)$ 을 세웁니다.대립가설과 귀무가설을 아래처럼 표기합니다.\\[H_0: \\theta = \\theta_0 \\qquad H_1 : \\theta \\neq \\theta_0 \\\\H_0: \\theta = \\theta_0 \\qquad H_1 : \\theta \\lt \\theta_0 \\\\H_0: \\theta = \\theta_0 \\qquad H_1 : \\theta \\gt \\theta_0\\]그렇다면 위에서 말한 동전문제의 검정 가설은 어떻게 세울까요?\\[H_0 : \\mu=0.5 \\qquad H_1 : \\mu \\neq 0.5\\]이를 증명하려면 이제 귀무가설이 틀렸다는 증거를 보여줘야합니다.검정 통계량귀무가설이 맞거나 틀렸다는 것을 증명하려면 어떤 증거가 있어야 합니다. ‘어떤 병에 걸렸다’라는 가설을 증명하려면 환자의 혈액을 채취하여 혈액 내의 특정한 성분의 수치를 측정해야 한다고 가정하자. 이때 해당 수치가 바로 검정통계량이 된다. ‘어떤 학생이 우등 상장을 받을 수 있는 우등생이다’라는 가설을 증명하려면 시험 성적을 측정하면 된다. 이 시험 성적을 검정통계량이라고 부를 수 있다.이 증거를 담당하는 수치가 바로 검정통계량($t$) 이라고 합니다.문제는 여기서 이 검정통계량도 특정표본에서 계산된 함수이기 때문에 또 다시 확률변수의 표본이 됩니다.예를 들면 위의 예시로 설명해보면 성적을 통해 학생이 우등생임을 보이려면 우등생인 모든 학생의 시험 성적 분포를 보고 판단해야합니다.이 과정이 수식적 유도과정으로는 굉장히 어렵기 때문에 시뮬레이션을 활용하거나 통계학자들이 특정한 검정통계량 확률분포만 사용합니다.유의확률지금까지 2가지 정보를 얻었습니다. 검정통계량이 따르는 검정통계량 $t$의 확률분포 $p_{T}(x)$를 알고 있습니다. (통계학자들이 구해줌) 실제 데이터에 구한 검정통계량의 값 $t_0$, 즉 확률분포 $p_{T}(x)$의 표본 1개를 갖고 있습니다.만약 우리가 설정한 귀무가설이 사실이라면 실제 데이터에서ㅓ 구한 검정통계량 값은 검정통계량 확률분포를 따르므로 기댓값 근방의 값이 나올 겁니다.반대로 귀무가설이 사실이 아니면 실제 데이터에서 구한 검정통계량의 값은 검정통계량에서 나오기 어려울 것입니다.xx1 = np.linspace(-4, 4, 100)black = {&quot;facecolor&quot;: &quot;black&quot;}plt.figure(figsize=(8, 4))plt.subplot(121)plt.title(&quot;가능성이 높은 검정통계량이 나온 경우&quot;)plt.plot(xx1, sp.stats.norm.pdf(xx1))plt.plot(0.5, 0, &quot;ro&quot;)plt.annotate(&#39;실제 검정통계량&#39;, xy=(0.5, 0.01), xytext=(0.85, 0.1), arrowprops=black)plt.subplot(122)plt.title(&quot;가능성이 낮은 검정통계량이 나온 경우&quot;)plt.plot(xx1, sp.stats.norm.pdf(xx1))plt.plot(2.2, 0, &quot;ro&quot;)plt.annotate(&#39;실제 검정통계량 $t_0$&#39;, xy=(2.2, 0.01), xytext=(0.85, 0.1), arrowprops=black)plt.suptitle(&quot;검정통계량 분포와 실제 검정통계량의 값&quot;, y=1.05)plt.tight_layout()plt.show()그렇다면 이렇게 나오기 쉬운 값인지 아닌지를 숫자로 판단할 수 있는 방법이 있을까요?그 방법이 바로 유의확률(p-value) 입니다.유의확률은 확률분포와 확률분포의 표본값 1개가 주어졌을 때 그 확률분포에서 해당 표본값 혹은 더 희귀한 값이 나올 수 있는 확률로 정의합니다.xx1 = np.linspace(-4, 4, 100)black = {&quot;facecolor&quot;: &quot;black&quot;}plt.figure(figsize=(8, 4))plt.subplot(121)plt.title(&quot;유의확률이 큰 경우&quot;)plt.plot(xx1, sp.stats.norm.pdf(xx1))plt.plot(0.5, 0, &quot;ro&quot;)plt.annotate(&#39;실제 검정통계량 $t_0$&#39;, xy=(0.5, 0.01), xytext=(0.85, 0.1), arrowprops=black)xx2 = np.linspace(-4, -0.5, 100)xx3 = np.linspace(0.5, 4, 100)plt.fill_between(xx2, sp.stats.norm.pdf(xx2), facecolor=&#39;blue&#39;, alpha=0.35)plt.fill_between(xx3, sp.stats.norm.pdf(xx3), facecolor=&#39;blue&#39;, alpha=0.35)plt.annotate(&#39;유의확률&#39;, xy=(-1.5, 0.05), xytext=(-3.5, 0.1), arrowprops=black)plt.subplot(122)plt.title(&quot;유의확률이 작은 경우&quot;)plt.plot(xx1, sp.stats.norm.pdf(xx1))plt.plot(2.2, 0, &quot;ro&quot;)plt.annotate(&#39;실제 검정통계량 $t_0$&#39;, xy=(2.2, 0.01), xytext=(0.85, 0.1), arrowprops=black)xx2 = np.linspace(-4, -2.2, 100)xx3 = np.linspace(2.2, 4, 100)plt.fill_between(xx2, sp.stats.norm.pdf(xx2), facecolor=&#39;blue&#39;, alpha=0.35)plt.fill_between(xx3, sp.stats.norm.pdf(xx3), facecolor=&#39;blue&#39;, alpha=0.35)plt.annotate(&#39;유의확률&#39;, xy=(-2.5, 0.01), xytext=(-3.5, 0.1), arrowprops=black)plt.suptitle(&quot;검정통계량 분포와 실제 검정통계량 $t_0$의 값&quot;, y=1.05)plt.tight_layout()plt.show()유의수준과 기각역유의확률이 아주 작다는 것은 귀무가설이 맞다는 가정하에 현재 통계량 값이 나올 가능성이 매우 작다는 것입니다.결국 유의확률값이 아주 작으면 귀무가설을 기각하고 대립가설을 채택할 수 있습니다.쉽게 말하면 귀무가설이 맞다는 가정하에 현재의 실제 결과가 나올 가능성이 없다면 귀무가설이 틀렸다는 반증이 됩니다.문제는 여기서 말하는 아주 작다의 기준이 뭐냐?입니다.그래서 보통 기준값을 정하는데 이를 유의수준이라 하고 1%, 5%, 10%선에서 결정합니다. 유의확률이 유의수준보다 작으면 귀무가설을 기각하고 대립가설을 채택한다.실제 예시 : 동전의 공정성 분석자 대충 이론적인 내용을 봤으니 위의 동전예시에 적용해봅시다.우선 $H_0 : \\mu = 0.5$ 입니다.여기서 우리는 동전이 공정하지 않다는 것을 보이고 싶으니까 $H_1 : \\mu \\neq 0.5$로 설정합시다.우리의 동전은 0.8의 확률을 갖고 있었습니다.sp.stats.binom_test(12, 15)0.035156250000000014귀무가설 0.5 가정하에 15번 수행 중 12번의 1이 나오면 p-value는 0.035가 나옵니다.이 경우 유의확률 5%로 볼 경우 $0.05 &amp;gt; \\text{p-value}$이므로 귀무가설이 기각되고 해당 동전은 공정하지 못합니다.scipy를 통한 검정법그렇다면 이제 scipy를 통해 다양한 분포 검정을 해봅시다. 종류는 다음과 같이 있습니다.이항검정scipy.stats.binom_test(x, n=None, p=0.5, alternative=&#39;two-sides&#39;) x : 1이 나온 횟수 n : 총 시도 횟수 p : 귀무가설의 $\\mu$ 값 alternative : 양측검정은 ‘two-sided’, 단측검정이면 ‘less’ 또는 ‘greater’이항검정은 이항분포를 활용하여 베르누이 확률변수의 모수 $\\mu$에 대한 가설을 조사하는 방법입니다.sp.stats.binom_test(12, 15)0.035156250000000014카이제곱검정scipy.stats.chisquare(f_obs, f_exp=None) f_obs : 데이터 행렬 f_exp : 기댓값 행렬카이제곱검정은 범주형 확률분포에 대한 검정을 할때 사용합니다. 적합도검정이라고도 합니다.전반적으로 범주형 자료를 담다보니 입력 parameter는 행렬값으로 가져옵니다. 하지만 각 값은 이항검정 케이스와 유사합니다.데이터 개수 N=10, 귀무가설 모수 $\\mu_0 = (0.25, 0.25, 0.25, 0.25)$ 이고 실제 데이터는 (0, 3, 5, 2)인 경우에 대해 검정을 해봅시다.N = 10K = 4mu_0 = np.ones(K)/Knp.random.seed(0)x = np.random.choice(K, N, p=mu_0)n = np.bincount(x, minlength=K)narray([0, 3, 5, 2])sp.stats.chisquare(n)Power_divergenceResult(statistic=5.199999999999999, pvalue=0.157724450396663)카이제곱 독립성검정scipy.stats.chi2_contingency(table)두 범주형 변수의 상관관계를 분석할 때 사용합니다.보통 두 변수가 독립인지 아닌지를 확인할 때 많이 사용합니다.obs = np.array([[5, 15], [10, 20]])obsarray([[ 5, 15], [10, 20]])sp.stats.chi2_contingency(obs)[0], sp.stats.chi2_contingency(obs)[1] # cor, p-value(0.0992063492063492, 0.7527841326498471)# Create contingency table OD = pd.DataFrame({&#39;notObese&#39;: [173, 259], &#39;Obese&#39; : [27, 61]},index ={&#39;nonDiabetic&#39;,&#39;Diabetic&#39;})OD notObese Obese Diabetic 173 27 nonDiabetic 259 61 # chi-square testchiRes = sp.stats.chi2_contingency(OD)# Detailsprint(f&#39;chi-square statistic: {chiRes[0]}&#39;)print(f&#39;p-value: {chiRes[1]}&#39;)print(f&#39;degree of freedom: {chiRes[2]}&#39;)print(&#39;expected contingency table&#39;) print(chiRes[3])chi-square statistic: 2.3274739583333344p-value: 0.12710799319896815degree of freedom: 1expected contingency table[[166.15384615 33.84615385] [265.84615385 54.15384615]]# Create the contigency tableGE = pd.DataFrame({&#39;Blue&#39; :[370, 359], &#39;Brown&#39;:[352, 290], &#39;Green&#39;:[198, 110], &#39;Hazel&#39;:[187, 169]},index={&#39;Female&#39;,&#39;Male&#39;})GE Blue Brown Green Hazel Male 370 352 198 187 Female 359 290 110 169 # chi-square testchiRes2 = sp.stats.chi2_contingency(GE)# Detailsprint(f&#39;chi-square statistic: {chiRes2[0]}&#39;)print(f&#39;p-value: {chiRes2[1]}&#39;)print(f&#39;degree of freedom: {chiRes2[2]}&#39;)print(&#39;expected contingency table&#39;) print(chiRes2[3])chi-square statistic: 16.589883392436516p-value: 0.0008581329522990205degree of freedom: 3expected contingency table[[396.56167076 349.23538084 167.54594595 193.65700246] [332.43832924 292.76461916 140.45405405 162.34299754]]점 양분 상관계수 (point-biserial correlation)두 범주형 변수의 상관성은 카이제곱 독립성 검정을 통해 알 수 있습니다.그렇다면 연령과 범주형 변수처럼 연속형 변수와 범주형 변수의 상관관계는 어떻게 알 수 있을까요?scipy.stats.pointbiserialr(X, y)import random# initialize random number generatorrandom.seed(36)# Assume y is studied (1)/ not studied (0) # X is the test score, max 20 marksy = [random.randint(0, 1) for num in range(0, 10)]print(f&#39;y values: {y}&#39;)X = []for i in y: if i != 0: ele = random.randint(11, 20) else: ele = random.randint(1, 10) # 1-10 not studied X.append(ele)print(f&#39;X values: {X}&#39;)y values: [1, 0, 0, 1, 0, 0, 0, 0, 1, 1]X values: [16, 9, 5, 19, 2, 10, 7, 8, 17, 16]# point-biserial correlation# output is a tupleresult = sp.stats.pointbiserialr(X, y)print(f&#39;correlation between X and y: {result[0]:.2f}&#39;)print(f&#39;p-value: {result[1]:.2g}&#39;)correlation between X and y: 0.91p-value: 0.00021References 김도영의 데이터사이언스 스쿨 : 9.4 검정과 유의확률, https://datascienceschool.net/02%20mathematics/09.04%20검정과%20유의확률.html 김도영의 데이터사이언스 스쿨 : 9.5 사이파이를 사용한 검정, https://datascienceschool.net/02%20mathematics/09.05%20사이파이를%20사용한%20검정.html# Navigating Statistical Tests https://towardsdatascience.com/levels-of-measurement-statistics-and-python-implementations-8ff8e7867d0b" }, { "title": "[MLOps Specialization / Step3] AutoML", "url": "/posts/cousera3_2/", "categories": "Data Engineering, MLOps", "tags": "Data Engineering, MLOps, Coursera", "date": "2022-02-18 07:50:00 +0900", "snippet": "AutoML이 포스트는 Coursera의 MLOps Specialization 강의 내용을 기반으로 작성되었습니다.Intro to AutoML AutoML은 pipeline, model 생성, 학습을 모두 통합한 기술 모든 ML workflow를 자동화하는 것이 AutoML AutoML의 핵심 기술은 Neural Architecture Search(NAS는 AutoML의 sub-field일뿐 전체가 아님) Search Space 신경망 구조의 범위를 정의 (search space) 문제의 크기를 감소하거나 인간 지식 너머의 것들 등… Search Strategy 선정된 search space를 어떻게 탐색할 것인가? search space의 신경망 후보군을 선택 Performance Estimation Strategy 선택된 후보군에 대해 지표를 측정하고 비교를 진행 평가를 기반으로 다시 탐색 전략을 선정 Understanding Search Spaces Search Space에는 2가지 종류가 존재 Macro Micro Macro 한층씩 쌓으며 최적을 찾는 방법 Chain structed space : 레이어를 연속적으로 쌓는 것 Complex search space : 연속적으로 레이어를 연결하는 것이 아닌 가지로 뻗어나가거나 skip connection 원리를 활용 Micro cell(작은 neural net)을 쌓는 방식 Macro보다 좋은 성능을 냄 1개의 레이어를 다른 cell로 대체하면 더 복잡한 구조를 표현할 수 있음 Search StrategiesGrid / Random Search Grid Search는 모든 가능성을 탐색 매우 소모적인 탐색방식 Random Search는 다음 옵션을 무작위로 선정 둘 다 작은 search space에 적합 search spcae 크기가 커지면 빠르게 실패Bayesian Optimization 성능에 기반한 특정한 확률분포를 추정 테스트된 architecture들은 확률분포를 제한하고 이를 토대로 다음 옵션을 선정 이 방식은 유망한 architecture이 stochastic하게 결정됨= 테스트 결과와 제한된 분포에 기반한 결과라는 의미Evolutionary Methods 기본 원리는 유전 알고리즘 랜덤한 초기의 n개의 architecture를 생성하고 estimation strategy로 성능 평가 X개의 높은 부모 architecture를 선정하여 새로운 N개의 자식 구조 생성 이때 자식 architecture는 부모를 복제하되 무작위로 일부를 대체하거나 변형 부모 구조의 변형은 특정 층이나 연결을 제거하거나 레이어의 크기나 하이퍼파라미터를 변경하는 경우도 있음 부모 구조를 조합하는 경우도 있음 다시 estimation strategy에 기반해 성능 평가 이 중 오래되거나 좋지 않은 성능을 보이는 N개의 architecture 제거(탈락) 2에서 생성한 자식구조들은 4에서 제거된 N개의 부모 자리에 대체되어 과정을 반복Reinforcement Learning Agent는 보상을 최대화하는 방향으로 학습 가능한 옵션은 search space에서 선택하는 것들 성능평가 전략은 보상을 주는 방식으로 결정Measuring AutoML Efficacy 성능 평가 전략으로는 컴퓨팅적으로 부담스러운지, 시간 소모와 높은 GPU 사용량, 비용적으로 비싼가 등이 있음Lower Fidelity Estimates 문제를 재학습하는데 들이는 시간을 줄이는 것이 목표 Data subset 비용을 줄일 수 있음 성능에서 저평가 발생가능 Low resolution images Fewer filters and cells 최근에는 이 방식 사용 안함 Learning Curve Extrapolation learning curve가 믿을만하게 예측하기 위한 매커니즘 initail learning에 기반하여 추론을 진행 낮은 성능을 보이는 것들은 탈락Wight Inheritance/Network Morphisms 이전에 학습한 architecture에 기반하여 새 architure의 가중치를 초기화 간단한 방식의 전이학습 Network Morphism을 활용 Network morphism : 수정을 하되 내부의 함수는 변경하지 않음 Underlying function을 변경하지 않으므로 새로운 네트워크는 부모 네트워크의 지식을 상속받고 계산 성능이 향상 " }, { "title": "[MLOps Specialization / Step3] Hyperparameter tuning", "url": "/posts/coursera3_1/", "categories": "Data Engineering, MLOps", "tags": "Data Engineering, MLOps, Coursera", "date": "2022-02-18 07:40:00 +0900", "snippet": "Hyperparameter tuning: searching for the best architecture이 포스트는 Coursera의 MLOps Specialization 강의 내용을 기반으로 작성되었습니다.Hyperparameter TuningNeural Architecture Search (NAS) NAS는 neural network 디자인을 자동화해주는 기술 적절한 구조를 찾는 것을 도와줌 넓은 공간을 탐색 AutoML은 자동 탐색 알고리즘Types of parameters 학습 가능 파라미터 학습 프로세스 동안 알고리즘에 의해 학습가능 자동적으로 업데이트를 진행 ex) NN의 weight나 bias 하이퍼 파라미터 모델이 어떻게 학습하는가에 영향을 미침 학습 프로세스 이전에 세팅 학습 과정동안 업데이트되지 않음 자동적인 업데이트가 이뤄지지 않음 ex) learning rate, # of units 수동으로 하이퍼파라미터를 튜닝하는 것은 확장가능하지 않음 또한 수동 튜닝은 수수께끼 퍼즐같은 것 Keras Tuner를 통해 하이퍼파라미터 튜닝의 자동화가 가능Keras Autotuner DemoSetting up libraries and datasetimport tensorflow as tffrom tensorflow import kerasmnist = tf.keras.datasets.mnist(x_train, y_train), (x_test, y_test) = mnist.load_data()x_train, x_test = x_train/255.0, x_test/255.0Deep learning “Hello world!”model = tf.keras.models.Sequential([ tf.keras.layers.Flatten(input_shape=(28, 28)), tf.keras.layers.Dense(512, activation=&#39;relu&#39;), tf.keras.layers.Dropout(0.2), tf.keras.layers.Dense(10, activation=&#39;softmax)])model.compile(optimizer=&#39;adam&#39;, loss=&#39;sparse_categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;])model.fit(x_train, y_train, epochs=5)model.evaluate(x_test, y_test)Parameters rational: if any model 코드에서 모델의 구조 중 tf.keras.layers.Dense와 Dropout에 들어가는 파라미터들은 많은 시도를 통해 알아내거나 단순히 복사해서 쓰는 경우임$\\rightarrow$ 부정확하거나 근거가 부족Automated search with Keras tunerpip install -q -U keras-tunerimport kerastuner as ktdef model_builder(hp): model = keras.Sequential() model.add(keras.layer.Flatten(input_shape=(28, 28)) hp_units = hp.Int(&#39;units&#39;, min_value=16, max_value=512, step=16) model.add(keras.layers.Dense(units=hp_units, activation=&#39;relu&#39;)) model.add(tf.keras.layers.Dropout(0.2)) model.add(keras.layers.Dense(10)) model.compile(optimizre=&#39;adam&#39;, loss=&#39;sparse_categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) return modelSearch strategytuner = kt.Hyperband(model_builder, objective=&#39;val_accuracy&#39;, max_epochs=10, factor=3, directory=&#39;my_dir&#39;, project_name=&#39;intro_to_kt&#39;) 여기서 Hyperband는 RandomSearch, BayesianOptimization, Sklearn 등 다양한 전략을 사용할 수 있음Callback configurationstop_early = tf.keras.callbacks.EarlyStopping(moniotr=&#39;val_loss&#39;, patience=5)tuner.search(x_train, y_train, epochs=50, validation_split=0.2, callbacks=[stop_early])" }, { "title": "[BoostCamp AI Tech] Day21", "url": "/posts/day21/", "categories": "NAVER BoostCamp AI Tech, Review", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech", "date": "2022-02-17 23:00:00 +0900", "snippet": "Day21 Review당신은 오늘 하루 어떻게 살았나요? MLOps Chapter1 2주차 MLops Chapter 2-1 MLops Chapter 3-1 MLops Chapter 4-1오늘 하지 못한 것들 MLOps Chapter2 2주차 심화 포스팅 - scipy 통계검정내일은 어떤 것을 할까? 심화 포스팅 - scipy 통계검정 MLOps Chapter 2 2주차 MLOps Chapter 3 2주차 MLOps Chapter 4 2주차 부캠 특강 정리마무리 주말 목표 MLOps Chapter 1, 2 2주차 정리 가능하면 Chapter 3, 4 까지 정리 그래도 MLOps 1주차 정리 다함…. ^^ 일단 업로드 하고 나중에 기간을 수정해서 분배" }, { "title": "[MLOps Specialization / Step2] Validating Data", "url": "/posts/coursera2_4/", "categories": "Data Engineering, MLOps", "tags": "Data Engineering, MLOps, Coursera", "date": "2022-02-17 19:30:00 +0900", "snippet": "Validating Data이 포스트는 Coursera의 MLOps Specialization 강의 내용을 기반으로 작성되었습니다.Detecting Data Issues Drift : 시간 흐름에 따라 데이터의 변화 Skew : 훈련 셋과 서빙 셋고 같은 2가지 정적요소간의 차이가 발생 Model Decay : Data drift 데이터 분포가 변화하여 잘못된 분류가 발생하여 모델이 붕괴 Performance decay : Concept drift 라벨의 분포가 변화하여 훈련 상황에 비해 서비 상황 성능이 좋지 않음 데이터 이슈 감지 Detectic schema skew 훈련 및 서빙 데이터가 동일한 스키마 구조를 갖지 않음 Detecting distribution skew 데이터셋이 변화함 $\\rightarrow$ covariate나 concept이 이동 Detecting distribution skew   Training Serving Joint $P_{train}(y, x)$ $P_{serve}(y, x)$ Conditional $P_{train}(y|x)$ $P_{serve}(y|x)$ Marginal $P_{train}(x)$ $P_{serve}(x)$ Dataset shift : $P_{train}(y, x) \\neq P_{serve}(y, x)$ Covariate shift : 입력의 분포 변화 $P_{train}(y|x) = P_{serve}(y|x)$$P_{train}(x) \\neq P_{serve}(x)$ Concept shft : 입출력 관계의 변화$P_{train}(y|x) \\neq P_{serve}(y|x)$$P_{train}(x) = P_{serve}(x)$TensorFlow Data Validation ML data를 모니터링하는 도구로 TensorFlow Data Validation(TFDV)를 제안 ML pipeline의 상태를 유지하고자 TFX 사용자를 도와줌 TFDV 사용처 데이터 통계를 생성하고 시각화 데이터 스키마 추론 스키마에 반하는 유효성 검사 데이터 셋의 skew 감지 실제 사용은 깃허브 참조" }, { "title": "[MLOps Specialization / Step2] Labeling Data", "url": "/posts/coursera2_3/", "categories": "Data Engineering, MLOps", "tags": "Data Engineering, MLOps, Coursera", "date": "2022-02-17 19:28:00 +0900", "snippet": "Labeling Data이 포스트는 Coursera의 MLOps Specialization 강의 내용을 기반으로 작성되었습니다.Case Study: Degraded Model Performance 신발을 팔고 있는데 남성 신발(특정 부분)에서 예측력과 AUC가 감소하는 것을 발견 문제의 원인이 되는 요소는 너무 많음 유행이 지난 디자인 비즈니스 구조의 변화 등… What causes problems? 문제의 종류는 2가지가 있음 느린종류(Gradual) : 데이터가 너무 오래되어 변화함 빠른종류(Sudden) : 잘못된 센서, 잘못된 SW 업데이트 점진적인 문제로는 데이터 변화와 세계의 변화가 있음 데이터의 변화 : 트랜드, 계절성, feature 분포 변화에 따른 관련성의 변화 세계의 변화 : 스타일 변화, 프로세스, scope의 변화, 경쟁자의 변화 등…. 급격한 문제로는 데이터 수집 문제와 시스템적 문제가 있음 데이터 수집 문제 : 잘못된 센서, 로그데이터 등… 시스템적 문제 : 잘못된 업데이트, 네트워크 연결 문제, 시스템 다운 등등… Why “understand” the model? 잘못된 예측은 균일하지 못한 비용지출을 야기하므로 이를 모니터링해야함 데이터가 좋지 않아도 어쩔 수 없는 경우가 존재함 유저가 좋지 못한 서비스 경험을 할 수도 있음 무엇보다 실제 세상은 가만히 있지 않음 The real world does not stand still!!Data and Concept Change in Production MLData problems 데이터나 scope가 변화하기도 함 모델과 검증 데이터를 모니터링하면 문제를 빠른시기에 찾을 수 있음 기저지식(ground truth)이 변화 : 새로운 훈련 데이터를 라벨링Problem case Easy problem 기저지식이 몇달, 몇년마다처럼 천천히 바뀌는 경우 모델의 향상, 더 좋은 데이터 등을 기반으로 모델 재학습 진행 선별된 데이터, crowd 기반의 라벨링 진행 Harder problem 기저지식이 몇주마다처럼 빠르게 변하는 경우 모델은 감소하는 모델 성능을 기반으로 재학습 진행 즉각적인 피드백이나 crowd 기반의 라벨링 진행 Really hard problem 기저지식이 몇일 몇시간 단위처럼 매우 빠르게 변화 모델은 감소하는 모델 성능을 기반으로 재학습 진행 즉각적인 피드백, 전문가를 통한 라벨링 진행 Process Feedback and Human Labeling 인간이 라벨링을 하는 경우로는 process feedback과 Human Labeling이 존재Why is lableling important? 비즈니스, 기관에는 많은 경우 라벨링되지 않은 데이터들 빈번하게 모델을 재학습 훈련 데이터셋을 만드는 경우 라벨링이 필요Process Feedback 장점 데이터셋 훈력이 지속적으로 생성됨 라벨이 빠르게 형성 매우 강한 label signal 확보 (eg. 클릭여부) 단점 문제의 고유한 특성의 방해 기저지식 확보 실패 대부분 맞춤형으로 디자인된 경우가 많음 Logstash, Fluentd등을 통해 로그분석을 진행 Google Cloud Loggin, AWS ElasticSearch, Azure Monitor 등을 통해 클라우드 로그 분석 진행Human labeling Rater들이 데이터를 판단하고 직접 라벨링을 진행 라벨링 방법론 라벨링되지 않은 데이터 수집 human “raters” 모집 rater들에게 라벨링 가이드라인 제공 데이터를 rater를 통해 분류 장점 많은 라벨 순수하게 지도적인 학습 (Pure supervised learning) 단점 quality consistency : 데이터에 따라 라벨링이 어려운 경우 존재 specialist에게 부탁하는 경우 비용소모가 큼 느림 " }, { "title": "[MLOps Specialization / Step2] Collectiong Data", "url": "/posts/coursera2_2/", "categories": "Data Engineering, MLOps", "tags": "Data Engineering, MLOps, Coursera", "date": "2022-02-17 19:27:00 +0900", "snippet": "Collecting Data이 포스트는 Coursera의 MLOps Specialization 강의 내용을 기반으로 작성되었습니다.Importance of DataThe importance of data ML에서 데이터는 일급객체 (first class citizen) Software 1.0에서는 우리가 모든 코드를 직접 컨트롤하며 컴퓨터에 명시적으로 지시 Software 2.0에서는 모든 코드를 우리가 고려하지 않음 최적화 기술을 사용해 해결책을 찾음 좋은 데이터는 성공적인 성능의 키 포인트 ML의 데이터는 SW의 코드와 같은 존재 모델은 마법처럼 모든걸 해결해주지 않음 유의미한 데이터가 더 중요 predictive content의 최대화 = 실질적 정보가 있는 데이터 non-informative data 제거 feature space coverage Data pipeline 데이터 파이프라인에 필요한 요소는 다음과 같음 데이터 수집 Data ingestion 데이터 표현 (Data formatting) feature engineering feature 추출 Data monitoring 배포 후 모니터링 요소 시스템 동작 불가능 시간 (downtime) 에러 분포의 변화 Data failure Service failure 본격적으로 이 강의가 만들어진 이유들이 나왔습니다. 최근 많은 머신러닝 모델이 연구되고 각 모델의 발전은 하이퍼파라미터 탐색과 같은 아주 일부분의 것들에 사람들의 인력이 낭비되었습니다.그래서 데이터의 중요성이 많이 대두되며 데이터를 바라보는 관점에 대한 소개를 했습니다.Robert Crowe는 데이터에서 유저를 이해하는데 도움을 받을 수 있다고 합니다. 또한 좋은 데이터는 data coverage와 높은 예측력을 보장해주기도 합니다. 이를 위해 좋은 품질의 데이터를 source, store monitor 해야한다고 합니다.Example Application: Suggesting Runs Robert Crowe는 이해를 돕기 위해 달리기 관련 앱에 대한 예시를 들었음 핵심적으로 고려할 사항을 크게 2가지 정도로 분류 Data availability and collection 어떤 종류/얼마나 많은 데이터가 유용한가? 얼마나 자주 신규 데이터가 들어오는가? 라벨링이 되어 있는가? 아니라면 라벨을 얻기 어려운가…? Translate user need into data needs Data needed Features needed Labels needed 먼저 사용자의 니즈를 알아야 너무 과도하게 많은 데이터를 수집하는걸 방지함 달리기 관련 앱을 예시로 들며 데이터를 바라보는 포인트를 설명했습니다. 데이터 수집/전처리 방법, 데이터셋의 formatting, data효율성 측정 등 여러가지 관점을 이야기 했으며 이를 통해 사용자의 니즈를 파악할 필요가 있다고 했습니다.반대로 사용자의 니즈를 정확히 알아야 너무 과도한 양의 데이터를 수집하는 것을 막을 수 있다고 했습니다.Responsible Data: Security, Privacy &amp;amp; FairnessAvoiding biases in datasets 공개 이미지 데이터셋으로 결혼식에 대한 학습을 진행할 경우 편향적 학습이 발생하여 일반적인 유형의 결혼식이 아닌 경우 제대로 분류하지 못함 이처럼 데이터를 어떻게 수집했는가는 매우 중요한 요소Data security and privacy 데이터 보안과 개인정보에 대한 것도 중요한 요소 유저에게 어떤 데이터를 수집할 지 컨트롤하게 했는가? 실수로 데이터를 공개할 위험이 있는가? 개인정보는 철저하게 안전이 보장되어야 함 Aggregation : 특정인물을 구분가능한 unique value는 summary value로 대체 Redaction : 일부로 덜 완전한 형태를 만들고자 일부 데이터를 제거 Faireness 데이터 자체에 형평성이 존재해야함 문제점들 Representational harm - eg) 고정관념 Opportunity denial은 system이 부정적인 결과를 예측하게 하는 것 Disproportinate product failure은 모형이 치우쳐진 효과 등 왜곡된 표현이 있는 것 데이터 수집시 고려할 것들 모델이 형평성있게 만들기 인간이 레이블링 하거나 데이터를 수집할 때 선입견을 주의 ML model은 편향을 증폭시킬 수 있음 bias를 줄이는 방법 : 형평성있는 labeling 시스템 Automation Human raters 일반인 분야 전문가 유저 피드백 " }, { "title": "[MLOps Specialization / Step2] Introduction to ML Engineering in Production", "url": "/posts/coursera2_1/", "categories": "Data Engineering, MLOps", "tags": "Data Engineering, MLOps, Coursera", "date": "2022-02-17 19:25:00 +0900", "snippet": "Introduction to Machine Learning Engineering in Production이 포스트는 Coursera의 MLOps Specialization 강의 내용을 기반으로 작성되었습니다.Overview 실제 머신러닝 시스템에서 ML Code가 차지하는 것은 매우 적은 부분 그 외에 처리하는 것들이 더 중요하고 많은 부분을 차지 Academic 관점과 Production 관점에서 ML은 차이가 존재   Academic/Research ML Production ML Data Static Dynamic-Shifting Priority for design Highest overall accuracy Fast inference, good interpretability Model training Optimal tunig and training Continuously assess and retrain Fairness Very important Crucial Challenge High accuracy algorithm Entire system Production ML에서의 과제 ML 시스템 통합환경 구축 끊김없는 지속적인 배포 지속적으로 변화하는 데이터 핸들링 컴퓨팅 자원 최적화 ML Pipelines ML pipeline이란 모델의 자동화, 모니터링, 유지보수에 필요한 infrastructure ML Pipeline은 대체적으로 directed acyclic graph(DAG)구조 DAG는 사이클이 없는 방향 그래프 좀 더 심화된 상황에는 사이클이 존재할 수도 있음 파이프라인 통합관리 프레임워크 (pipeline orchestration frameworks) DAG 기반의 ML pipeline의 다양한 구성요소들을 스케쥴링할 필요가 있음 파이프라인 자동화를 도와줌 예시로는 Airflow, Argo, Celery, Luigi, Kubeflow가 있음TensorFlow Extended (TFX) 강의에서는 구글이 제공하는 TensorFlow Extended(TFX)는 ML pipeline 구축에 도움을 줌 큰 규모의 데이터 핸들링이 가능한 sequnce of scalable component 각 단계에 맞는 모듈들을 제공해주고 있음 ExampleGen은 데이터 수집단계에 도움을 줌 StatisticsGen은 데이터의 통계적 정보를 수집 ExampleValidator은 데이터의 문제 상황을 볼 수 있음SchemaGen는 피처의 스키마를 구성해줌Transform은 feature engineering 역할을 함 Tuner와 Trainer는 학습과정과 조정단계에 사용 Evaluator와 InfraValidator는 각각 Deep Learning 분석과 실제로 측정하는 과정에 사용됨이번 강의는 Andrew Ng이 아닌 Robert Crowe가 진행하였습니다. 전반적인 ML pipeline 구성과 관련된 내용을 다루고 있습니다.주요 핵심적인 포인트는 서비스 배포를 위한 파이프라인은 자동화, 모니터링, end-to-end process의 유지보수입니다. 이때 ML 배포에서는 ML 코드가 차지하는 부분은 매우 적습니다.이를 도와주는 도구로 오픈소스인 TFX를 활용할 수 있다고 합니다." }, { "title": "[BoostCamp AI Tech] Day20", "url": "/posts/day20/", "categories": "NAVER BoostCamp AI Tech, Review", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech", "date": "2022-02-16 23:00:00 +0900", "snippet": "Day20 Review당신은 오늘 하루 어떻게 살았나요? 캐글 스터디 발표 및 피드백 MLOps Chapter 1 Week 1 정리 알고보니 안한 Chapter 3 과제오늘 하지 못한 것들 MLops Chapter 2-1 MLops Chapter 3-1 MLops Chapter 4-1 부캠 특강 내용 정리내일은 어떤 것을 할까? MLOps Chapter1, 2 2주차 MLops Chapter 2-1 MLops Chapter 3-1 MLops Chapter 4-1 심화 포스팅 - scipy 통계검정마무리 왜 갑자기 게을러졌나…. 할 게 많은데 밀리지 말고 빠르게 처리 일단 목요일은 위 일정 무조건 진행 금요일 목표 MLOps Chapter 3, 4 2주차 부캠 특강 정리 주말 목표 MLOps Chapter 1, 2 2주차 정리 가능하면 3, 4주차까지 정리 " }, { "title": "[MLOps Specialization / Step1] Deployment", "url": "/posts/coursera1_2/", "categories": "Data Engineering, MLOps", "tags": "Data Engineering, MLOps, Coursera", "date": "2022-02-16 19:23:00 +0900", "snippet": "Deployment이 포스트는 Coursera의 MLOps Specialization 강의 내용을 기반으로 작성되었습니다.Key challenges ML model의 배포에서 핵심과제 2가지 ML &amp;amp; Statistical issues SW engine issues ML 배포에서는 detect change와 mange change가 중요Concept drift and Data drift 실서비스에서는 데이터가 변하는 경우가 많은데, 크게 2가지 케이스가 존재 점진적 변화의 경우 새로운 단어가 나타나는 등의 변화를 말함이런 변화는 자주 일어나진 않고 낮은 빈도로 발생 급격한 변화의 경우 COVID-19이 발생한 후 개인의 소비패턴이 급격히 변화하여 신용카드 사기 시스템이 제대로 동작하지 못한 것처럼 급격한 데이터 분포의 변화를 말함 Concept Drift : $X\\rightarrow Y$의 mapping 관계가 변하는 경우 ex) COVID-19이후 online shopping 증가로 신용카드 사용량 급증 기존에 $X$에 따른 $Y$의 mapping 관계에서 변화가 나타난 경우 Data Drift : input data의 분포가 변경되는 경우 ex) 특정 영화, 인물이 갑작스러운 유명세를 타는 경우 / 주변 밝기의 변화 deploy 후 데이터가 변경되는 경우 훈련데이터와 테스트 데이터 관점에서도 존재 Training set purchased data, historical user data user data의 경우 privacy issue를 반드시 체크 Test set 최근 데이터에서 잘 동작하지만 시간이 흐른뒤 성능이 떨어지는 경우가 발생할 수 있음 Software engineering issues SW적 관점에서 확인해야 할 것 목록 Realtime or Batch Realtime : 즉각적인 음성인식처리 Batch : 하루동안 환자의 전반적 의료기록 Cloud VS Edge/Browser 최근에는 Cloud 환경에서 자주 진행 browser도 wifi 성능 향상등의 이유로 성능이 좋아짐 Compute resource (CPU/GPU/memory) SW 구조 설계에 도움 Latency, throughput(QPS) Logging system building시 데이터만큼 유용하게 활용 analysis review, retraining algorithm 등… Security and PrivacyFirst deployment VS maintenance 서비스 배포에서는 소프트웨어적인 포인트가 중요 이후 모니터링 과정에서는 data drift, concept drift 처리가 중요Deployment patterns 배포를 진행하는 경우는 3가지 정도가 있음 새로운 상품과 기술 수동진행 과업의 자동화 이전 ML 시스템 대체 배포과정에서 핵심 포인트 점진적으로 배포과정을 증가하면서 모니터링 검증이 덜된 algorithm이라면 적은 양의 traffic부터 시작해서 지속적 모니터링을 통한 트래픽 조절 롤백 알고리즘이 동작하지 않으면 이전 시스템으로 롤백 배포 과정은 단순히 배포만하고 끝나지 않습니다. 이런 다양한 배포 과정에서 문제를 최소화하기 위해 다양한 방식으로 배포를 합니다.강의에서는 핸드폰 액정 기스 불량품 분류 모델 예시를 들었습니다.배포 초기 과정 ML 시스템은 인간과 함께 동작하며 인간의 결정을 대부분 따라함(ML system shadows the human) ML 시스템이 결정은 이 과정동안에는 사용되지 않음 이 과정에서는 인간의 판단과 비교 데이터를 수집가능 인간의 결정과 다른 ML 시스템의 결정을 기반으로 ML 시스템의 예측을 검증 가능Canary deployment 모델의 실제 의사결정 과정이 준비가 된 경우 적용하는 배포방식 아주 약간의 트래픽 (약 5%정도)에 대해서만 algorithm을 적용 적용이후 지속적인 모니터링을 통해 성능에 확신이 드는 경우 트래픽을 점진적으로 증가적용Blue Green deployment 기존 배포 혹은 최초 배포된 버전을 Blue vesion(Old version)이라고 함 새로운 모델 및 버전(Green Version)이 나오면 배포를 진행 이때 기존에 있는 blue version이 사라지는 것이 아니고 특정 서비스에서는 사용될 수 있음 Green version에서 문제가 발생하면 blue version으로 롤백 진행Degree of automation 왼쪽으로 갈수록 인간, 오른쪽으로 갈수록 자동화 성능이 강화 전반적인 배포과정의 자동화 단계라고 볼 수 있음 중간단계인 AI assistance와 Partial automation 단계에서는 사람의 도움이 함께 존재(human in the loop)MonitoringMonitoring dashboard 모니터링 dashboard에는 server load, non-null output, missing input 등의 정보를 보여줌 일반적으로 각 지표들의 상하한을 결정해서 기준점으로 삼음 무언가 잘못된 지표를 보인다면 brainstorm을 통해 해결책을 찾아야함 초기에 많은 metric을 쓰는 것은 괜찬지만 점진적으로 필요가 없는 것은 제거하는 것이 좋음Example of metrics to track software metrics memory, compute, latency, throughput, server load Input metrics : distribution change를 측정 Avg input length Avg input volume Num missing value Avg image brightness Output metrics # times return null # times user redoes search # times user switches to typing CTR (click thorough rate) $\\rightarrow$ web search 지속적인 추적을 해야하는 지표는 다양하게 있습니다. SW적인 부분도 추적을 지속적으로 해야하며 Input과 Output에 대한 지표도 지속적으로 추척해야합니다.Input에서 평균 길이나 볼륨이 변화하면 알고리즘의 성능저하에 직접적인 영향이 있을 수 있다고 합니다.Output에서는 사용자의 입력이 없는 경우나 음성인식의 경우 음성인식을 포기하고 직접 입력하는 등과 같은 행위적인 추적을 할 필요도 있습니다.두 경우 모두 팀원들과의 논의를 통해 MLOps 설계시 두 지표를 모두 추적하게 설계할 필요가 있습니다.왼쪽은 모델 개발, 오른쪽은 모델의 서비스 배포 관점의 반복적 흐름입니다.모델의 연구 개발관점에서는 학습과 성능 향상에 집중해서 반복적 학습을 진행합니다.서비스 배포적 관점에서는 시스템의 동작과 성능분석을 진행하고 재배포를 진행합니다. 일반적으로 특정 metric monitoring $\\rightarrow$ 지표결정 $\\rightarrow$ 지표 고정 $\\rightarrow$ 새로운 metric setting의 과정을 반복합니다.Model maintenance 모델의 유지보수 과정에서는 개발자가 직접 컨트롤하는 Manual retraining과 Automatic retraining이 있음 대부분의 개발자는 직접 컨트롤 하는 manual retraining을 선호함 재학습과 새로운 방식의 적용을 완전 자동화하는 것을 개발자는 꺼린다고 함Pipeline monitoringSpeech recognition example 음성인식 케이스에서 일반적으로 바로 음성인식 처리를 하는 경우도 있지만 일부 기기는 voice activity detection 모듈이 있는 경우가 존재 이런 경우 추가 모듈에 의해 성능이 감소할 수 있음$\\rightarrow$ 연결된 모듈은 하나가 다른 하나에 영향을 미치기 때문User profile example user profile을 통해 추천목록을 작성하는 경우 유저 데이터 변경을 고려해야함 추천/비추천/알 수 없음으로 분류하는 ML system이었다고 가정하면 기존에는 알 수 없음의 비중은 높지 않음 시간이 흘러 user profile의 분포가 변경되면 y의 분류가 알 수 없음이 되는 경우가 많음 그 결과 성능자체에 영향을 미치게되고 이런 비정상적 비율 증가 사실을 알려줘야 성능에 영향이 덜 함 pipeline에서는 위와 같은 cascading effect를 추적하기가 어려움Metrics to monitor Monitor SW metrics Input metrics Output metrics pipeline의 개별구성요소 metric 여러 변화로 인한 unknown 증가와 같은 유지보수를 위한 사항을 알려줌 데이터 변화 속도 $\\rightarrow$ problem dependent 유저 데이터는 일반적으로 천천히 변화함 ex) 얼굴인식 변화는 느림 기업 데이터는 빠르게 변화함 ex) 패션 데이터의 변화는 빠름 " }, { "title": "[MLOps Specialization / Step1] The ML Project Lifecycle", "url": "/posts/coursera1_1/", "categories": "Data Engineering, MLOps", "tags": "Data Engineering, MLOps, Coursera", "date": "2022-02-16 19:20:00 +0900", "snippet": "The Machine Learning Project Lifecycle이 포스트는 Coursera의 MLOps Specialization 강의 내용을 기반으로 작성되었습니다.Step of an ML Project ML project는 크게 4단계의 구성으로 이루어져 있음 Scoping Define project : project의 목적성을 정의 Data Define data and establish baseline : 데이터를 정의하고 기준점 설정 Label and organize data : 데이터의 상태에 따라 라벨링 및 구조화 Modeling Select and train model : 여러 모델 후보군을 설정하고 적절한 모델 선택 Perform error analysis : 모델의 에러 분석 Deployment Deploy in production : 실제 서비스 배포 Monitor &amp;amp; maintain system : 배포한 서비스를 지속적 모니터링 4가지의 과정은 단순히 한방향으로만 흐르는 것이 아닌 상황에 따라 이전 단계로 돌아갈 수 있음 많은 개발자들이 배포(Deplyment)단계에서 서비스를 배포하는 것이 끝인 줄 알지만 Andrew Ng은 배포 후 모니터링까지가 완전한 배포단계라고 함Andrew Ng은 전반적인 서비스 배포 단계에서 배포를 제외한 다른 부분은 동일하게 흘러가지만 배포단계에서 단순히 배포만 하는 것을 끝이라고 생각하는 것을 문제점으로 제시했습니다. Andrew Ng은 이것은 배포 단계 중 절반정도이며 나머지 절반은 배포 후 모니터링과 유지보수까지가 완전한 배포단계라고 했습니다.좀 더 이해를 하고자 음성 인식 서비스를 배포하는 과정을 예로 들었습니다.Case study : speech recognitionScoping Stage Scoping 단계에서는 프로젝트의 목적을 학실히하는 것이 중요 음성인식 서비스의 목적은 voice search에 필요한 음성인식 음성인식 성능을 측정하기 위한 핵심 측정 지표를 설정해야함 이는 매우 문제 의존적임 정확도, 지연율, 처리속도 등… 프로젝트에 사용될 resource와 전반적인 일정 설계도 요구됨Data Stage 데이터를 정의하는 단계 ML 연구와 서비스 배포에 필요한 데이터의 정의는 다름 ML 연구 : 머신러닝 성능 향상을 위한 고정된 데이터셋 서비스 배포 : 필요한 경우 train/test 데이터셋을 수정할 수도 있음 데이터 라벨이 일관적인가? 같은 데이터에따라 다양한 라벨링이 붙을 수 있음 예시에는 “Um, today’s weather”, “Um… today’s weather”, “Todya’s weather”처럼 하나의 음성을 다른 라벨링이 붙을 수도 있음 이 경우 모델이 처리하는 과정에서 문제가 발생 음성인식 케이스는 주변 소음 처리도 중요할것임 어떤 데이터는 음성이 과도하게 크거나 작을 수도 있으므로 볼륨의 정규화도 필요데이터를 설정하는 과정에서는 다양한 관점으로 데이터의 특징을 살려야합니다. Andrew Ng은 최근에 들어 데이터의 중요성을 많이 강조하고 있습니다. production이든 research든 모델을 구축하는 과정에선 더 많은 데이터 수집보다, 분석을 통해 정확한 수집 data를 파악하는 것이 모델구축에 더 효율적이라고 합니다.Modeling Stage 모델 선정과정에서 Reseach와 Production 관점에 따라 주요하게 바라보는 것이 다름 Research/Academia Code (algorithm/model) Hyperparameters Product Team Hyperparameter Data 에러분석 과정에서는 어떻게 systematically 하게 data를 향상할지를 고민 Hyperparamter를 수정하는 것은 코드나 데이터를 수정하는 것보다 미비한 효과가 나타남연구와 서비스관점에서 모델링은 차이가 나타납니다. 연구관점은 데이터를 고정하고 나머지를 변경하는 것이 주 목적이지만 서비스 배포관점에서는 코드를 고정하고 나머지를 변경하는 것이 더 효율적일 수 있습니다.모델링 과정에서 원하는 정도의 퍼포먼스가 나오지 않는다면 데이터 관점으로 돌아가는 것도 좋은 방법이라고 합니다.Deployment Stage 배포 과정에서는 서비스를 배포하는 것보다 이후 모니터링 과정도 중요 모니터링 과정에서 음성인식의 분포가 변화할 수 있음 예를 들면 주 데이터 분포가 어린 나이 목소리였으나 이후 시간이 지나 목소리 연령층이 올라가며 분포가 변하는 경우가 존재 데이터 분포나 라벨 매핑 등이 변화하는 Concept/Data Drift가 발생하는 것을 모니터링 해야함MLOps란? 위의 전체 과정을 사람이 직접하기엔 너무 느림 ML lifecycle을 지원해주는 sw tool이 MLOps의 핵심 아이디어" }, { "title": "[BoostCamp AI Tech] Day19", "url": "/posts/day19/", "categories": "NAVER BoostCamp AI Tech, Review", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech", "date": "2022-02-15 23:00:00 +0900", "snippet": "Day19 Review당신은 오늘 하루 어떻게 살았나요? 특강 공부 카카오 아레나 CF 코드 발표 캐글 스터디 분석 완료 및 발표자료 완성오늘 하지 못한 것들 Coursera MLOps 정리 부캠 특강 내용 정리내일은 어떤 것을 할까? Coursera MLOps 정리 부캠 특강 내용 정리 MLOps Chapter1, 2 2주차마무리 내가 지금 하고 있는 것들 부캠 강의 MLOps Coursera 캐글 스터디 카카오 아레나 스터디 가끔 논문리뷰 하고 있는게 너무 많다. 일단 스터디가 2개나 있는건 좀 사치인거 같기도…. 카카오 아레나 스터디는 포기하는게 나을듯 싶음" }, { "title": "[BoostCamp AI Tech / Level 3 - Product Serving] Day19 - 서비스 향 AI 모델 개발", "url": "/posts/day19_1_service_model/", "categories": "NAVER BoostCamp AI Tech, Level 3 - Product Serving", "tags": "NAVER, BoostCamp, AI Tech, Product Serving", "date": "2022-02-15 13:00:00 +0900", "snippet": "Product Serving : 서비스 향 AI 모델 개발서비스 AI 모델 개발 VS 연구 AI 모델 개발 연구 관점의 AI 개발은 정해진 데이터셋과 평가 방식에서 더 좋은 모델을 찾는 것이 목표 서비스의 AI 개발은 데이터셋과 테스트 방법이 없고 서비스 요구사항만 있는 경우가 많음1. 학습 데이터셋 준비 서비스 개발에 있어 가장 우선적으로 할 일은 학습 데이터셋을 준비하는 것 서비스 요구 사항에 맞는 데이터셋의 종류/수량/정답을 정해야 함 데이터 셋을 준비할 때는 구체적인 요구사항을 파악하는 것이 중요하고 데이터의 예외도 충분히 고려할 필요가 있음 종류를 정의할 때는 데이터의 특징점을 기준으로 나누는데, 이때 기준에 따라 전반적인 데이터의 수량과 방향이 달라짐 기술 모듈을 설계해야하는데, 입력에 맞는 결과를 내보내는 과정을 설계해야 함 데이터를 모으면 예상치 못한 경우가 많이 나타남 이런 경우 기본적인 입출력 모듈뿐 아닌 전처리 단계의 모듈도 필요 정답은 AI 모델별로 정답의 케이스가 달라지는데, 하나의 역할을 하는 모델에서도 여러가지 모델로 나눠서 조합을 하는 게 더 좋을 수도 있음 필요에 따라 데이터는 외주를 통해 처리하는 경우도 있음2. 테스트 방법 필요한 테스트 데이터 셋과 훈련 셋을 활용해 모델을 학습 간단한 예시로 게임대전 AI를 만든다고 하자 이때 프로게이머의 스킬 사용, 움직임을 녹화해서 10FPS단위로 학습한다고 하자 이런 경우 AI는 아무런 스킬도 쓰지 않고 움직이기만 함 이유는 실제 프로게이머는 스킬의 사용은 중요한 타이밍에만 사용하고 대부분은 특별한 행동을 하지 않음 따라서 모델은 “대부분 행동을 하지 않음”이라는 학습을 하기 때문에 정확도는 99%지만 정작 승률은 0%가 나오는 기현상이 발생 실 서비스 적용전 개발환경에서 평가를 Offline 테스트라고 함 실 서비스 적용시 평가를 Online 테스트라고 함 이 둘의 이질감이 상당히 클 수가 있는데, 이 차이를 줄이는 offline 테스트를 잘 설계해야함   offline online 정량평가 완벽하지 않으므로 모델 후보군 선택으로 활용 해당 모델을 서비스 시나리오에서 자동 정량 평가 정성평가 각 후보 모델에 대한 분석 후 서비스 출시 버전 선택 Voice of Customer로 평가 및 개선점 파악 추가적으로 모델에 대한 처리시간, 목표 정확도, 목표 qps, Serving 양식, 장비 사양 등을 고려하는 요구사항을 도출할 필요가 있음 요구사항들은 offline test와 online test에 차이가 있기 때문에 잘 구분해서 확인해야 함조직구성 크게 구성하면 모델팀, 서빙팀으로 구분됨1. 모델팀 모델링 팀 모델러는 AI 모델을 개발하는 팀 모델 구조, 성능 분석, 디버깅 등을 진행 데이터 팀 데이터 팀은 데이터 관리를 담당 학습 데이터 준비 정량 평가 수립 정성 평가 분석 DevOps 팀 (Tool) 필수적인 팀은 아니지만 효율성을 위해 필요 라벨링 툴, 모델 분석 툴, 개발 자동화, 파이프 라인 개발등 모델 개발에 필요한 데브옵스(MLOps)를 개발하는 역할 모델 관리 매니저 전체적인 팀을 총괄하는 매니저 전체 모델의 품질 관리를 담당 2. 모델 서빙팀 모델 엔지니어 Mobile, GPU Server, toolkit, 경량화 등 전반적이 모델을 심어주는 역하 데이터 엔지니어에 해당하는 역할군이라 생각하면 편할듯 모델을 직접 개발해서 적용하는 경우도 있으므로 개발, ML에 대한 전반적인 지식이 필요함 APP / BE 개발자 각 역할에 맞춰 GPU/CPU 처리를 하거나 모바일에 이식하는 역할 " }, { "title": "[BoostCamp AI Tech / Level 3 - Product Serving] Day18 - MLflow", "url": "/posts/day18_MLflow/", "categories": "NAVER BoostCamp AI Tech, Level 3 - Product Serving", "tags": "NAVER, BoostCamp, AI Tech, Product Serving, MLOps, MLflow", "date": "2022-02-14 16:00:00 +0900", "snippet": "Product Serving : MLflowMLflow 머신러닝 실험, 배포를 쉽게 관리할 수 있는 오픈소스 과련 오픈소스 중 제일 빠르게 성장 CLI, GUI 지원MLflow의 목적 실험의 지속적 추적 코드의 재현 모델의 패키징과 배포의 용이성 모델 관리를 위한 중앙 저장소MLflow 핵심기능 Experiment Management &amp;amp; Tracking 머신러닝 실험 관리와 실험 내용을 기록할 수 있음 추가적으로 하나의 MLflow 서버 위에서 자기 실험을 공유 가능 Model Registry MLFlow로 실행한 머신러닝 모델을 Model Registry에 등록 가능 Model Serving Model Registry에 등록한 모델을 REST API 형태의 서버로 서빙할 수 있음 Input = model input Output = model output 직접 도커 이미지를 안 만들어도 생성 가능 MLflow Component MLflow Tracking 머신러닝 코드 실행, 로깅을 위한 API, UI 결과를 local과 server에 기록해 실행을 비교할 수 있음 다른 사용자의 결과와 비교하며 협업가능 MLflow Project 머신러닝 프로젝트 코드를 패키징하기 위한 표준 MLflow Tracking API를 사용하면 MLflow는 프로젝트 버전을 모든 파라미터와 자동으로 로깅 MLflow Model 모델은 모델 파일과 코드로 저장 다양한 플랫폼에 배포할 수 있는 여러 도구 제공 API를 사용하면 자동으로 해당 프로젝트 내용을 사용 MLflow Registry 전체 Lifecycle에서 사용할 수 있는 중앙 모델 저장소 간단한 MLflow 작업 해보기MLflow project settingmlflow experiments create --experiment-name [project name] mlflow명령어를 통해 프로젝트를 생성하면 작업 폴더에 Default 프로젝트와 함께 mlruns 폴더에 프로젝트가 생성됨Machine Learning Code &amp;amp; MLProject# train.pyimport numpy as npfrom sklearn.linear_model import LogisticRegressionimport mlflowimport mlflow.sklearnif __name__ == &quot;__main__&quot;: X = np.array([-2, -1, 0, 1, 2, 1]).reshape(-1, 1) y = np.array([0, 0, 1, 1, 1, 0]) penalty = &quot;elasticnet&quot; l1_ratio = 0.1 lr = LogisticRegression(penalty=penalty, l1_ratio=l1_ratio, solver=&quot;saga&quot;) lr.fit(X, y) score = lr.score(X, y) print(&quot;Score: %s&quot; % score) mlflow.log_param(&quot;penalty&quot;, penalty) mlflow.log_param(&quot;l1_ratio&quot;, 0.1) mlflow.log_metric(&quot;score&quot;, score) mlflow.sklearn.log_model(lr, &quot;model&quot;) 간단한 머신러닝 코드를 작성하고 MLProject파일과 반드시 같은 폴더에 있어야함name: tutorialentry_points: main: command: &quot;python train.py&quot; MLProject 파일에 위와 같이 적어준다. 이때 탭키로 구분하면 에러가 나는 것으로 보인다. 정확히 character ‘\\t’ token 에러가난다. 스페이스바 2칸을 indentation으로 맞춰주자… MLflow Trackingmlflow run logistic_regression --experiment-name [project name] --no-conda 코드를 실행하면 MLProject가 위치한 곳에서 코드가 수행됨mlflow ui ui로 실행하면 localhost:5000으로 수행됨 자신이 설정한 프로젝트 이름으로 들어가면 모델의 수행 결과가 기록됨MLflow autolog# train.pyimport numpy as npfrom sklearn.linear_model import LogisticRegressionimport mlflowimport mlflow.sklearnif __name__ == &quot;__main__&quot;: mlflow.sklearn.autolog() X = np.array([-2, -1, 0, 1, 2, 1]).reshape(-1, 1) y = np.array([0, 0, 1, 1, 1, 0]) penalty = &quot;elasticnet&quot; l1_ratio = 0.1 lr = LogisticRegression(penalty=penalty, l1_ratio=l1_ratio, solver=&quot;saga&quot;) with mlflow.start_run() as run: lr.fit(X, y) score = lr.score(X, y) print(&quot;Score: %s&quot; % score) autolog를 활용하면 파라미터를 명시하지 않아도 활용이 가능 단, 모든 프레임워크가 가능한 것은 아님 PyTorch는 지원하지 않지만 PyTorch Lightning은 지원 MLProject파일에 파라미터를 세팅해주면 파라미터 튜닝도 가능함" }, { "title": "[BoostCamp AI Tech / Level 3 - Product Serving] Day18 - Docker", "url": "/posts/day18_docker/", "categories": "NAVER BoostCamp AI Tech, Level 3 - Product Serving", "tags": "NAVER, BoostCamp, AI Tech, Product Serving, MLOps, docker", "date": "2022-02-14 14:00:00 +0900", "snippet": "Product Serving : Docker가상화 개발 환경에서는 서버에 직접 들어가서 개발을 하지 않음 local개발 $\\rightarrow$ staging 서버 $\\rightarrow$ production 서버 배포 과정을 거침 local 환경과 production 서버 환경이 다른 경우 배포시 제대로 동작하지 않을 수도 있음 실제 환경에서는 운영하는 서버가 1대가 아니라 수십, 수백대인 경우도 존재 서버 환경 자체를 소프트웨어화하는 방법을 위해 가상화환경이 등장Virtual Machine docker 이전에는 가상화 환경으로는 주로 VM(Virtual Machine)을 활용 기본적인 원리가 Host OS 위에 Virtual OS를 구동시키는 방식이라 리소스 소모도 크고 성능 자체를 완전하게 발휘하기 어려움 로컬에서 VM을 동작하는 것이 부담되므로 AWS, GCP와 같은 클라우드 컴퓨팅 기반의 원격 서버가 등장 하지만 기본적으로 VM은 리소스 소모가 많기 때문에 이를 해결하고자 container 기술이 등장하고 이를 기반하여 docker가 개발됨Docker container 기술을 기반에 둔 오픈소스 프로그램 Docker image를 활용하여 동일한 환경을 공유할 수 있음Docker 원리 도커는 Docker file $\\rightarrow$ Docker Image $\\rightarrow$ Docker Contatiner 순서로 동작함 Docker Image 컨테이너를 실행할 때 사용할 수 있는 템플릿 여러가지 setting을 설치한 일종의 set Read Only Docker Container Docker Image를 활용해 실행된 인스턴스 Write 가능 서버 실행 방식은 docker hub, GCR, ECR 등 Container Registry에 도커 이미지를 업로드하고 서버에서 받아서 실행하는 방식Docker 설치 Docker Install에 접속해서 설치가 가능docker실행시 커맨드가 정상적으로 동작하면 설치완료docker 실행하기docker pull [image info]pull 명령어를 사용하면 docker 이미지를 다운받을 수 있습니다. 보통 docker desktop을 쓰는 경우라면 실행해주시고 명령어를 실행하셔야 합니다. pull을 통해 이미지가 다운되었으면 아래처럼 이미지가 등록된 것을 볼 수 있습니다.docker images위 명령어를 실행해도 다운받아진 이미지들 목록을 확인할 수 있습니다.docker run --name [container name] -e [environment] -d -p [local port:container port] imagedocker run을 통해 도커를 실행하면 해당 환경이 실행이 됩니다. 여러가지 옵션을 넣어서 실행할 수 있는데, -d는 컨테이너의 실행을 백그라운드로 실행하는 것을 의미합니다. 그렇다면 실행된 환경을 확인해봐야겠죠?docker exec -it [container name/ID] /bin/bash실행중인 container에 진입하려면 exec 명령어를 사용하면 됩니다. 다른 AWS나 GCP에 접속하는 방식이랑 유사합니다.docker image 만들고 배포docker image를 만드는 연습을 하고자 간단한 FastAPI 코드를 활용한 도커 이미지 생성 해보겠습니다. 우선 특정 폴더를 만들고 터미널에서 아래의 virtual environment를 설정해줍니다.python -m venv .venvsource ./venv/bin/activatepip install pip --upgradepip install &quot;fastapi[all]&quot;그 후 간단한 API를 작성해봅시다.from pip import mainfrom fastapi import FastAPIimport uvicornapp = FastAPI()@app.get(&quot;/hello&quot;)def hello(): return { &quot;message&quot;: &quot;This is hello world!&quot; } if __name__ == &#39;__main__&#39;: uvicorn.run(app, host=&quot;0.0.0.0&quot;, port=8001) localhost:8001/hello로 접속하면 This is hello world를 json 형태로 전달해주는 간단한 코드입니다.동일한 파이썬 라이브러리 세팅을 위해 터미널에서 pip freeze &amp;gt; requirements.txt를 입력해서 파일로 저장해줍시다. 그 후 Dockerfile을 만들어서 docker 세팅을 해줍시다. 파일이름을 Dockerfile로 하면 자동적으로 인식을 해줍니다.FROM python:3.8.7-slim-busterCOPY . /appWORKDIR /appENV PYTHONPATH=/appENV PYTHONUNBUFFERED=1RUN pip install pip==21.2.4 &amp;amp;&amp;amp; \\ pip install -r requirements.txt CMD [&quot;python&quot;, &quot;main.py&quot;]이렇게 도커 이미지를 만들고 터미널에서 아래처럼 입력해주면 도커 이미지가 만들어집니다.docker build . -t [image name:tag].은 Dockerfile이 위치한 경로를 지정해주면 되는데, 보통 같은 폴더내에서 동작하기도 하고, 이름도 Dockerfile로 많이 짓다보니 .을 많이 적습니다.이렇게 하면 도커 이미지가 생성되고 앞서 수행한 방법처럼 이미지를 빌드하면 동일하게 수행이 가능합니다!" }, { "title": "[BoostCamp AI Tech / Level 3 - Product Serving] Day18 - Linux &amp; Shell Command", "url": "/posts/day18_2_linux/", "categories": "NAVER BoostCamp AI Tech, Level 3 - Product Serving", "tags": "NAVER, BoostCamp, AI Tech, Product Serving, Linux", "date": "2022-02-14 13:30:00 +0900", "snippet": "Product Serving : Linux &amp;amp; Shell CommandShell쉘의 종류 쉘 : 사용자가 문자를 입력해 컴퓨터에 명령할 수 있도록 하는 프로그램 터미널/콘솔 : 쉘을 실행하기 위해 사용자 입력을 받아 컴퓨터에 전달 sh : 최초의 쉘 bash : Linux 표준 쉘 zsh : Mac OS Catalina 표준 쉘쉘을 사용하는 경우 서버에서 접속해서 사용하는 경우 데이터 전처리를 위해 쉘 커맨드를 사용 Docker를 사용 수백대의 서버를 관리 Test code 실행 배포 파이프라인 실행Shell Commandmanman python 쉘 커맨드의 매뉴얼 문서를 확인 종료는 :q 입력Directory commmand mkdir mkdir test test라는 이름의 폴더를 생성 ls ls ls -l ls -al ls -lh 현재 접근한 폴더의 폴더, 파일을 확인 실행 옵션 -a : . 포함 모든 파일, 폴더를 출력 -l : 권한, 소유자, 만든 날짜, 용량까지 출력 -h : 용량을 사람이 읽기 쉽도록 GB, MB로 표시 pwd pwd 현재 폴더 경로 (Current Working directory)를 보여줌 cd cd test 폴더 변경, 폴더 이동 I/O Command echo echo &quot;hi&quot; echo `pwd` print처럼 터미널에 텍스트를 출력 echo `command`를 입력하면 쉘 커맨드의 결과를 출력 vi vi test.txt vim 편집기로 파일 생성 i를 눌러 INSERT 모드를 활성화하고 수정 ESC를 누르고 :wq를 누르면 저장하고 나가기 :wq!는 강제저장 후 나가기 :q는 그냥 나가기 Shell Command bash bash test.sh shell script를 bash를 통해 실행 sudo sudo rm -rf / 관리자 권한으로 실행하는 경우 sudo를 붙여서 실행 superuser do의 약자로 최고 권한자의 권한으로 명령어를 수행함 cp, mv cp test.sh vi-test2.sh mv test.sh vi-test3.sh cp와 mv는 둘다 비슷하게 사용함 cp는 파일이나 폴더를 복사함 폴더를 복사할 때는 옵션으로 -r을 사용해야 함 mv는 파일, 폴더를 이동할 때 쓰지만 이름을 바꿀때도 사용함 find find . -name &quot;File&quot; 현재 폴더에서 File이란 이름을 갖는 파일 및 디렉토리 검색 export export water=&quot;물&quot; echo $water # 물 export로 환경 변수를 설정 export로 환경 변수 설정한 경우 터미널이 꺼지면 사라짐 매번 쉘을 실행할 때 설정하려면 .bashrc나 .zshrc에 저장하면 됨 내용을 저장하고 source ~/.bashrc나 source ~/.zshrc를 수행하면 적용됨 alias alias ll2=&#39;ls -l&#39; alias는 명령어들을 간단히 줄여 단축 키워드처럼 사용하게 해줌 ETC command sort cat fruits.txt | sort 내용을 오름차순으로 정렬 -r 옵션은 내림차순 정렬 uniq cat fruits.txt | uniq cat fruits.txt | sort | uniq 중복된 행이 연속으로 있는 경우 중복 제거 일반적으로 sort와 함께 사용 -c 옵션은 중복 행의 개수 출력 grep grep [option] [pattern] [filename] 특정 정규표현식에 맞는 라인을 검색 options -i : insnsitively, 대소문자 구분없이 -w : 정확하게 그 단어만 찾기 -v : 특정 패턴 제외한 결과 출력 -E : 정규 표현식 사용 Server command ps ps -ef 현재 실행되고 있는 프로세스 출력 options -e : 모든 프로세스 -f : Full Format으로 자세히 보여줌 curl curl -X localhost:5000/{data} httpie등 여러가지가 있고 더 가독성 좋게 출력해줌 REST API 기반 request를 테스트 가능 scp scp local_path user@ip:remote_directory # local -&amp;gt; remote scp user@ip:remote_directory local_path # remote -&amp;gt; local SSH를 이용해 네트워크로 연결된 호스트 간 파일을 주고 받는 명령어 options -r : 재귀적으로 복사 -P : ssh 포트 지정 -i : SSH 설정을 활용해 실행 nohup nohup python3 app.py &amp;amp; 터미널 종료 후에도 계속 작업이 유지되도록 백그라운드 실행 실행을 위해선 permission이 755여야 함 종료는 ps -ef | grep app.py 후 kill -9 pid로 프로세스 kill 로그는 nohup.out에 저장 chmod chmod [filename] [mode] 특정 파일의 mode를 설정 3자리 숫자로 모드는 표현되며 각자리는 2진수 3비트로 Read, Write, Execution 역할을 함 일반적으로 755, 644로 퍼미션을 줌 Shell Script#!/bin/bashSTART=$(date + %s)echo &quot;Calculate run-time&quot;sleep 3END=$(date + %s)DIFF_SECOND=$(( $END-$START ))DIFF_MINUTE=$(( $DIFF_SECOND / 60 ))echo ${DIFF_SECOND} .sh 확장자 파일로 돌아감 쉘 커맨드가 동작하며 bash 명령어로 수행가능" }, { "title": "[BoostCamp AI Tech / Level 3 - Product Serving] Day18 - ML Project Life Cycle", "url": "/posts/day18_1_mllifecycle/", "categories": "NAVER BoostCamp AI Tech, Level 3 - Product Serving", "tags": "NAVER, BoostCamp, AI Tech, Product Serving, MLOps", "date": "2022-02-14 13:00:00 +0900", "snippet": "Product Serving : ML Project Life Cycle문제 정의의 중요성 문제 정의 : 특정 현상을 파악하고 현상에 있는 문제(Problem)를 정의하는 과정 문제를 잘 풀기 위해서는 문제 정의(Probelm Decision)가 매우 중요 기본적으로 문제 인식을 잘 해야 이후 프로세스를 짤 수 있음 ML project에서는 How 보다는 Why에 집중하는 것이 중요프로젝트 Flow1. 현상파악 발생한 현상을 분석하고 현재 상황을 파악 어떤 일이 발생하는가? 문제의 어려움이 무엇인가? 추가적으로 무엇을 해볼 수 있을까? 어떤 가설을 만들어 볼 수 있을까? 어떤 데이터가 있을까? 2. 구체적 문제 정의 무엇을 해결하고 싶은가? 무엇을 알고 싶은가? 앞서 발생한 현상을 좀 더 구체적이고 명확한 용어로 재정의 해결 방안을 고려할 때, 비용적 측면과 시간적 제약 등 현실적인 방안을 잘 고려해야 함 데이터로 할 수 있는 일을 진행하되 무조건 알고리즘이 최고의 접근은 아닐 수 있음 원인을 파고들면 구체적 해결책이 나올 수 있음 문제를 쪼개서 분석 여러 해결방안을 확인 점진적 실행 3. 프로젝트 설계 문제 정의에 기반해서 프로젝트를 설계 해결하고자 하는 문제 구체화 머신러닝 문제 타당성 확인 흥미성보다는 제품, 회사 비즈니스에서 어떤 가치를 줄 수 있느냐가 중요 필요한 데이터 종류, 기존 모델이 있는지를 고려 반드시 머신러닝만이 답이 아닐 수도 있음 머신러닝이 사용되면 좋은 경우 특정 패턴이 존재하거나 발견되는 경우 목적 함수 : 학습을 위한 목적함수를 만들 수 있어야함 복잡성 : 어느정도 패턴이 복잡함을 가져야함 너무 간단한 문제라면 오히려 과한 사용일 수 있음 데이터 존재 여부 : 데이터가 존재하거나 수집 가능해야 함 반복 : 사람이 반복적인 수행을 하는 경우 = 패턴이 존재 머신러닝을 사용하면 좋지 않은 경우 비윤리적 문제 간단한 case 좋은 데이터 수집이 어려운 경우 예측 오류가 치명적인 결과를 내는 경우 target과 metric을 설정하는 것이 중요 Goal : 프로젝트의 큰 목적 Objectives : 목적 달성을 위한 세부단계 목표 NSFW (Not Safe For Work)는 사용자에게 불쾌감을 주므로 조심해야함 윤리적인 문제를 반드시 고려해야함 (극단적 클릭 유도로 자극적 컨텐츠 노출) Multiple Objective Optimization은 서로 충돌이 발생할 수 있으므로 선정 기준을 잘 선택해야함 데이터의 value와 품질에 충돌이 일어나는 경우 방법 1 : 두 loss를 하나의 loss로 결합한 loss를 최소화하기 위해 단일모델을 학습 $\\alpha \\text{ qualty loss} + \\beta \\text{ engagement loss} = \\text{loss}$에서 $\\alpha$, $\\beta$를 조정해야하지만 매 조정때마다 재학습을 해야하는 문제가 있음 방법 2 : 2개 모델로 각 loss를 학습 각 loss에 대해 $\\alpha$, $\\beta$를 따로 조정이 가능하고 이미 각 loss가 최소이므로 모델을 재학습하지 않아도 됨 베이스라인을 설정해야 비교가 가능 간단한 모델부터 시작해야 모델의 위험을 낮추는 방향으로 갈 수 있음 가장 좋은 방법은 최악 성능인 랜덤 픽을 진행하는 허수아비 모델로 시작하는 것 유사한 문제를 해결하고 있는 SOTA 논문을 파가하는 것이 좋음 Voila, Streamlit, Gradio 같은 방식으로 프로토타입 웹페이지를 구현하는 것이 좋음 Metric 설점 모델의 성능지표는 크게 보면 비즈니스의 지표일 수도 있음 지표를 잘 설정해야 기존보다 더 성과가 있는지를 확인할 수 있음 이를 위해 A/B Test를 진행 4. 배포 &amp;amp; 모니터링 정의된 지표를 배포후 지속적인 모니터링을 하는 것이 중요 현재 모델의 결과 파악 잘못 예측하고 있다면 문제점 파악 예측 기반 파악 Feature 사용시 어떤 부분을 특히 잘못 예측하는가?비즈니스 모델 파악 회사의 비즈니스 이해도가 높을수록 문제 정의를 잘 할 가능성이 높음 비즈니스 모델에서 어떤 데이터가 존재하는지 알아야 기반 비즈니스 모델을 파악하기 좋음 회사가 어던 서비스, 가치를 제공하고 있는가? 다양한 서비스 구조를 파악하고 가치 창출을 관점으로 파악하는 것이 중요" }, { "title": "[BoostCamp AI Tech / Level 1 - DL Basic] Day16 - Transformer", "url": "/posts/day16_transformer/", "categories": "NAVER BoostCamp AI Tech, Level 1 - DL Basic", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech, DL Basic, DL, ML, Transformer", "date": "2022-02-11 17:00:00 +0900", "snippet": "DL Basic : TransformerTransformer 2017년 NIPS에 올라온 구글 개발진의 논문 “Attention Is All You Need” 에 등장 sequence-to-sequence 처리 모델 (번역 case model) 입출력 sequence의 길이가 다름 입출력 sequence의 도메인이 다름 기존 RNN과 다르게 재귀구조를 사용하지 않고 attention을 사용 입력이 시간 순에 따른 순차적인 형태가 아닌 한번에 데이터가 들어옴 인코더, 디코더 구조로 구성 기본 구조는 Self-Attentio $\\rightarrow$ Feed Forward NN Transformer 원리 기본적인 원리는 n개의 $x$에서 n개의 $z$를 찾을 때 (x는 번역 전, z는 번역 후) $x_i \\rightarrow z_i$에서 n-1개의 $x$를 같이 고려함 좀 더 직관적인 이해는 vector space에 뿌려져있는 단어들에 특정 단어 정보가 들어오면 vector space의 단어들 중 어떤 단어와 더 가까운지를 확인 이 과정에서 일종의 내분점아이디어가 도입 이후 특정 점 K에 대해 mapping된 V를 통해 다른 vector space로 mapping이 가능문장 임베딩 입력으로 들어온 문장을 단어단위로 분리하고 임베딩Q, K, V vector 각 임베딩된 단어에 대해 Q, K, V 벡터를 생성 각 X에 대해 Q, K, V를 찾아주는 MLP가 존재 Q 벡터는 단어의 query 정보, K 벡터는 단어의 vector space 상의 위치, V는 각 단어의 value vectorScaled-dot-product attention 처리하는 단어의 Query vector와 모든 Key vector의 내적을 진행하고 Key vector의 dimension인 $d_k$의 제곱근으로 나눠주며 scaling을 진행 이렇게 나온 값을 attention score라고 함 이후 결과값에 softmax 함수를 처리 이후 각 Key vector에 해당하는 Value vector를 곱해주고 합을 진행 이 과정은 softmax의 특성상 내분점 연산과 비슷한 효과가 나타남 최종 결과가 Attention Value가 됨행렬 연산으로 일괄처리\\[\\begin{aligned} \\text{softmax}\\left( \\frac{\\mathbf{Q} \\times \\mathbf{K}^\\intercal}{\\sqrt{d_k}} \\right) \\mathbf{V} = \\mathbf{Z}\\end{aligned}\\qquad \\mathbf{Z} : \\text{Attention}\\] 각 Q에 대해 scaled-dot 연산을 진행하였지만 실제론 행렬 연산으로 한번에 처리 가능Multi-head attention (MHA) MHA는 head를 여러개를 만들어서 병렬적 연산을 수행하는 것 이는 attention 연산 특성상 행렬의 크기가 너무 커지기 때문 동시에 attention이 가능하기 때문에 다른 단어들과의 연관성을 한번에 볼 수 있음" }, { "title": "[BoostCamp AI Tech / Level 1 - DL Basic] Day16 - Recurrent Neural Networks", "url": "/posts/day16_rnn/", "categories": "NAVER BoostCamp AI Tech, Level 1 - DL Basic", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech, DL Basic, DL, ML, RNN", "date": "2022-02-11 15:00:00 +0900", "snippet": "DL Basic : Recurrent Neural NetworksSequential Model sequential model의 특징은 받아들여야 할 data의 dimension을 모름Naive sequence model\\[p(x_t | x_{t-1}, x_{t-2}, \\cdots)\\] 과거에 있는 모든 정보를 고려함 고려해야할 정보량이 점차 증가함Autogressive model\\[p(x_{t} | x_{t-1}, \\cdots, x_{t-\\tau})\\] naive 모델의 조건부 데이터의 양이 너무 많아서 과거의 데이터 일부만 고려 AR모델은 과거의 데이터 몇개를 참고하냐에 따라 이름이 바뀜Markov model (firts-order autoregressive model)\\[p(x_{1}, \\cdots, x_{T}) = p(x_{T}|x_{T-1})p(x_{T-1}|x_{T-2})\\cdots p(x_2|x_1)p(x_1) = \\prod_{t=1}^{T}p(x_{t}|x_{t-1})\\] Morkov assumption을 활용한 모델 AR모델에서 1개의 이전 데이터까지만 참고함 데이터의 가정으로는 말이 되긴 어려움 오늘 일어나는 일은 어제의 일에만 영향을 받는다. Latent autoregressive model\\[\\begin{aligned} &amp;amp; \\hat{x} = p(x_{t}|h_{t}) \\\\ &amp;amp; h_t{} = g(h_{t-1}, x_{t-1})\\end{aligned}\\] 과거 정보들을 한번에 사용하는 것은 어려움 과거 정보들을 요약해주는 $h_n$을 다음 $h_{n+1}$로 전달Recurrent Neural Network (RNN) 기본적인 구조는 MLP와 동일함 hidden layer인 $h_{t}$는 이전 cell state에 의존함 대체로 왼쪽 그림처럼 표현하지만 시간순으로 펼치면 우측처럼 나타남RNN의 문제점 RNN은 연속적인 Backpropagation 때문에 과거로 갈수록 데이터의 영향력이 줄어드는 장기 의존성 (short-term dependencies)문제를 갖는다.\\[\\begin{aligned} &amp;amp; h_{1} = \\phi(W^\\intercal h_0 + U^\\intercal x_1) \\\\ &amp;amp; h_{2} = \\phi(W^\\intercal \\phi(W^\\intercal h_0 + U^\\intercal x_1) + U^\\intercal x_2) \\\\ &amp;amp; h_{3} = \\phi(W^\\intercal \\phi(W^\\intercal \\phi(W^\\intercal h_0 + U^\\intercal x_1) + U^\\intercal x_2) + U^\\intercal x_3)\\end{aligned}\\] 위 수식과정에서 계속해서 weight가 곱해지면 vanishing gradient나 exploding gradient가 발생Long Shot Term Memory (LSTM) LSTM은 Vanila RNN의 단점인 장기 의존성 문제를 해결 왼쪽부터 각 $\\sigma$들은 Forget gate, Input gate, Output gate를 의미 이전 cell state ($C_{t-1}$)와 이전의 hiddne state($h_{t-1}$)를 그대로 받아옴 기본 아이디어는 $C_{t-1}$에 각 gate를 통과한 정보들이 $C_{t-1}$ 정보를 업데이트Forget Gate\\[0 \\leq f_{t} = \\sigma(W_{f}\\cdot [h_{t-1}, x_{t}] + b_{f}) \\leq 1\\] 이전 cell state에서 버릴 것들을 선택Input Gate\\[\\begin{aligned} &amp;amp; i_{t} = \\sigma(W_i\\cdot [h_{t-1}, x_t] + b_i) \\\\ &amp;amp; \\tilde{C}_{t} = \\tanh(W_{C}\\cdot[h_{t-1}, x_{t} + b_{C}])\\end{aligned}\\] 앞으로 새로 들어오는 정보 중 어떤 것을 새롭게 올릴지를 결정($i_t$) $\\tanh$이 새로운 후보값들인 $\\tilde{C}_{t}$ vector를 생성하고 cell state에 더할 준비를 함Update Cell\\[\\begin{aligned} &amp;amp; i_{t} = \\sigma(W_{i} \\cdot [h_{t-1}, x_{t}] + b_{i}) \\\\ &amp;amp; C_{t} = f_t * c_{t-1} + i_{t} * \\tilde{C}_{t}\\end{aligned}\\] 이전 state에 $f_{t}$를 곱해서 잊어버리기로 한 데이터르 잊음 $i_t$를 통해 어떤 데이터가 들어갈지 선택하고 $\\tilde{C}_t$를 곱한 input gate 결과를 합해서 새로운 cell state를 형성Output Gate\\[\\begin{aligned} &amp;amp; o_{t} = \\sigma(W_{o} [h_{t-1}, x_t] + b_{o}) \\\\ &amp;amp; h_t = o_t * \\tanh(C_t)\\end{aligned}\\] 어떤 값을 내보낼지 결정함 시그모이드를 통해 원하는 output을 결정함Gated Recurrent Unit (GRU) LSTM의 변형 모델 peephole connection을 추가하였고 일부 게이트를 변형함 reset gete, update gate가 존재 cell state는 없고 hidden state만 존재 LSTM보다 파라미터 수가 적어서 더 성능이 좋음" }, { "title": "[BoostCamp AI Tech] Day15", "url": "/posts/day15/", "categories": "NAVER BoostCamp AI Tech, Review", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech", "date": "2022-02-09 23:00:00 +0900", "snippet": "Day15 Review당신은 오늘 하루 어떻게 살았나요? AlexNet 리뷰 포스팅 완성 Kaggle Study오늘 하지 못한 것들 Coursera MLOps 1주차 Day15 강의 정리내일은 어떤 것을 할까? Coursera MLOps 1주차 Kaggle Study Titanic 분석 스토리보드 생성 Movie Lens RS check마무리 Kaggle Study, Kakao Arena Study 에 시간 분배 잘할 것 시간을 쪼개서 써보자" }, { "title": "[Paper Review] AlexNet (2012)", "url": "/posts/alexnet/", "categories": "Paper Review, Computer Vision", "tags": "NAVER, BoostCamp, AI Tech, CNN, AlexNet, paper review", "date": "2022-02-09 00:00:00 +0900", "snippet": "AlexNet (2012)GitHub : AlexNet Implementation논문 소개출처 : NIPS2012 ImageNet Classification with Deep Convolutional Neural Networks (a.k.a AlexNet)[NIPS2012] ImageNet Classification with Deep Convolutional Neural NetworksAlexNet은 본격적으로 딥러닝이 실전적인 효과를 나타낸 대표적인 신경망 모델입니다. 논문에 따르면 AlexNet은 224 X 224의 이미지를 분류하는 대회인 ImageNet LSVRC(ILSVRC) 에 2010년과 2012년 총 2번 참가를 하였고 2012년 대회에서 우승을 차지하며 딥러닝의 시대를 열었습니다.기존에도 CNN이 존재하였지만 이를 직접적으로 활용하기에 어려움이 있었습니다. 하지만 GPU를 활용한 CNN구조의 구현과 Dropout을 적용시킨 AlexNet이 등장하며 이미지 분류에서 CNN의 입지가 굉장히 뛰어올랐습니다. 이번 글에서는 AlexNet의 논문을 리뷰하며 자세한 내용을 공부해보도록 하겠습니다.개인 공부의 목적도 포함하고 있어서 각 파트별로 나눠서 작성하였으므로 필요한 부분에 맞춰 우측의 목차를 클릭하셔서 이동해주시길 바랍니다.우측 목차가 제대로 리스트업이 되지 않아 아래에 목차를 따로 두었습니다.이 글에는 오타 및 오역이 존재할 수 있습니다. 참고하여 읽어주시고 번역이 부자연스러운 경우 원문 부분을 함께 작성하였습니다. Abstract 1. Introduction 2. The Dataset 3. The Architecture 4. Reducing Overfitting 5. Details of learning 6. ResultsAbstract AlexNet은 120만개의 고해상도(224 X 224)이미지를 분류하는 ILSVRC 2012에서 error rate 16.4%를 기록하며 우승 총 6천만개의 parameter와 65만개의 뉴런으로 구성된 모델 5개의 convolution layer + 3 fully-connected layer로 구성 5개의 convolution layer 중 일부는 max-pooling layer가 연결되어있음 3개의 fully-connected layer는 최종 1000-way softmax로 구성되어있음 훈련 속도의 향상을 위해 비포화 뉴런과 GPU구현의 convolution 연산을 사용 과적합 방지를 위해 Dropout을 활용1. IntroductionChapter 요약 기존의 이미지 분류 문제들의 데이터 셋은 수만개 정도로 작은 크기의 데이터 셋이었고 학습을 통해 인식을 진행하는 데에 큰 무리가 없었음 기존에도 이미지를 변형하는 방식은 image augmentation이 존재해서 지정된 label을 지키는 선에서 데이터를 증폭하는 방식으로 학습을 했기 때문 현실의 데이터는 상당히 가변적인 환경(considerable variability)을 갖고 있기 때문에 이를 적용하기 위해서는 큰 규모의 데이터 셋이 필요 작은 데이터 셋의 문제점은 이미 알려졌지만 어쩔 수 없는 한계로 최근에서야 큰 규모의 데이터 셋을 만들 수 있었음 대표적인 예시가 ILSVRC의 ImageNet 데이터 셋 큰 규모의 데이터를 학습하기 위한 모델은 규모가 커져야 하는데, object recognition 작업은 매우 큰 복잡도를 갖는 작업이고 따라서 ImageNet에 적용하기 어렵다는 문제가 발생함 이를 해결하고자 모델은 갖고 있지 않은 데이터에 대해 많은 사전지식이 있어야 함(원문: so our model should also have lots of prior knowledge to compensate for all the data we don’t have.) CNN의 가정에 따르면 이런 복잡도와 데이터의 사전지식 문제를 해결하기에 좋음 CNN은 넓이와 폭에 의해 capacity가 결정되고 이미지의 특성에 대해 올바른 추정을 함 CNN은 parameter와 connection의 수가 fully-connected layer에 비해 훨씬 적기 때문에 빠른 훈력속도와 성능의 보존이 가능 하지만 기존의 CNN은 적용에 있어서 대규모의 비용이 든다는 단점이 존재 2D convolution과 결합된 GPU로 인해 학습에 적용할 수 있게 되었음 이 논문에서는 2D convolution에 최적화된 GPU 연산구현, 성능 향상과 동시에 학습 시간을 줄이기 위한 새로운 feature와 자주 사용하지 않는 feature제시, 과적합 방지를 위한 Dropout의 적용에 대해 설명 최종 신경망의 구조는 5개의 convolution layer와 3개의 fully-connected layer이고 이 깊이는 성능자체제 중요한 영향을 미침 모델 학습 환경은 2개의 GTX 580 3GB GPU로 5~6일 정도 훈련을 진행했음Chapter 분석 CNN 가정에 나오는 이미지의 특성(nature of images)은 총 2가지가 언급됩니다.내용 참고 : CNN과 이미지가 찰떡궁합인 이유 Stationary of statistics stationary란 확률론에서 시계열의 통계적인 속성이 시간의 영향을 받지 않는 것을 의미합니다. 대체적으로 일정한 패턴을 갖는 경우를 말합니다. 이미지는 특정 패턴을 갖고 있습니다. convolution 연산의 핵심 아이디어는 증폭 또는 축소를 통한 특징 및 패턴 추출입니다. 이런 관점에서 바라보면 stationary는 convolution 연산과 매우 좋은 시너지를 발휘합니다. Locality of pixel dependencies locality of pixel dependencies는 이미지는 작은 특징들의 구성이기 때문에 픽셀들은 주변의 일부 픽셀에 한해서만 영향을 받는다는 것입니다. 간단히 말하면 이미지는 여러개의 픽셀로 구성되는 데 특정 지역(locality)에 있는 픽셀은 이미지 전체에 영향을 받는 것이 아닌 해당 부분의 근처 픽셀들의 영향을 받는 것입니다. 즉 코 근처의 픽셀은 눈이나 입 부근의 픽셀의 영향을 받는 것이 아닌 코를 구성하는 콧볼, 콧대같은 픽셀의 영향을 받는다는 것입니다. 이러한 점은 stride에 따라 모든 이미지를 convolution하는게 아닌 부분적인 이미지에 convolution을 적용하는 CNN과 잘 맞는다는 것을 알 수 있습니다. 궁금증 Introduction 파트를 읽으면서 큰 규모의 데이터 학습에 있어서 매우 큰 복잡도를 갖는 모델이 필요하여 ImageNet에 적용하기 어렵다는 부분이 있었습니다.(원문 : However, the immense complexity of the object recognition task means that this prob- lem cannot be specified even by a dataset as large as ImageNet, so our model should also have lots of prior knowledge to compensate for all the data we don’t have.)CNN의 가장 큰 장점은 이미지 해상도가 높아져도 kernel의 크기는 고정적으로 유지되므로 weight parameter의 크기에 대한 부담이 적다라는 것이라서 이 부분을 “기존의 Fully-connected만으로는 224 X 224의 고해상도 이미지는 각 픽셀 개수에 비례해서 layer의 weight가 필요해서 무리가 있다는 것” 으로 이해했는데 이게 맞는 이해인지가 확실치 않습니다.2. The DatasetChapter 요약 ImageNet 데이터 셋은 약 22000개의 카테고리에 속하는 총 1500만개의 라벨링 고해상도 이미지로 구성되어 있음 ILSVRC에는 1000개의 카테고리에 각각 약 1000개의 이미지가 있는 ImageNet의 subset을 사용함 120만개의 train set, 5만개의 valid set, 15만개의 test set으로 구성 측정 기준은 top-1 error, top-5 error를 사용하였음 ImageNet은 가변 해상도로 구성되어 있어 모델에 적용할 때는 256 X 256의 고정 해상도로 다운샘플링해야 했음 직사각형 이미지가 주어질 경우 짧은 변을 256이 되게 크기를 조정하고 중앙 256 X 256 패치를 사용했음 데이터 셋의 정규화 과정을 제외하고는 추가적인 데이터 전처리는 하지 않았음(원문 : We did not pre-process the images in any other way, except for subtracting the mean activity over the training set from each pixel.) 신경망은 raw RGB 값의 이미지로 훈련을 진행함Chapter 분석 top-1 error, top-5 error내용 참고 : 이미지 분류 모델 평가에 사용되는 top-5 error와 top-1 error top-n error는 일반적으로 이미지 분류(image classification)에서 많이 사용되는 평가 지표입니다. 일반적으로 top-1 error와 top-5 error를 많이 씁니다. top-1 error는 모델의 카테고리 분류 결과 중 상위 확률 1개의 카테고리가 정답과 같다면 top-1 error는 0%입니다. 만약 분류 카테고리가 5개인 상황에서 모델이 [0.1, 0.2, 0.25, 0.3, 0.15]라는 결과를 내면 정답 class는 3이라고 처리합니다. 이때 실제 정답이 3이 맞다면 top-1 error는 0%입니다. 그렇다면 이를 활용하여 top-5 error를 이해해봅시다. top-1은 상위 1개의 class가 정답 클래스와 같은지를 확인합니다. top-5 error는 모델의 예측에서 상위 5개의 클래스를 확인합니다. 상위 5개의 class에 정답 class가 있는지 없는지를 확인 합니다. 픽셀 정규화Why substracting dataset’s image mean? 각 이미지의 pixel에 training set의 평균을 제거해서 normalize를 진행합니다. 이 전처리는 정형 데이터(tabular data)에서 feature별 정규화를 진행하는 것과 같은 역할을 합니다. 3. The ArchitectureAlexNet 구조 | 출처 : Deep Learning’s Most Important Ideas - A Brief Historical Review5개의 convolution layer와 3개의 fully-connected layer로 구성된 총 8개의 학습 layer로 구성 중요도에 따라 3.1 ~ 3.4 순서로 설명3.1 ReLU Nonlinearity 기존에 자주 쓰이던 비선형 함수인 $\\tanh$, sigmoid는 포화 비선형성 함수(saturating nonlinearities)였음 기존 비선형 함수는 함수 특성상 saturation 지점이 발생하여 vanishing gradient 문제가 발생 이를 해결하고자 부분적 non-saturating nonlinearity인 $ReLU = \\max(0, x)$ 를 사용 결과적으로 $\\tanh$보다 몇 배 더 빠른 속도로 loss가 수렴CIFAR-10 데이터셋의 25% training error 도달에 필요한 epochs 수 전통적인 activation function을 사용했다면 큰 규모의 Neural Net 구성이 불가능했을 것 단, 최초로 전통적 CNN의 대안을 제안한 것은 아님 이전에 Improving neural networks by preventing co-adaptation of feature detectors에서 $|\\tanh(x)|$을 사용한 적이 있음 하지만 이 논문의 관찰 포인트는 overfitting을 방지하는 것이므로 속도를 높이는 것과는 약간 다른 방향성을 가짐 빠른 학습 속도는 큰 데이터로 훈련된 큰 모델 성능에 큰 영향을 줌3.2 Training on Multiple GPUS 모델 학습에 GTX 580 GPU를 사용하였지만 1개를 사용하면 네트워크 크기가 제한됨 이를 해결하고자 2개의 GPU로 학습을 하였는데, 이를 GPU parallelization이라 함 GPU 병렬화를 사용한 방법에는 크게 2가지 기법이 사용됨 전체 커널을 절반씩 나눠 각각의 GPU에 할당함 특정 레이어에서 나눠진 GPU를 통합시켜 학습을 진행 논문에서는 3번째 Conv layer에서만 GPU를 통합 결과적으로 계산량의 허용가능한 부분까지 통신량을 정확하게 조정가능 위의 48개 커널은 GPU1, 아래 48개 커널은 GPU2의 학습 kernel map 각 GPU는 독립적으로 학습하는 과정에서 이런 현상이 매번 발생함 논문에서는 Specialization이라고 함 GPU를 분산해서 처리한 결과 top-1과 top-5 error를 1.7%, 1.2% 감소하는 효과를 보임 추가적으로 2개 GPU는 1개 GPU보다 약간 빠른 학습속도를 보임3.3 Local Response Normalization (LRN) LRN은 generalization을 위해 진행 최근에는 LRN보다는 Batch Normalization으로 대체됨 LRN을 진행하는 이유는 ReLU의 양수를 살리는 성질때문 ReLU는 양수값을 그대로 받아 neuron에 전달하므로 image에 존재하는 매우 큰값이 존재하면 주변부 낮은 값의 정보를 neuron에 전달을 방해하는 later inhibition이 발생 LRN은 later inhibition을 방지하는 형태로 구현\\[\\begin{aligned} b_{x,y}^i = a_{x, y}^i/{\\left( k+\\alpha \\sum_{j=\\max(0,i-n/2)}^{\\min(N-1, i+n/2)} (a_{x, y}^j)^2 \\right)}^\\beta\\end{aligned}\\] 변수 설명 $b_{x,y}^{i}$ : x, y 위치에 적용된 $i$번째 kernel의 정규화된 output $a_{x,y}^{i}$ : x, y 위치에 적용된 $i$번째 kernel의 output $N$ : 총 kernel의 수 $n$ : 정규화 이웃의 수 (hyperparameter) $\\alpha$, $\\beta$, $k$ : hyperparameters hyperparameter는 $k=2, n=5, \\alpha=10^{-4}, \\beta-0.75$로 실험 특정 레이어에서 ReLU를 통과한 후 normalization 진행 정규화를 진행한 결과 top-1과 top-5 error가 1.4%, 1.2% 감소하는 효과를 보여줌3.4 Overlapping Pooling 기존의 겹치지 않고 독립적으로 pooling을 하는 방식이 아닌 중첩하는 방식의 Pooling을 사용 이 과정은 과적합을 방지하는 방법으로 사용함 pooling layer의 세팅은 kernel size=3, stride=2을 사용함 적용 결과 top-1, top-5 error는 0.4%, 0.3% 감소3.5 Overall ArchitectureAlexNet 구조 | 출처 : Deep Learning’s Most Important Ideas - A Brief Historical Review 전체 8개의 레이어 (5 Conv + 3 FC) output FC에는 1000-way softmax 활용 output class는 1000개 Convolution Layer Setting 2, 4, 5번째 레이어는 같은 GPU상의 이전 레이어에 있는 kernel map과 연결 3번째 레이어에서는 2번째 레이어의 모든 kernel map을 연결 LRN layer는 1, 2번째 레이어에 적용 Max-pooling 모든 LRN layer 이후에 적용하고 5번째 레이어에도 적용 ReLU는 모든 레이어의 결과에 적용 그 외 다른 부분에 있는 내용도 추가해보면…. pooling layer의 세팅은 kernel size=3, stride=2을 사용함 LRN layer의 hyperparameter는 $k=2, n=5, \\alpha=10^{-4}, \\beta-0.75$로 실험 init weight, bias참고해서 bias, weight 초기화 구현시 첫번째 CNN을 제외하고 padding, stride가 언급되지 않아서 직접 계산해야하는 것 같음\\(O = \\frac{I - K + 2P}{S} + 1\\) $O$ : size of output image $I$ : size of input image $K$ : size of kernel in Conv layer $N$ : Number of kernels $S$ : Stride $P$ : padding Chapter 분석 전반적인 AlexNet의 구조와 핵심 포인트들을 설명하고 있습니다. 현재 CV 분야에서는 이미 많이 사용된 내용들이라 자세하게 다룰 내용이 크게 없었습니다. 실제 구현시 최종 구조에 적힌 레이어별 특성을 참고하면 좋을 것 같습니다.4. Reducing Overfitting AlexNet은 6천만개의 parameter를 가짐 많은 수의 parameter 때문에 1000개의 class를 분류하는 과정에서 overfitting이 필연적으로 발생함 overfitting을 줄이고자 2가지 방식을 사용4.1 Data Augmentation ovefitting을 가장 쉽게 줄이는 방법은 label을 보존하는 변환 방식을 활용하여 많은 데이터 셋을 구성하는 것 이미지를 생성하고 수평반전(horizontal reflections) 을 수행 256 X 256 이미지를 224 X 224로 잘라내는데, 중앙, 좌상단, 좌하단, 우상단, 우하단 기준으로 총 5 case로 분류해서 crop 이후 각 이미지를 수평반전을 수행 이 과정이 없는 경우 network는 상당한 과적합을 보여주었음 Image RGB의 변화\\([\\mathbf{p}_{1},\\mathbf{p}_{2},\\mathbf{p}_{3}][\\alpha_{1}\\lambda_{1}, \\alpha_{2}\\lambda_{2}, \\alpha_{3}\\lambda_{3}]^{\\intercal}\\) tarining set의 RGB pixel에 PCA를 적용했음 PCA로 나온 각 RGB에 대한 eigen value와 평균이 0, 분산이 0.1인 가우시안 분포에서 추출한 random variable을 곱해 RGB값에 더해줌 이 과정은 마치 조명의 영향과 색의 intensity 변화에 대한 불변성을 지내게 함 이 기법으로 top-1 error를 1% 감소시킴 4.2 Dropout 서로 다른 모델을 결합하는 앙상블 기법은 test error를 감소시키기 효과적인 방법 AlexNet의 학습 시간 문제로 앙상블 기법을 활용하기엔 무리가 있었음 비슷한 효과를 주는 Dropout을 모델에 적용 probability = 0.5로 설정 입력마다 dropout을 적용하면 가중치는 공유되지만 신경망의 구조는 변화됨 neuron은 특정한 다른 neuron의 존재성에 영향을 받지 않음 train에서 dropout을 적용하고 test 과정에서는 모든 neuron을 활용하는 대신 neuro의 결과에 0.5를 곱해서 사용함 Dropout은 첫 2개의 FC layer에만 적용 수렴에 필요한 iteration은 2배 증가하였음5. Details of learning 전반적인 실험에 사용한 Hyper-parameters 정보 SGD(momentum=0.9, batch_size=128, weightdecay=0.0005)를 사용 각 layer의 weight는 평균이 0, 분산이 0.01인 가우시안 분포 사용하여 초기롸 2, 4, 5번째 conv layer의 bias와 FC의 bias는 모두 1로 초기화 그 외 나머지 bias는 0으로 초기화 모든 layer에 동일한 learning rate를 적용하였음 0.01로 시작해서 validation error가 상향되지 않으면 10으로 나눠주었고 학습 종료전에 3번의 감소과정이 있었음 6. Results ILSVRC-2012에서 top-5 error의 test error를 16.4%를 기록하며 우승 일부 라벨링이 굉장히 애매한 케이스를 제외하고 대부분 정답을 나타냄 또한 top-5 class의 분류도 실제 답과 매우 유사한 클래스들이 많이 나타남 Euclidean distance를 계산해서 거리가 짧을수록 유사한 사물로 인지한다는 점에서 놀라운 성과를 기록 당시에는 인간의 뉸의 정확도를 따라가기엔 아직 멀었다고 하지만 CNN 발전의 초석이 된 AlexNet은 충분히 혁신적인 모델이라고 할 수 있음Reference AlexNet 논문 리뷰 및 해석 LRN(Local Response Normalization)이란 무엇인가? [논문리뷰]AlexNet(2012) 리뷰와 파이토치 구현 [논문읽기]AlexNet(2012)-이론정리" }, { "title": "[BoostCamp AI Tech] Day14", "url": "/posts/day14/", "categories": "NAVER BoostCamp AI Tech, Review", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech", "date": "2022-02-08 23:00:00 +0900", "snippet": "Day14 Review당신은 오늘 하루 어떻게 살았나요? DL Basic CNN, Generative Model 내용 정리 AlexNet Reading오늘 하지 못한 것들 Coursera MLOps 1주차내일은 어떤 것을 할까? Coursera MLOps 1주차 AlexNet 정리 &amp;lt;- 가능할까.. ㅠ마무리 내용 정리 시간을 줄이는 것은 어느정도 성공한듯 일단 할 일이 많을 때는 시간을 낭비하지 않도록 해야할 거 같다. 큰 일정 계획 내일까지 AlexNet 정리 완료 AlexNet 내일 새벽 전에 완료하면 Intro to ML in production 1주차 듣기 안되면 목, 금, 토, 일 진행 " }, { "title": "[BoostCamp AI Tech / Level 1 - DL Basic] Day14 - Generative Models", "url": "/posts/day14_3_gen1/", "categories": "NAVER BoostCamp AI Tech, Level 1 - DL Basic", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech, DL Basic, DL, ML, Generative Model", "date": "2022-02-08 17:00:00 +0900", "snippet": "DL Basic : Generative Models주의! 열심히 노력했으나 어려워서 틀린 내용이 있을 수 있습니다!Generative Model 생성모델은 단순히 “생성”만이 목적을 갖는 것은 아님 생성에 이어 분류/판별도 목적을 두고 있음 Generation : 확률분포 $p(x)$로부터 추출한 $x_{new}$가 특정 클래스를 띔 Density estimation : $p(x)$가 특정 클래스가 높게 나온다면 다른 것들은 낮게 나오게된다 (anomayly detection) Unsupervised representation learning 그렇다면 여기서 $p(x)$는 어떻게 결정할까? 2진 데이터는 Bernoulli dist’n $\\rightarrow$ 1개의 parameter 그외 여러개 데이터는 카테고리 분포를 사용함 $\\rightarrow$ n-1개의 parameter Auto-regressive Model AR 모델에 사용되는 중요한 3가지 규치 Chain Rule$p(x_1, …, x_n) = p(x_1)p(x_2|x_1)p(x_3|x_1,x_2)\\cdots p(x_n|x_1,\\cdots ,x_{n-1})$ Bayes’ Rule Conditional independence$\\text{If } x \\perp y | z, \\text{then } p(x|y,z) = p(x|z)$ 28 X 28 binary 이미지를 생성한다고 하자 $p(x) = p(x_1, …, x_{784})$를 예측해야함 chain rule에 따르면 $p(x_{1:784}) = p(x_1)p(x_2|x_1)p(x_3|x_{1:2})\\cdots$ 이 처럼 이전 정보에 dependency 한 경우를 AR model이라고 함 binary pixel의 순서를 정하는 방법과 몇번재 이전 정보까지 depend할 지에 따라 성능이 달라짐 Variational Auto-encoder Auto Encoder는 Encoder에 들어온 입력에서 특징을 추출하여 입력의 압축정보를 latent variable에 담고 latent variable을 통해 다시 입력값을 복원함 수학적으로 보면 PCA와 유사한 의미를 지니고 있음 그렇다면 Latent Space로부터 데이터를 생성할 수는 없을까? 라는 관점에서 VAE가 주목을 받음문제인식 문제는 기존에 갖고 있는 데이터들은 너무 고차원적이라 실제 데이터의 분포를 찾는 것은 불가능 그렇다면 샘플들의 분포를 사전에 알고 있는 어떤 분포로 표현할 수 있을까? 대표적으로 Latent Space가 가우시안 분포를 따른다면 평균과 표준편차만으로도 분포를 표현이 가능Posteriror distribution인 $p_{\\theta}(z|x)$은 알기가 어려움$\\rightarrow$ Variational distribution $q_\\phi(z|x)$로 근사 따라서 실제 posterior와 variational distribution의 분포 차를 나타내는 KL-Divergence를 최소화하는 것이 목표\\[\\begin{aligned} D_{KL}(p||q) &amp;amp; = \\int p(x)\\log\\frac{p(x)}{q(x)}dx \\\\ &amp;amp; = \\int p(x)\\log p(x)dx - \\int p(x) \\log q(x) dx\\end{aligned}\\] KL-Divergence는 두 분포가 동일하면 0이 되고 계산순서가 바뀌면 값이 바뀔 수 있음 Latent Space의 분포가 가우시안 분포를 따른다는 가정을 한 이유는 KL-Divergence가 가우시안 case에서 간략하게 표현이 가능하고 미분가능성을 갖기 때문해결 접근\\[\\begin{aligned} \\ln{p_\\theta (D)} &amp;amp; = \\mathbb{E}_{q_\\phi (z|x)}[\\ln{p_\\theta (x)}] \\\\ &amp;amp; = \\mathbb{E}_{q_{\\phi} (z|x)}\\left[\\ln\\frac{\\theta (x, z)}{p_\\theta (z|x)} \\right] \\\\ &amp;amp; = \\mathbb{E}_{q_\\phi (z|x)}\\left[\\ln\\frac{p_\\theta (x, z)q_\\phi (z|x)}{q_\\phi (z|x)p_\\theta (z|x)} \\right] \\\\ &amp;amp; = \\mathbb{E}_{q_\\phi(z|x)}\\left[\\ln\\frac{p_{\\theta} (x, z)}{p_\\phi (z|x)} \\right] + \\mathbb{E}_{q_\\phi (z|x)}\\left[\\ln\\frac{q_\\phi (x, z)}{p_\\theta (z|x)} \\right]\\\\ &amp;amp; = \\underbrace{\\mathbb{E}_{q_\\phi (z|x)}\\left[\\ln\\frac{p_\\theta (x, z)}{q_\\phi (z|x)} \\right]}_{\\text{ELBO}} + \\underbrace{D_{KL}(q_\\phi (z|x)||p_\\theta (z|x))}_{\\text{KL-Divergence}}\\end{aligned}\\] $D_{KL}(q_\\phi(z|x)||p_\\theta(z|x))$ 을 낮추는 것이 목적이지만 $p_\\theta(z|x)$는 알아내는 것이 불가능하기 때문에 직접적인 objective를 줄이는 것은 불가능 전체 값은 상수로 일정하므로 ELBO를 키우면 간접적으로 KL-Divergence를 낮추는 것과 같은 효과를 줌\\[\\begin{aligned} \\mathbb{E}_{q_\\phi(z|x)}\\left[\\ln\\frac{p_\\theta(x, z)}{q_\\phi(z|x)} \\right] &amp;amp; = \\int\\ln\\frac{p_\\theta(x|z)p(z)}{q_\\phi(z|x)}q_\\phi(z|x)dz \\\\\\\\ &amp;amp; = \\mathbb{E}_{q_\\phi(z|x)}[p_\\theta(x|z)] - D_{KL}(q_\\phi(z|x)||p(z))\\end{aligned}\\] ELBO를 연산한 결과는 총 2개의 항이 존재하는데 앞부분을 Reconstruction Term이라고 함 encoder를 통해 데이터를 latent space에 보내고 그로부터 동일한 샘플이 나올 확률이 Likelihood 뒤쪽 KL-Divergence 부분은 prior fitting term이라고 함 우리가 예측한 latent 분포와 prior 분포가 얼마나 차이가 있는지를 나타내는 값 Adversarial Auto-encoder VAE에서 prior fitting term을 GAN objective로 바꾼 것 latent distribution을 smapling 가능한 distribution만 있으면 충분함Genrative Adversarial Network (GAN) GAN은 기본적으로 Two-ply minimax Game을 근간에 둔 모델 2개의 모델이 사용되며 Generator와 Descriminator가 서로 경쟁하며 학습을 진행 결과적으로 성능이 좋은 descriminator를 얻어냄\\[\\begin{aligned}\\begin{matrix} \\text{Discriminator : } \\quad \\underset{D}{\\max}V(G,D) = E_{\\mathbf{x} \\sim p_{\\text{data}}}[\\log D(\\mathbf{x})] + E_{\\mathbf{x} \\sim p_{G}}[log(1 - D(\\mathbf{x}))] \\\\ D^{*}_{G}(\\mathbf{x}) = \\frac{p_{data}(\\mathbf{x})}{p_{data}(\\mathbf{x})} + p_G(\\mathbf{x})\\end{matrix}\\end{aligned}\\] 기본적인 컨셉이 Two-ply minimax Game이라 generator와 descriminator는 각자의 선택 중 항상 최선을 선택함 (minimax game이란?) generator가 fix된 상황에서 항상 최적으로 결과를 분류하는 discriminator를 $D^{*}_{G}$라고 함\\[\\begin{aligned} \\text{Generator : } \\quad \\underset{G}{\\min}V(G,D) = E_{\\mathbf{x} \\sim p_{\\text{data}}}[\\log D(\\mathbf{x})] + E_{\\mathbf{x} \\sim p_{G}}[log(1 - D(\\mathbf{x}))]\\end{aligned}\\] 이렇게 나온 학습을 통한 $D$를 기반으로 generator가 학습을 함" }, { "title": "[BoostCamp AI Tech / Level 1 - DL Basic] Day14 - CV Applications", "url": "/posts/day14_2_cvapps/", "categories": "NAVER BoostCamp AI Tech, Level 1 - DL Basic", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech, DL Basic, DL, ML, CV", "date": "2022-02-08 15:00:00 +0900", "snippet": "DL Basic : CV ApplicationsSemantic Segmentation Semantic Segmentation이란 이미지에서 다양한 item을 구분하는 것 자율주행에서 많이 사용 Convolution의 마지막 연산에서 Fully connected를 쓰면 classifiaction의 역할을 함 마지막 연산에서 Convolution을 하면 특정 item의 heatmap을 표현 특정 item을 재현하려면 deconvolution연산을 해야하지만 convolution의 역연산은 실제로 불가능함 (두 수의 합이 10일 때 두 수의 경우의 수는 n/2이므로 알 수 없음) 대신 padding을 크게 적용해서 convolution 연산을 진행DetectionR-CNN R-CNN은 이미지에서 detection을 하고자하는 영역을 정하고 CNN을 통해 영역을 모든 class에 대해 비교하며 분류함 결과적으로는 Brute Forcing idea가 적용되는 것SPPNet R-CNN은 브루트 포스 방식을 쓰기 때문에 상당이 오랜시간이 걸리다는 것이 문제 이를 해결하고자 CNN 탐색을 1번만 하는 방식으로 시간을 줄임 이미지 기반 bounding box를 추출 전체에서 convolution feature map을 형성 bounding box 위치의 tensor만 가져오는 방식 결과적으로 CNN을 적용하는 point는 (2)번 과정 1번만 진행Fast R-CNN 핵심 아이디어는 SPPNet과 비슷하게 CNN을 1번만 진행하는 것 ROI pooling을 활용해서 각 영역에 적용함 결과로는 클래스와 bounding-box regressor 2가지를 반환 여기서 물체가 있을만한 영역을 찾는 network를 학습하는 Region Proposal Network를 추가하면 Faster R-CNN이 됨 Region Proposal Network는 이미지에서 특정 영역이 bounding box로써 의미가 있을지 없을지를 찾음 YOLO You Only Look Once의 약자로 이미지에서 한번에 bounding box를 여러개 찾는 알고리즘 YOLO는 이미지를 (0) S X S의 그리드로 분할하고 (1) 각 셀은 B개의 bounding box를 예측, 동시에 (2) 각 셀은 C 개의 클래스의 확률을 예측하고 최종적으로 (3) SxSx(Bx5+C) 크기의 텐서로 구성" }, { "title": "[BoostCamp AI Tech / Level 1 - DL Basic] Day14 - Convolutional Neural Networks", "url": "/posts/day14_1_cnn/", "categories": "NAVER BoostCamp AI Tech, Level 1 - DL Basic", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech, DL Basic, DL, ML, CNN", "date": "2022-02-08 13:00:00 +0900", "snippet": "DL Basic : Convolutional Neural NetworksConvolution convolution 연산은 기본적으로 Input에 대해 커널단위로 연산을 하는 것을 의미 특징 추출을 목적으로 할 때 많이 사용함 필터(kernel)의 종류에 따라 다른 결과가 나타남 ex) convolution filter가 3X3이고 value가 모두 1/9이면 이미지의 평균이 convolution의 output이 되어 Blur효과와 같은 결과가 나타남$\\rightarrow$ 전반적으로 기존 이미지보다 수치가 평균적으로 낮아진 것 자세한 내용은 수학적 의미의 CNN 참고RGB Convolution 기본적인 convolution 연산의 원리에 따라 사용된 필터의 개수는 최종 결과의 feature scale을 결정 위의 사진처럼 4개의 filter를 사용하면 4개의 특징이 추출된 결과가 나타남Stack of Convolutions CNN의 핵심은 파라미터 수를 줄이는 것에 있기 때문에 convoluton 연산에 사용된 전체 파라미터를 계산하는 것이 중요\\[\\text{\\# of Paramter} = \\text{Kernel size} \\times \\text{\\# of Input Channel} \\times \\text{\\# of Output Channel}\\]Convolution Neural Networks (CNN) CNN은 크게 2가지 구조로 구성됨 Convolution ans pooling layers : feature 추출 Fully connected layer : decision making 최근에는 Fully connected를 없애는 추세 Fully Connected는 dimesion에 맞춰 weight를 가지므로 parameter 수가 급격하게 증가하는 영향을 줌 parameter 수가 증가하면 학습 난이도가 높아지고 generalization performance가 낮아짐 Stride stride란 kernel이 움직이는 반경을 말함 위의 사진에 따르면 stride는 1임 stride에 따라 output의 크기가 달라짐Padding Padding은 boundary에 버려지는 convolution 연산을 처리해주는 역할을 함 사진처럼 zero padding이 주어지면 input dim과 output dim을 동일하게 할 수도 있음Convolution Parameters CNN의 parameter를 계산할 때 padding, stride는 고려하지 않아도 됨 Kernel size, input channel size, output channel size를 모두 곱해주면 됨1 X 1 Convolution 약간 특수한 convolution 연산 실제로 이미지에서 뭔가 특징을 추출하거나 하는 역할을 하지는 않음 convolution 연산의 특성상 channel size를 바꿔주는 역할을 함 보통 큰 input channel을 reduction하는 과정에 사용Modern CNN CNN은 시대별로 변화하는 핵심 특징은 크게 3가지가 있음 network depth 증가 number of parameter 감소 performance 향상 AlexNet AlexNet은 CNN의 초석을 다진 모델 Key ideas ReLU GPU implementation pooling data augmentation Dropout 지금보면 매우 당연한 것들이지만 원래 당연한걸 시작한 것들이 대단한 법인 것처럼 AlexNet은 지금의 CNN 모델의 시작이었음 기존의 sigmoid나 tanh와 같은 activation function의 gradient 소멸 문제를 해결하고자 ReLU를 사용함 한계점은 kernel에서 11 X 11을 사용했다는 점으로 영역은 넓지만 parameter를 늘리게 된다는 것VGGNet VGGNet은 3 X 3 convolution filter만 사용 Dropout은 0.5로 설정했음 layer의 수에 따라 VGG16, VGG19로 구분channel size가 모두 128이라하자 3 X 3만 사용한 것은 같은 연산이라도 parameter를 더욱 줄이기 위한 방법 두 convolution 연산결과는 동일 하지만 parameter는 차이가 큼 3 X 3 : $3 \\times 3 \\times 128 + 3 \\times 3 \\times 128 = 294912$ 5 X 5 : $5 \\times 5 \\times 128 \\times 128 = 409600$ 이는 제곱연산의 수치 증가 폭이 숫자에 비례해서 커지기 때문에 나타나는 현상GoogLeNet GoogLeNet은 비슷한 네트워크가 반복되는 Networ in Network(NiN)방식을 사용 핵심은 NiN은 활용한 inception blocks inception block은 여러 response를 합치는 것도 있지만 핵심 역할은 parametr 수의 감소 앞서 1 X 1 convolution에서 설명한 아이디어를 활용 각 연산 앞이나 뒤에 1 X 1 Conv를 적용하여 channel의 dimension을 줄이는 방법을 활용함ResNet ResNet은 딥러닝의 Deep의 가치를 높여준 모델 Deep learning이란 이름에 맞다면 층이 깊어질수록 성능이 좋아야하는데, 실제로는 그렇지 않다는 것이 항상 문제였음 일정 layer 개수를 초과하면 오히려 성능이 떨어지는 결과가 나타남 ResNet은 identity map을 통해 이를 해결함 사실 layer의 수가 많아지면 성능이 떨어지는 이유는 Vanishung Gradient때문이었으므로 이를 해결하는 것이 목적 이전 층까지 학습된 결과와 추가된 레이어의 출력의 차이인 Residual만 학습하면 된다는 아이디어를 적용DenseNet ResNet의 아이디어를 확장한 개념 ResNet은 이전 층까지의 결과를 합치기 때문에 결과에 직접적인 영향을 주는 것이 문제가 될 수 있음 따라서 이전 측 결과를 합하는 게 아닌 concatenation, 즉 행렬적으로 값을 붙이는 방식을 활용 이때 channel 크기가 기하급수적으로 커지는 것을 막고자 채널 증폭 후 1X1 Conv를 활용한 dimension 감소를 반복하는 방식을 사용" }, { "title": "[BoostCamp AI Tech] Day13", "url": "/posts/day13/", "categories": "NAVER BoostCamp AI Tech, Review", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech", "date": "2022-02-07 23:00:00 +0900", "snippet": "Day13 Review당신은 오늘 하루 어떻게 살았나요? DL Basic 1~3강 팀구성오늘 하지 못한 것들 Coursera MLOps 1주차 AlexNet 3, 4 정리내일은 어떤 것을 할까? Coursera MLOps 1주차 AlexNet 3, 4 정리마무리 내용 정리를 좀 더 압축화할 필요가 있을듯 시간이 너무 많이 뺏김 어떻게 해결가능? 강의의 핵심 포인트를 집어내는 것이 중요할듯 MLOps 강의, 캐글 스터디, 카카오 아레나 스터디 시간 분배 잘하기" }, { "title": "[BoostCamp AI Tech / Level 1 - DL Basic] Day13 - Optimization", "url": "/posts/day13_3_optimization/", "categories": "NAVER BoostCamp AI Tech, Level 1 - DL Basic", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech, DL Basic, DL, ML, Optimization", "date": "2022-02-07 13:00:00 +0900", "snippet": "DL Basic : OptimizationIntroduction Model 학습의 기본적인 원리는 Gradient Descent Gradient Descent는 1차 미분의 결과의 최적화 알고리즘 1차 미분 결과의 최적화는 local minimum을 찾는 것 Important Concepts in OptimizationGeneralization Generalization은 모델의 성능으로 사용함 좋은 generalization performance는 Network의 성능이 train과 비슷한 것을 의미 하지만 기본적인 train set 성능이 낮으면 generalization performance가 좋아도 모델 성능 자체가 좋지 않을 수 있음 Overfitting(과적합) 과 Underfitting을 방지하는 것이 generalization의 가장 핵심적인 포인트Cross-validation K-Fold validation이라고도 함 학습 data가 적은 경우의 문제를 해결하고자 등장 전체 train data를 n등분하여 k번째 등분을 validaation data로 설정하고 n-k개의 데이터를 train으로 설정하여 학습을 하는 과정 n개의 valid set으로 검증하는 과정과 같음 일반적으로 cross-validation을 통해 최적 hyper parameter를 탐색하고 최종적으로 전체 데이터로 모델을 학습함Bias / Variance Variance : 유사 input에 대해 prediction이 얼마나 비슷한가? Bias : 전반적인 prediction이 target과 얼마나 떨어져있는가?\\[\\begin{aligned} \\mathbb{E}\\left[ (t-\\hat{f})^2 \\right] &amp;amp; = \\mathbb{E}\\left[ (t-f + f-\\hat{f})^2 \\right] \\\\ &amp;amp; = \\mathbb{E}\\left[ (f - \\mathbb{E}[\\hat{f}]^2)^2 \\right] + \\mathbb{E}\\left[ (\\mathbb{E}[\\hat{f}] - \\hat{f})^2 \\right] + \\mathbb{E}[ \\epsilon ]\\end{aligned}\\] 위 공식은 cost 에 대한 bias와 variance, noise의 관계성을 나타낸 것 공식에 따라 bias, variance, noise는 tradeoff 관계를 가짐Booststraping 데이터 셋에서 subsampling을 통해 여러 데이터 셋을 형성하고 그에 맞는 여러 모델들을 만드는 기법 형성된 model들의 예측값들을 보고 예측의 consensus(일관성)로 전반적인 uncertainty를 판단 보통 앙상블(Ensemble)기법에 많이 활용됨Bagging VS Boosting Bagging (Bootstrapping aggregating) 여러 모델들의 학습결과를 합쳐서 판단하는 방법 가장 좋은 결과를 선택하는 voting이나 평균치를 연산하는 averaging을 활용해서 정답을 도출 Boosting 모델을 여러개 만드는 것은 동일하지만 연속적으로 연결시켜 이전에 좋지 않은 결과를 보이는 case를 집중적으로 학습함 여러개의 weak learner를 합쳐서 1개의 strong model을 형성 Gradient Descent Methods Stochastic Gradient Descent : 1번에 1개의 데이터를 활용해서 gradient를 update Mini-batch gradient Descent : batch-szie sample을 활용해서 gradient를 update Batch Gradient Descent : 전체 데이터를 활용해서 gradient를 update 보통 min-batch 방식이 효과가 좋은 것으로 알려져있음Batch-Size batch-size를 적절히 잘 조절하는게 학습과정의 핵심 이때 보통 작은 batch-size가 더 효과가 좋은데, 이는 small-batch method는 flat minimizer이기 때문임 그림을 보면, flat minimum에서는 prediction이 target에서 벗어나도 실제 값이 큰 차이를 보이지 않음 하지만 sharp minumum은 prediction이 약간만 벗어나도 매우 큰 오차를 갖게되는 것을 볼 수 있음 Gradient Descent &amp;amp; Momentum\\[\\begin{aligned} W_{t+1} \\leftarrow W_{t} - \\eta g_{t}\\end{aligned}\\] $\\eta$ : learning rate $g_{t}$ : gradient 일반적으로 알고있는 gradient descent\\[\\begin{aligned} &amp;amp; a_{t+1} \\leftarrow \\beta a_{t} + g_{t} \\\\ &amp;amp; W_{t+1} \\leftarrow W_{t} - \\eta a_{t+1}\\end{aligned}\\] $a_{t+1}$ : accumulation $\\beta$ : momentum momentum의 역할은 진행한 gradient의 방향을 유지해주는 역할 수식을 보면 이전의 학습결과에 momentum을 처리해서 반영 학습을 하다보면 batch에 따라 oscillation의 형태로 gradient가 움직이는 경우가 있는데, 이때 momentum이 적용되면 이를 어느정도 보완해줄 수 있음Nestrov Accelerated Gradient (NAG)\\[\\begin{aligned} &amp;amp; a_{t+1} \\leftarrow \\beta a_{t} + \\nabla\\mathcal{L}(W_{t} - \\eta\\beta a_{t}) \\\\ &amp;amp; W_{t+1} \\leftarrow W_{t} - \\eta a_{t+1}\\end{aligned}\\] $\\nabla\\mathcal{L}(W_{t} - \\eta\\beta a_{t})$ : Lookahead gradient NAG는 momentum의 최적값을 지나치는 문제를 방지하는 목적을 갖고 있음 lookahead gradient는 momentum에의해 이동하는 곳에서 gradient를 계산하여 적용한 것 Adagrad\\[\\begin{aligned} W_{t+1} = W_t - \\frac{\\eta}{\\sqrt{G_{t} + \\epsilon}} g_{t}\\end{aligned}\\] $G_{t}$ : Sum of gradient squares $\\epsilon$ : for numerical stability Adagrad의 핵심 아이디어는 learning rate를 건드린다는 것 $G_{t}$에 이전 gradient들의 제곱이 누적되기 때문에 많이 변화한 parameter의 학습률을 낮추고 적게 변화한 parameter의 학습률을 높이는 방식을 선택 $\\epsilon$은 0으로 나누는 것을 방지하는 상수 Adagrad는 $G_{t}$가 너무 커지면 더 이상 학습이 일어나지 않는 문제가 발생Adadelta\\[\\begin{aligned} G_t = \\gamma G_{t-1} + (1-\\gamma)g_{t}^2 \\\\ W_{t+1} = W_{t} - \\frac{\\sqrt{H_{t-1} + \\epsilon}}{\\sqrt{G_{t}+\\epsilon}}g_t \\\\ H_t = \\gamma H_{t-1} + (1-\\gamma)(\\Delta W_t)^2\\end{aligned}\\] $G_t$ : EMA of gradient squares $H_t$ : EMA of differentce squares Adadelta에서 $G_t$와 $H_t$는 $\\gamma$가 recursive한 값으로 영향을 주므로 time의 영향을 받게 적용함 (Exponential Moving Average)RMSprop\\[\\begin{aligned} G_t = \\gamma G_{t-1} + (1-\\gamma)g_{t}^2 \\\\ W_{t+1} = W_{t} - \\frac{\\eta}{\\sqrt{G_{t}+\\epsilon}}g_t\\end{aligned}\\] RMSprop은 공식적으로 출판한 내용은 없고 그냥 이런식으로 하니 잘 되었다~ 라는 식으로 등장 이전에 Adagrad는 stepsize가 없었는데, 이를 수정하여 stepsize를 적용함Adam\\(m_t = \\beta_1 m_{t=1} + (1-\\beta_1)g_t \\\\v_t = \\beta_@ v_{t-1} + (1-\\beta_2)g_t^2 \\\\W_{t+1} = W_t - \\frac{\\eta}{\\sqrt{v_t + \\epsilon}}\\frac{\\sqrt{1-\\beta_2^t}}{1-\\beta_1^t}m_t\\) $m_t$ : momentum $v_t$ : EMA of gradient squares $\\frac{\\sqrt{1-\\beta_2^t}}{1-\\beta_1^t}$ : unbiased estimator Adam은 momentum에 RMSprop 개념이 합쳐진 방식 단, 연산에서 지수평균을 사용RegularizationEarly Stopping test set을 활용해 학습을 하는 것은 cheating이므로 valid set의 결과를 활용해 특정 metric 조건에 맞춰 학습을 조기멈춤하는 것Parameter Norm Penalty\\[\\text{total cost} = \\text{loss}(\\mathcal{D}; W) + \\frac{\\alpha}{2}\\lVert W \\rVert_2^2\\] weight decay라고도 함 parameter 폭발을 방지하는 것이 목적임Data Augmentation 주어진 데이터의 방향 혹은 거울상과 같은 다양한 변형을 취하는 것을 의미 더 많은 데이터셋을 위해 자주 사용 이미지 분류 문제에서 많이 사용Noise Robustness input이나 weight에 노이즈를 추가하는 것Label smoothing Mix-up, CutMix같은 방식을 통새 label의 decision boundary 처리를 도와주는 것 이미지 처리를 할 때 두 개의 label을 섞은 이미지를 사용하는 Mixup이나 일부를 잘라서 하비는 cutmix 데이터를 활용하면 더 좋은 학습 성능을 보이기도 함Dropout forward 과정에서 일정 확률로 랜덤하게 neuron의 연결 가중치를 0으로 설정하는 것 1개의 모델 구조에서 뉴런 구성을 변경하기 때문에 마치 여러개의 모델을 학습하는 것과 같은 효과를 가짐Batch Normalization\\[\\begin{aligned} &amp;amp; \\mu_{B} = \\frac{1}{m}\\sum_{i=1}^m x_i \\\\ &amp;amp; \\sigma^2_{B} = \\frac{1}{m}\\sum_{i=1}^m (x_i - \\mu_{B})^2 \\\\ &amp;amp; \\hat{x}_i = \\frac{x_i - \\mu_{B}}{\\sqrt{\\sigma^2_{B}+\\epsilon}}\\end{aligned}\\] Batch 정규화는 Gradient Vanishing/Exploding 문제를 해결하고자 등장 각 배치별로 데이터를 정규화하여 0 ~ 1의 수치로 조정" }, { "title": "[BoostCamp AI Tech / Level 1 - DL Basic] Day13 - MLP (Multi-Layer Perceptron)", "url": "/posts/day13_2_mlp/", "categories": "NAVER BoostCamp AI Tech, Level 1 - DL Basic", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech, DL Basic, DL, ML, MLP", "date": "2022-02-07 13:00:00 +0900", "snippet": "DL Basic : MLP (Multi-Layer Perceptron)Neural Networks 일반적으로 Neural Netwrok라고 하면 인간의 뇌 구조를 모방해서 만든 시스템이라고 생각함 하지만 우리의 비행기가 새를 모방했다고 하지만 새와는 다른 것처럼 Neural Net이 정확하게 인간의 뇌를 구현한 것이라 보긴 어려움 Neural Net의 실질적 정의는 function approximators가 적합 Affine transformations : layer간의 연산은 행렬 연산인 affine 연산을 사용 Nonlinear transforamtions : layer 중간에 activation function을 추가하여 non-linear 변환을 진행 Linear Neural Networks Linear NN은 기본적으로 정답모델을 찾을 때 각 parameter에 따라 편미분을 활용한 backpropagation을 진행\\[\\begin{aligned} w \\leftarrow w - \\eta \\frac{\\partial \\text{loss}}{\\partial w} \\\\ b \\leftarrow b - \\eta \\frac{\\partial \\text{loss}}{\\partial b}\\end{aligned}\\quad\\quad \\eta : \\text{stepsize}\\] loss값을 backpropagation을 통해 update할때 stepsize값을 잘 고려하는 것이 중요Multiplayer Perceptron Affine transform은 linear NN의 핵심 중 하나 weight matrix의 역할은 x의 input dimension을 y dimension으로 vector space를 변환해주는 역할을 함 선형대수학에선 변환을 행렬로 표현 \\[y = W_3^{\\intercal} \\mathbf{h}_{2} = W_{3}^{\\intercal}\\rho(W_{2}^\\intercal\\mathbf{h}_{1}) = W_{3}^{\\intercal}\\rho(W_{2}^\\intercal\\rho(W_{1}^\\intercal\\mathbf{x}))\\] hidden layer ($\\mathbf{h}_{n}$)을 통과한 값에 activation function($\\rho$)을 적용하지 않으면 단순히 weight matrix간의 행렬 곱이 되므로 큰 의미가 없음 따라서 hidden layer value에 non-linear function인 activation function을 추가해줌 Multilayer feedforward networks are universal approximators에 따르면 hidden layer가 1개라도 있으면 대부분의 continuous function을 포함하기 때문에 우리가 찾고자하는 모델 구조가 존재함을 보장Activation function 종류Loss function\\[\\begin{aligned} \\text{Regression Task} &amp;amp; \\qquad &amp;amp; \\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{d=1}^{D}(y_{i}^{(d)} - \\hat{y}_{i}^{(d)})^2\\end{aligned}\\] MSE를 많이 사용하지만 모델의 목적에 따라 알맞은 loss function을 선택해야함 MSE의 경우 반드시 squared로 계산할 필요는 없음 MSE는 오차의 제곱을 연산하므로 outlier target 발생시 loss 값이 급격하게 커져 전체 neural net을 망가트릴 수도 있음 이 경우 오차의 절댓값의 합을 연산하는($L_1$) MAE가 적당한 경우도 있음 \\[\\begin{aligned} \\text{Classification Task} &amp;amp; \\qquad &amp;amp; \\text{CE} = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{d=1}^{D}y_{i}^{(d)} \\log\\hat{y}_{i}^{(d)}\\end{aligned}\\] Cross Entropy는 보통 분류문제에서 많이 사용되는 Loss 예측의 probability가 정답 클래스가 아닌 경우에 값이 클수록 $\\log$ 연산에 의해 값이 증폭되는 현상이 나타남 \\[\\begin{aligned} \\text{Probablistic Task} &amp;amp; \\qquad &amp;amp; \\text{MLE} = \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{d=1}^{D}\\log \\mathcal{N}(y_{i}^{(d)};\\hat{y}_{i}^{(d)}, 1) &amp;amp; \\qquad \\text{(=MSE)}\\end{aligned}\\] MLE는 확률적 연산이 사용되는 loss function" }, { "title": "[BoostCamp AI Tech / Level 1 - DL Basic] Day13 - Historical Review", "url": "/posts/day13_1_history/", "categories": "NAVER BoostCamp AI Tech, Level 1 - DL Basic", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech, DL Basic, DL, ML", "date": "2022-02-07 10:00:00 +0900", "snippet": "DL Basic : Introcution &amp;amp; Historical Review딥러닝의 기본요소 Data 데이터는 “어떤 문제를 해결할 것인가?”에 따라 형식이 달라짐 Model 모델의 성질에 따라 동일한 데이터라도 다른 성능을 보인다. 심지어 같은 task를 해결하기 위해서도 모델에 쓰이는 Technique들이 다양하다. Loss Function 모델과 데이터가 정해졌을 때 어떻게 모델을 학습할 지를 결정하는 요소이다. Neural Net의 weight 조정을 할 때 사용 MSE, CE, MLE와 같은 다양한 loss function이 있는데, 이는 문제의 유형에 맞춰 선택한다. 하지만 loss function이 줄어드는 것이 반드시 좋은 결과를 말할 수는 없다. 또한 데이터의 특성에 맞춰 loss function을 다르게 결정할 필요도 있다. Optimiztion Algorithm 위의 모든 것들이 결정된 후 네트워크를 어떻게 줄일지를 결정하는 역할을 한다. 1차 미분을 활용한 SGD 접근법을 활용하였지만 성능이 좋지 않아 다른 방식도 사용 Momentum, NAG, Adagrad, Adadelta, Rmsprop와 같은 방식들을 사용하게된다. 추가적으로 regularizer들을 활용하기도 한다. Historical ReviewDenny Britz가 2020년에 발표한 Deep Learning’s Most Important Ideas - A Brief Historical Review에는 2020년까지의 주요 Deep Learning 논문들의 review가 담겨있다. 이를 간단하게 정리하겠다.AlexNet (2012)그림에서 알 수 있듯이 CNN이다. AlexNet이 탄생한 것은 224 X 224의 이미지를 분류하는 대회인 ILSVRC였다. 지금보면 왜 평범한 CNN이 딥러닝의 역사를 바꾼 것의 맨 처음을 장식하는 것일까? 라는 생각을 할 수 있는데 사실 AlexNet은 평범한 CNN은 아니었다.AlexNet이 나오기 이전, ILSVRC의 우승 모델은 DL 기반의 모델들이 아닌 커널기반, SVM과 같은 고전방식을 활용했다. AlexNet의 등장으로 최초의 DL 모델이 우승을 차지했고 이후 대 Deep Learning 시대가 열리게 되었다.단순히 “되겠지~”라고만 여겨지던 딥러닝이 드디어 Machine Learning에서 빛을 보이며 실력을 발휘하게 된 역사적인 사건이다.DQN (2013)지난 2016년에 있었던 역사적인 사건인 알파고 VS 이세돌의 시작인 DQN이다. DQN은 알파고를 개발한 딥마인드의 연구결과인데, 아타리라는 블록깨기 게임을 인공지능 모델 스스로 학습을 통해 해결하는 강화학습으로 풀어낼 때 사용한 방식이다. 사실 Q-Learning이라는 방식은 이전에 사용이 되었는데 이를 deep learning과 접목시킨 방식으로 학습을 한 것이다. DQN 연구로 구글이 딥마인드를 인수했다는 설도…Encoder / Decoder (2014)Encoder와 Decoder는 NLP에서 translation 문제를 해결하고자 나타났다. 다른 언어의 문장을 또 다른 언어의 문자으로 바꿔야하는 것이다. 즉 특정 Sequence를 다른 Sequence로 변환하는 작업을 하는 것이다. 결과적으로 Sequence-to-sequence(Seq-2-Seq)를 할 수 있게되었고 NLP분야의 발전이 일어났다.Adam (2014)Adam이다. ML/DL을 공부하는 사람 중 과연 Adam을 한번도 못 들어는 봤어도 한번만 본 사람은 없을 것이다. 그만큼 Adam optimizer는 가장 많이 사용되는 optimizer이고 준수한 성능을 낸다. 가장 통용적으로 많이 사용되는 optimizer가 등장한 것이다.GAN (2015)GAN은 어떻게보면 DL에서 가장 중요한 것 중 하나라고 말할 수 있다. Network가 스스로 학습데이터를 만들어서 학습을 하게 된 것이다.ResNet : Residual Network (2015)기존의 딥러닝은 많은 층의 network를 쌓게되면 오히려 성능이 떨어지게 되었다. 이름은 깊게 쌒았다는 의미의 딥러닝인데 정작 많이 쌓으면 성능이 떨어진다니… 하지만 ResNet이 나타나며 이전보다 깊은 수의 네트워크를 설정해도 overfitting이나 underfitting문제가 덜 발생하게 해주며 딥러닝이 이름 그대로의 값을 하게 되었다.Transformer (2017)Transformer가 등장하면서 기존의 RNN의 분야에서 RNN을 대체하게되었다. 하지만 이는 겨우 시작에 불과하였고 이제는 transfomer가 다른 분야에까지 사용되면서 CV영역까지 넘어오고있다.BERT (2018)앞서서 이미지 분류(Image Classification), 강화학습(Reinforcement Learning)의 패러다임을 일으킨 연구를 알아봤다. BERT는 대표 분야 중 하나인 NLP(Natural Language Processe)의 가장 큰 패러다임 변화를 일으킨 연구이다.많은 사람들이 NLP는 BERT 이전과 이후로 나눈다고 많이들 말하는데, 솔직히 개인적으로 써본 결과도 굉장히 압도적인 성능을 보였다.Fine-tuned NLP model이라고 하는데, 문제해결을 위한 학습 데이터가 많지 않을 때 일반적으로 주어진 큰 규모의 corpus를 활용하여 pre-training을 진행하고 해결하고자하는 데이터를 해당 모델에 fine-tuning을 하는 것이다.Big Language Models : GPT-X (2019)BERT의 최종 형태라고도 하는 GPT-X이다. Big Language Model이라고 하는데, 이는 parameter 개수가 굉장히 많아서 붙어진 이름이다.GPT-3의 등장으로 NLP의 연구는 굉장히 발전하게되었다.Self-supervised Learning (2020)모델을 학습할 때는 라벨이 붙은 데이터를 활용하는 것이 중요하다. 하지만 이는 cost가 많이 소요되는 작업이다. 이를 해결하고자 unlabled data를 활용해서 학습에 사용하는 것이다. 하지만 unsupervised와는 다르게 모델이 스스로 label을 형성하기 때문에 self supervised learning이라고 붙었다고 한다. 이해가 맞는지 모르겠음 ㅠㅠ" }, { "title": "[BoostCamp AI Tech] Day12", "url": "/posts/day12/", "categories": "NAVER BoostCamp AI Tech, Review", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech", "date": "2022-02-04 23:00:00 +0900", "snippet": "Day12 Review당신은 오늘 하루 어떻게 살았나요? 시각화 강의 Basic Seaborn Basic, Advanced 심화 포스팅 : jekyll blog &amp;amp; MathJax오늘 하지 못한 것들 없당~내일은 어떤 것을 할까? Coursera MLOps 강의 자금 지원 확인 AlexNet Paper Reading마무리 논문리뷰, 캐글 스터디, 카카오 아레나 스터디 준비 확실히 하고 시간 분배 잘하기" }, { "title": "[BoostCamp AI Tech / Level 1 - Data Viz] Day12 - Seaborn", "url": "/posts/day12_viz3/", "categories": "NAVER BoostCamp AI Tech, Level 1 - Data Viz", "tags": "NAVER, BoostCamp, AI Tech, Basic, Data visualization, Seaborn", "date": "2022-02-04 17:00:00 +0900", "snippet": "Data Viz : SeabornSeaborn Seaborn은 matplotlib에 확장성을 부여해준 라이브러리입니다. 개인적으로는 단순히 matplotlib만 쓰는 것보다는 seaborn을 위주로 많이 사용하는 편입니다. seaborn은 matplotlib에서보다 좀 더 디테일한 option을 부여해서 그래프와 같은 데이터 시각화가 가능하기 때문입니다.import seaborn as sns seaborn은 일반적으로 alias를 sns로 선언합니다.여러가지 plotBasic plotfig, axes = plt.subplots(1, 3, figsize=(18, 4))plots = [sns.countplot, sns.lineplot, sns.scatterplot]kws = [ {&#39;x&#39;:&#39;gender&#39;, &#39;data&#39;:student}, {&#39;data&#39;:flights_wide}, {&#39;x&#39;:&#39;math score&#39;, &#39;y&#39;:&#39;reading score&#39;, &#39;data&#39;:student, &#39;hue&#39;:&#39;race/ethnicity&#39;,}]for idx, ax in enumerate(axes.flatten()): plots[idx](**kws[idx], ax=ax) plt.show() seaborn은 기존의 matplotlib과 같이 barplot, lineplot, scatterplot과 같은 기능들도 지원하지만 seaborn의 가장 좋은 기능은 통계적 시각화가 가능하다는 것입니다.Statistical Plotfig, axes = plt.subplots(2, 4, figsize=(24, 8))plots = [sns.countplot, sns.boxplot, sns.violinplot, sns.histplot, sns.kdeplot, sns.ecdfplot, sns.rugplot]kws = [ {&#39;x&#39;:&#39;gender&#39;, &#39;hue&#39;:&#39;race/ethnicity&#39;, &#39;hue_order&#39;:sorted(student[&#39;race/ethnicity&#39;].unique()) }, {&#39;x&#39;:&#39;writing score&#39;}, {&#39;x&#39;:&#39;race/ethnicity&#39;, &#39;y&#39;:&#39;math score&#39;, &#39;hue&#39;:&#39;gender&#39;, &#39;order&#39;:sorted(student[&#39;race/ethnicity&#39;].unique())}, {&#39;x&#39;:&#39;writing score&#39;}, {&#39;x&#39;:&#39;writing score&#39;}, {&#39;x&#39;:&#39;writing score&#39;}, {&#39;x&#39;:&#39;writing score&#39;}]for idx, ax in enumerate(axes.flatten()): if idx == 7: continue plots[idx](data=student, ax=ax, **kws[idx])axes.flatten()[-1].axis(&#39;off&#39;)plt.show() seaborn은 countplot, boxplot, violinplot, histplot, kdeplot, ecdfplot, rugplot 등 다양한 종류의 통계적 지표를 표현해주는 시각화 메서드들이 있습니다.Heatmapfig, ax = plt.subplots(1,1 ,figsize=(7, 7))sns.heatmap(heart.corr(), ax=ax, vmin=-1, vmax=1, center=0, cmap=&#39;coolwarm&#39; )plt.show() seaborn의 heatmap입니다. 대체로 데이터 feature의 상관관계를 분석할 때 많이 사용합니다. 기본 heatmap은 시각적 효과가 좋지 않아서 약간의 수정을 진행하고 coolwarm을 사용하는 것이 좋습니다." }, { "title": "[BoostCamp AI Tech / Level 1 - Data Viz] Day12 - Matplotlib Details", "url": "/posts/day12_viz2/", "categories": "NAVER BoostCamp AI Tech, Level 1 - Data Viz", "tags": "NAVER, BoostCamp, AI Tech, Basic, Data visualization, Matplotlib", "date": "2022-02-04 15:00:00 +0900", "snippet": "Data Viz : Matplotlib DetailsText 시각화 과정에서 대부분 그림을 활용하지만 글자적인 요소를 많이 활용하기도 합니다. 텍스트는 대신 적절하게 사용하지 못하면 오히려 시각화를 방해하는 요소로 작용할 수 있습니다. 인간의 신체는 기본적으로 힘을 덜 들이는 방향을 우선하게됩니다. 어느정도 답을 제시하는 시각화 자료가 아니라 머리를 써 분석을 할 필요가 있다면 그림보다는 텍스트를 우선으로 읽을 수 있기 때문입니다. matplotlib에서 텍스트를 직접적으로 활용하는 것보다는 데이터의 수치나 제목, x/y축 값 등을 표현하는 데 많이 사용합니다. 이 처럼 일반적으로는 축이 어떤 값을 표현하는지, 이 그래프는 어떤 것을 주제로 처리하는지를 텍스트로 작성합니다. 개인적으로도 텍스트의 사용처 중 많이 사용하는 부분이 값을 표시하는 곳입니다.for idx, val in math_grade.iteritems(): ax.text(x=idx, y=val+3, s=val, va=&#39;bottom&#39;, ha=&#39;center&#39;, fontsize=11, fontweight=&#39;semibold&#39; ) 반복문을 활용해 직접 데이터에 접근해서 x, y 좌표에 맞춰 데이터를 입력한다. 이처럼 특정 데이터를 지칭하는 것도 가능합니다. 이는 보통 데이터프레임의 특정 index 데이터를 가져오는 방식을 사용하여 구현합니다.Color 시각화에서 물론 화려하게 표현해서 독자의 눈을 사로잡는 것이 반드시 좋은 것은 아닙니다. 색은 우리 생활에서 많은 의미를 담고 있기 때문입니다. 주변에서 간단하게 볼 수 있는 예시로 주식에서 빨간색은 양봉, 파란색은 음봉을 의미하는 것과 같습니다. 인간의 심리적 기저에 깔려있는 색상이 담고있는 의미들은 많습니다. 심지어 같은 색임에도 인간은 주제에 따라 다른 의미를 부여하기도 합니다.이 때문에 색은 시각화에서 함부로 쓰면 안되는 요소 중 하나입니다.출처 : CNN https://edition.cnn.com/election/2020/results/president 붉은색이 주식시장에서는 이익, 푸른색은 손해를 의미했지만 미국 대선 혹은 한국 대선에서는 다른 의미로 사용됩니다. 정말 신기하게도 인간은 같은 색이지만 다른 의미를 자동적으로 부여해서 해석합니다.범주형 자료 색상은 다양한 컨셉으로 의미를 갖습니다. 위에서 미국 대선의 공화당과 민주당은 일종의 범주형 자료(Categorical data)입니다. 범주형 자료는 색상 자체에 의미를 갖지는 않습니다. 물론 상징성은 있을 수 있습니다. 하지만 그 자체가 어떤 수치를 갖지는 않기 때문에 독립적인 색상을 활용하는 것이 좋습니다. 색 자체가 유사한 구성을 하는 것보다는 색이 뚜렷하게 구분이 되는 것을 고르는 것이 중요합니다.연속형 자료 정렬된 값을 가지는 순서형, 연속형 변수에 적합합니다. 배경색의 기준에 맞춰 큰 값이 변경되는데, 일반적으로 어두운 배경에는 밝은색이, 밝은 배경에는 어두운색이 큰 값을 의미합니다. 연속형 자료는 색상의 밝기가 곧 데이터의 강세를 의미하므로 단일 색조로 표현하는 것이 좋습니다.발산형 자료 발산형 자료는 연속형과 매우 유사하지만 중앙을 기준으로 데이터의 수치가 발산합니다. 기온, 지지율과 같은 상극의 값에 대한 비율적 지표를 나타내는 시각화에 효과적인 색상 지표입니다.Grid Grid는 데이터를 확인하는 것의 보조 자료로 사용하기 좋습니다.x = np.random.random(25)y = np.random.random(25)fig, ax = plt.subplots(1, 1, figsize=(5, 5))plt.scatter(x, y)ax.grid()ax.spines[&#39;right&#39;].set_visible(False)ax.spines[&#39;top&#39;].set_visible(False)ax.set_xlim([0, 1.2])ax.set_ylim([0, 1.2])plt.tilte(&#39;Grid&#39;)plt.show()) Grid는 다양한 형태로 표현할 수 있습니다. 이를 활용하면 다양한 데이터를 표현할 수 있습니다. 이처럼 특정 데이터들의 분류 군집화를 표시하는 방법도 가능합니다." }, { "title": "[BoostCamp AI Tech / 심화포스팅] Jekyll Blog와 MathJax", "url": "/posts/githubblog/", "categories": "NAVER BoostCamp AI Tech", "tags": "NAVER, BoostCamp, AI Tech, Jekyll, MathJax, LaTex", "date": "2022-02-04 00:00:00 +0900", "snippet": "Jekyll Blog와 MathJaxJekyll Blog 깃허브 블로그는 github_id.github.io 형식의 URL을 가진 블로그를 만들 수 있습니다. 블로그 URL을 자신의 깃허브에 repository로 생성하면 자동으로 contributors에 github-actions[bot]이 추가되고 Environments에 github-pages가 세팅이 됩니다. 깃허브 블로그는 Jekyll이라는 템플릿, 인라인 코드, 마크다운 같은 동적 요소들을 정적인 웹페이지로 구성해주는 파싱 엔진입니다. 직접 Jekyll Theme을 구성하고 제작하는 것도 좋지만 기본적으로 만들어진 템플릿 요소를 직접 커스터마이징 하는 것을 추천합니다.솔직히 말해서 직접 만드는 것은 신경써야 할 요소가 너무 많기도 하고 이 Jekyll이라 녀석이 liquid라는 언어를 활용하기 때문에 여러모로 번거로운 작업이 많아집니다. 설치 방법은 검색하면 저보다 잘 알려주는 글이 많기 때문에 저는 지킬 블로그 커스터마이징 위주로 설명하겠습니다.Jekyll Blog 테마 선택 Tip ‘Jekyll Blog 테마 추천’ 이런 검색어를 검색하면 맘에 드는 테마가 생각보다 많지 않을 것입니다. 이럴때 제가 대체적으로 많이 쓰는 방법은 여러 블로그를 돌아다니다가 맘에 드는 테마를 발견하면 그 블로그의 원본 테마링크를 타고 가거나 해당 블로그를 clone으로 다운받고 re cumtomizing을 하기도 합니다.대체로 해당 블로그 하단부에 테마의 원본 링크가 있습니다. 지금 제가 사용중인 테마는 Chirpy 테마인데 이 테마는 기존 지킬과 다르게 테마 적용에 상당히 난이도가 높았기때문에 지킬 블로그 구성이 익숙치 않으신 분에게는 추천드리지 않습니다. 초보 지킬 블로거에게 추천하는 테마는 Minimal Mistakes 기반의 테마입니다. 해당 검색어로 검색하면 세팅을 말하면서 Minimal Mistake 테마로 구성된 블로그들을 볼 수 있습니다. 참고하시면 될 것 같습니다.Jekyll Blog Customizing Tip제 블로그의 구성 폴더입니다. 지킬블로그는 Rudy를 기반으로 설치와 빌드를 하는 경우가 많기 때문에 .gem이나 .gemspec 파일이 동봉된 경우가 많습니다. 블로그의 구성 중 가장 중요한 파일은 _config.yml입니다.이 파일은 전체 블로그의 세팅을 처리하는 부분이 담겨있습니다. 또한 이 부분이 중요한 이유는 앞서 언급한 liquid의 특성 때문입니다. 최근의 웹페이지 구성은 head단, body단을 한 파일에 모두 넣는 방식이 아닌 분리해서 레고처럼 조립하는 방식을 선호하는 것 같습니다. 지킬 블로그 제작도 보통 동일한 방식으로 구성됩니다.blog head 태그 정보를 담고 있는 head.html의 일부 이 head.html에는 head태그에 들어가는 여러 javascript와 같은 세팅들이 포함되어 있습니다.따라서 다른 파일을 모두 건드릴 필요없이 새로운 CDN 기반의 애드온들을 추가하고 싶으면 이 파일에 코드 한줄만 추가/수정하면 됩니다. 예시 코드의 9번 줄을 보면 코드에 {% %}를 사용한 코드가 있는데 이 코드는 지킬의 전반적인 구성을 하는 liquid 언어가 동작되는 영역입니다. 이 부분 코드는 대략적으로 해석하면 page는 현재 페이지를 의미하고 그에 대해 여러가지 세팅 변수를 가져오는 것으로 이해했습니다.실제 자세히 분석은 안하고 대략적으로 필요할 때마다 체크합니다… 테마를 커스터마이징 할 때 팁이라면 우선 자신이 수정하고 싶은 부분을 크롬 개발자 모드(F12)등으로 페이지 분석을 합니다. 보통 디자인적 요소를 고치고 싶은 경우가 많아서 CSS파일 건드리는 경우가 많습니다. 예를 들어 어제 인용구 부분을 수정했었는데, 수정하고 싶은 부분을 Ctrl(cmd)+Shift+클릭 하면 해당 부분의 웹페이지 구성 코드를 볼 수 있습니다. 이렇게 찾은 부분의 태그를 폴더에서 검색으로 찾으면 연관된 코드를 찾을 수 있습니다. 그 후 해당 코드와 관련된 파일들을 수정하면서 변경사항을 체크하면됩니다. 변경사항 체크시 항상 깃에 푸시해서 체크하기가 어렵기 때문에 local mode로 페이지를 로드하고 supervisor모드로 해서 라이브로 수정된 변경사항을 볼 수 있는 방식으로 수정을 많이 합니다.bundle exec jekyll serve sMathJax MathJax는 MathML, LaTex를 활용해 마크 업 종류의 언어로 웹 페이지에 수학식을 표기해주는 자바스크립트 라이브러리입니다. 자바스크립트 라이브러리기 때문에 head부분에 cdn으로 구성된 JS 링크를 추가해줘야하는데, MathJax 설치 세팅은 검색해보시면 더 잘 나와있으므로 Skip… 사실 블로그마다 js 라이브러리 코드를 입력하는 파일이 달라서 잘 찾아서 넣으시면됩니다.MathJax Setting 기본적으로 MathJax 세팅을 알려드리겠습니다. 저같은 경우 파일 검색으로 MathJax를 해서 라이브러리 세팅코드가 된 부분을 찾았는데 없다면 javascript가 적용되는 파일을 찾아서 아래처럼 세팅해주시면 됩니다.{% if page.math %} &amp;lt;!-- MathJax --&amp;gt; &amp;lt;script&amp;gt; /* see: &amp;lt;https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options&amp;gt; */ MathJax = { loader: {load: [&#39;[tex]/color&#39;]}, tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ [&#39;$&#39;,&#39;$&#39;], [&#39;\\\\(&#39;,&#39;\\\\)&#39;] ], displayMath: [ /* start/end delimiter pairs for display math */ [&#39;$$&#39;, &#39;$$&#39;], [&#39;\\\\[&#39;, &#39;\\\\]&#39;] ], packages: {&#39;[+]&#39;:[&#39;color&#39;]} } }; &amp;lt;/script&amp;gt; &amp;lt;script src=&quot;https://polyfill.io/v3/polyfill.min.js?features=es6&quot;&amp;gt;&amp;lt;/script&amp;gt; &amp;lt;script type=&quot;text/javascript&quot; id=&quot;MathJax-script&quot; async src=&quot;https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js&quot;&amp;gt; &amp;lt;/script&amp;gt;{% endif %} 코드에서 가장 중요한 부분은 tex: 부분입니다. 이 부분에서 inlineMath와 displayMath의 명령어 세팅을 진행하는데, 보통 기본 세팅은 \\\\(과 \\\\[을 사용하는 방식이지만 편리성을 위해 많이들 $를 사용하는 방식을 씁니다. inlineMath란 지금 글에 적힌 것처럼 $1 + 2 = 3$ 과 같이 글 내부에 수식을 넣는 방식을 말합니다. displayMath란 글과 글 사이 간격에 공간을 차지하여 넓게 작성하는 방식입니다.\\[1 + 2 = 3\\] 그리고 밑에 있는 packages는 제가 컬러를 표현하기위해 직접 추가한 코드인데, 이런 추가적인 세팅은 MathJax공식 도규먼트에 사용법이 나와있습니다.자주 쓰는 수식 제가 자주 쓰는 수식들을 작성하는 방식을 설명하겠습니다.Basic\\[\\begin{aligned} ||\\mathbf{x} - \\mathbf{y}||_2 = ||\\mathbf{y} - \\mathbf{x}||_2 \\\\\\\\ \\cos\\theta = \\frac{&amp;lt; \\mathbf{x}, \\mathbf{y} &amp;gt;}{||\\mathbf{x}||_{2}||\\mathbf{y}||_{2}}\\end{aligned}\\]$$\\begin{aligned} ||\\mathbf{x} - \\mathbf{y}||_2 = ||\\mathbf{y} - \\mathbf{x}||_2 \\\\\\\\ \\cos\\theta = \\frac{&amp;lt; \\mathbf{x}, \\mathbf{y} &amp;gt;}{||\\mathbf{x}||_{2}||\\mathbf{y}||_{2}}\\end{aligned}$$ 기본적으로 알아두면 좋은 것들입니다. 가장 기본적으로 _{}와 ^{}는 각각 밑첨자, 윗첨자입니다. 중괄호로 묶지 않으면 바로 다음에 나타나는 문자만 인식하므로 대체로 중괄호를 쓰는 걸 추천합니다. 일반적으로 삼각함수 용어들은 \\text{cos} 등의 형태로 글을 정자로 작성하게 해주는 \\text{}를 쓰기도 하는데 보통 \\cos, \\sin, \\log 등 여러분이 생각하는 대부분의 표현은 이미 존재합니다. 분수를 표현하는 방식 중 가장 직관적인 표현 방법을 쓰려면 \\frac{}{}을 쓰면됩니다. 왼쪽 중괄호는 분자, 오른쪽 중괄호는 분모입니다. \\mathbf{}는 글자를 표현하는 방식입니다. 벡터는 일반적으로 위에 화살표는 긋는 경우($\\vec{X}$ a.k.a \\vec{})도 많지만 굵은 글씨로 표현하는 경우($\\mathbf{X}$)가 더 많습니다. 이때 사용하는 것이 \\mathbf{}입니다.Vector ML/DL 뿐 아니라 통계학 공부와 포스팅을 할 때 벡터를 사용하는 경우가 많습니다. 부캠 Vector에서도 벡터 수식을 굉장히 잘 쓴 것을 볼 수 있는데 작성 요령을 설명드리겠습니다.\\[\\begin{aligned} \\mathbf{X} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} \\quad \\mathbf{X} = \\begin{bmatrix} 1 &amp;amp; 2 &amp;amp; 3 \\end{bmatrix}\\end{aligned}\\]$$\\begin{aligned} \\mathbf{X} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} \\quad \\mathbf{X} = \\begin{bmatrix} 1 &amp;amp; 2 &amp;amp; 3 \\end{bmatrix}\\end{aligned}$$ 기본적으로 줄바꿈은 \\\\을 사용하는데, 이를 적용하기 위해서는 반드시 \\begin{aligned} 설정을 해야합니다. 벡터에서 줄바꿈은 \\\\를 쓰면되고 같은 칸을 맞춰 정렬할 때는 &amp;amp;를 쓰면 됩니다. 띄어쓰기의 경우 \\; 등 여러가지가 있는데 이게 칸이 매우 미세하게 조정되기 때문에 저는 보통 \\quad나 \\qquad를 씁니다. 행렬 혹은 벡터 표현에 사용되는 괄호는 디자인에 따라 명령어가 다릅니다. 사용하는 종류는 pmatrix, bmatrix, Bmatrix, vmatrix, Vmatrix 정도가 많이 사용됩니다. 그리고 이건 모든 displayMath에서 통용되는 것인데, \\begin과 \\end의 묶음이 하나의 줄을 차지합니다. 따라서 세로로 이어서 적고 싶으시다면 \\begin{aligned}안에 새롭게 \\begin{aligned}로 묶고 처리하셔도 됩니다.\\[\\begin{aligned} \\mathbf{X} = \\begin{bmatrix} x_{1} \\\\ x_{2} \\\\ x_{3} \\end{bmatrix} \\quad\\quad \\begin{aligned} L_{1} : ||\\mathbf{X}||_{1} = \\sum_{i=1}^{d}|x_{i}| \\\\ L_{2} : ||\\mathbf{X}||_{2} = \\sqrt{\\sum_{i=1}^{d}|x_{i}|^{2}} \\end{aligned}\\end{aligned}\\]$$\\begin{aligned} \\mathbf{X} = \\begin{bmatrix} x_{1} \\\\ x_{2} \\\\ x_{3} \\end{bmatrix} \\quad\\quad \\begin{aligned} L_{1} : ||\\mathbf{X}||_{1} = \\sum_{i=1}^{d}|x_{i}| \\\\ L_{2} : ||\\mathbf{X}||_{2} = \\sqrt{\\sum_{i=1}^{d}|x_{i}|^{2}} \\end{aligned}\\end{aligned}$$ 시그마를 쓰고 싶으실땐, \\sigma를 적으시면 $\\sigma$가 나옵니다. 이때는 \\sum을 적으면 됩니다. 참고로 말씀드리면 inlineMath모드에서는 bar(|)를 쓰고 싶으시다면 반드시 앞에 역슬래쉬 (\\)를 붙여주셔야 인식이 됩니다.Matrix\\[\\begin{aligned}\\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_1 \\\\ \\mathbf{x}_2 \\\\ \\mathbf{x}_3 \\\\ \\vdots \\\\ \\mathbf{x}_n \\end{bmatrix} = \\begin{bmatrix} x_{11} &amp;amp; x_{12} &amp;amp; ... &amp;amp; x_{1m} \\\\ x_{21} &amp;amp; x_{22} &amp;amp; ... &amp;amp; x_{2m} \\\\ \\vdots &amp;amp; \\vdots &amp;amp; x_{ij} &amp;amp; \\vdots \\\\ x_{n1} &amp;amp; x_{n2} &amp;amp; ... &amp;amp; x_{nm} \\end{bmatrix} = (x_{ij})\\end{aligned}\\]$$\\begin{aligned}\\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_1 \\\\ \\mathbf{x}_2 \\\\ \\mathbf{x}_3 \\\\ \\vdots \\\\ \\mathbf{x}_n \\end{bmatrix} = \\begin{bmatrix} x_{11} &amp;amp; x_{12} &amp;amp; ... &amp;amp; x_{1m} \\\\ x_{21} &amp;amp; x_{22} &amp;amp; ... &amp;amp; x_{2m} \\\\ \\vdots &amp;amp; \\vdots &amp;amp; x_{ij} &amp;amp; \\vdots \\\\ x_{n1} &amp;amp; x_{n2} &amp;amp; ... &amp;amp; x_{nm} \\end{bmatrix} = (x_{ij})\\end{aligned}$$ 행렬은 벡터의 연장선이라 크게 달라진 것은 없습니다. 같은 간격을 맞춰 열을 조정하려면 &amp;amp;, 행을 조정하려면 \\\\을 씁니다. 조금 참고할만한 내용은 가로, 세로, 대각선 점표시입니다. 이때는 \\vdots, \\cdots, \\ddots를 적으면 됩니다. 각각 vertical, column, diagonal의 약자인 것 같습니다.Long Expressions 대체로 저는 길게 수식을 적는걸 선호하진 않습니다. 이게 간지?는 나는데 적는 사람입장에선 굉장히 헷갈립니다. 우선 간단한 긴 식을 보여드리겠습니다.\\[\\begin{matrix} \\theta^{(t+1)} &amp;amp; \\leftarrow &amp;amp; \\theta^{(t)} - \\hat{\\nabla_{\\theta} \\mathcal{L}}(\\theta^{(t)}) \\\\\\\\ &amp;amp; \\Downarrow&amp;amp; \\\\\\\\ \\beta^{(t+1)} &amp;amp; \\leftarrow &amp;amp; \\beta^{(t)} + \\frac{2 \\lambda}{b}\\mathbf{X}^\\intercal_{(b)}\\left( \\mathbf{y}_{(b)} - \\mathbf{X}_{(b)}\\beta^{(t)} \\right)\\end{matrix}\\]$$\\begin{matrix} \\theta^{(t+1)} &amp;amp; \\leftarrow &amp;amp; \\theta^{(t)} - \\hat{\\nabla_{\\theta} \\mathcal{L}}(\\theta^{(t)}) \\\\\\\\ &amp;amp; \\Downarrow&amp;amp; \\\\\\\\ \\beta^{(t+1)} &amp;amp; \\leftarrow &amp;amp; \\beta^{(t)} + \\frac{2 \\lambda}{b}\\mathbf{X}^\\intercal_{(b)}\\left( \\mathbf{y}_{(b)} - \\mathbf{X}_{(b)}\\beta^{(t)} \\right)\\end{matrix}$$ 뭔가 여러가지가 많이 들어갑니다. 일단 이런 수식들은 정렬을 가지런히 맞추는 것이 중요합니다. 따라서 \\begin{matrix}를 활용해 괄호가 없는 행렬의 방식으로 작성합니다. 행렬 작성에서 &amp;amp;가 열을 정렬해준다고 했는데, 이 원리를 사용합니다.즉 쉽게 말하면 수식 묶음 하나하나가 하나의 행렬의 열에 들어간다고 생각하면됩니다. 제가 앞서 긴 수식은 한 흐름에 잘 안적는다고 했는데 이게 이유가 있습니다…\\[\\begin{aligned} \\begin{aligned} \\text{(1)}\\quad\\quad\\quad\\quad\\;\\;\\; \\partial_{\\beta_{k}}||\\mathbf{y} - \\mathbf{X}\\beta||_{2} = \\partial_{\\beta_{k}}\\left\\{ \\frac{1}{n}\\sum^{n}_{i=1}\\left( y_{i} - \\sum_{j=1}^{d}X_{ij}\\beta_{j} \\right)^2 \\right\\}^{1/2} \\end{aligned} \\\\\\\\ \\begin{matrix} \\text{(2)}\\quad\\quad\\quad \\nabla_{\\beta} ||\\mathbf{y} - \\mathbf{X}\\beta||_{2} &amp;amp;=&amp;amp; (\\partial_{\\beta_{1}}||\\mathbf{y} - \\mathbf{X}\\beta||_{2}, ..., \\partial_{\\beta_{d}}||\\mathbf{y} - \\mathbf{X}\\beta||_{2} ) \\\\ &amp;amp;=&amp;amp; \\left( -\\frac{\\mathbf{X}^{\\intercal}_{\\cdot 1}(\\mathbf{y}-\\mathbf{X}\\beta)}{n||\\mathbf{y}-\\mathbf{X}\\beta||_{2}}, ..., -\\frac{\\mathbf{X}^{\\intercal}_{\\cdot d}(\\mathbf{y}-\\mathbf{X}\\beta)}{n||\\mathbf{y}-\\mathbf{X}\\beta||_{2}} \\right) \\\\ &amp;amp;=&amp;amp; -\\frac{\\mathbf{X}^{\\intercal}(\\mathbf{y}-\\mathbf{X}\\beta)}{n||\\mathbf{y}-\\mathbf{X}\\beta||_{2}} \\\\\\\\ &amp;amp;\\Downarrow&amp;amp; \\\\\\\\ \\beta^{(t+1)} &amp;amp; \\leftarrow &amp;amp; \\beta^{(t)} - \\lambda\\nabla_{\\beta}||\\mathbf{y} - \\mathbf{X}\\beta^{(t)}|| \\\\ &amp;amp;=&amp;amp; \\beta^{(t)} + \\frac{\\lambda}{n}\\frac{\\mathbf{X}^{\\intercal}(\\mathbf{y}-\\mathbf{X}\\beta^{(t)})}{||\\mathbf{y}-\\mathbf{X}\\beta^{(t)}||_{2}} \\\\ \\end{matrix} \\\\\\end{aligned}\\] 이런 수식 적으려면 코드가 어마어마해집니다… 코드 첨부하면 너무 길어져서… 캡쳐로 대체하겠습니다." }, { "title": "[BoostCamp AI Tech] Day11", "url": "/posts/day11/", "categories": "NAVER BoostCamp AI Tech, Review", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech", "date": "2022-02-03 20:50:00 +0900", "snippet": "Day11 Review당신은 오늘 하루 어떻게 살았나요? 시각화 강의 기초오늘 하지 못한 것들 지킬 블로그 및 LaTex 내용내일은 어떤 것을 할까? 지킬 블로그 및 LaTex 내용마무리 쉴만큼 쉬었으니 다시 달려보자~~" }, { "title": "[BoostCamp AI Tech / Level 1 - Data Viz] Day11 - Basic Plots", "url": "/posts/day11_viz1/", "categories": "NAVER BoostCamp AI Tech, Level 1 - Data Viz", "tags": "NAVER, BoostCamp, AI Tech, Basic, Data visualization, Matplotlib", "date": "2022-02-03 19:00:00 +0900", "snippet": "Data Viz : Basic PlotsMatplotlibmatplotlib은 데이터 시각화에 가장 보편적으로 사용되는 라이브러리이다. 기존에 수학/통계에서 많이 사용하는 MATLAB에서 아이디어를 따왔다는 것으로 알려져있다. numpy, scipy와 같은 다양한 라이브러리와의 호환이 좋고 가장 많이 같이 사용되는 라이브러리는 pandas가 아닐까 싶다.최근에는 단순히 matplotlib만 사용하는 것이 아닌 확장버전인 seaborn과 plotly를 많이 사용하는 추세다.import matplotlib as mplimport matplotlib.pyplot as pltmatplotlib은 alias gudxofh mpl을 사용하고 가장 많이 사용되는 모듈은 pyplot으로 alias는 plt로 많이 활용한다.x1 = [1, 2, 3]x2 = [3, 2, 1]fig = plt.figure(figsize=(12, 7))ax1 = fig.add_subplot(211)ax2 = fig.add_subplot(212)ax1.plot(x1)ax2.plot(x2)plt.show()add_subplot()을 활용해서 plot들을 생성하는 방식을 사용하는 것이 기본 방식이다. 하지만 개인적으로 가장 많이 사용하는 방법은 plt.subplots(row, col, figsize=(r,c))를 사용하는 방법이다.fig, axes = plt.subplots(2, 2, figsize=(7, 7))plt.show()이 방법을 사용하면 axes에는 배열 형태로 plot의 위치가 주어지기 때문에 굳이 add_subplot을 하지 않아도 직접적으로 plot에 접근이 가능하다. 보통 seaborn과의 혼용에서 많이 사용하는 방법이다.Bar plotBar Plot은 우리가 자주 접하는 그래프 중 하나인 막대 그래프를 의미한다. 보통 범주형(Categorical) 자료를 비교할 때 많이 사용하는 그래프이다.막대 그래프를 표현하는 방식은 보통 plot을 여러개를 그리는 방식이 데이터를 직관적으로 바라보기 편리하다. 때로 일부 데이터에 한해서는 비율을 나타내는 Percentage Stacked Bar Chart를 사용하기도 한다.bar plot을 단순히 matplotlib 혼자서 쓰기에는 여러가지 테크닉을 적용하기에 어려움이 존재한다. 따라서 seaborn과 같이 사용해서 좀 더 다양한 테크닉을 적용한다.Principle of Proportion Ink The representation of numbers, as physically measured on the surface of the graphic itself, should be directly proportional to the numerical quantities represented.실제 값과 그에 표현되는 그래픽으로 표현되는 잉크 양은 비례해야 한다. bar plot은 기본적으로 데이터의 정량적인 크기를 주요하게 나타내야한다. 따라서 반드시 x축 값의 시작은 0에서 시작해야한다. 사실 이는 막대 그래프에만 적용되는 것은 아니다. 정량적인 지표를 표현해야하는 case에는 반드시 고려해야하는 내용이다.y축의 범위를 0에 맞추지 않으면 데이터의 차이가 매우 커보이는 효과를 갖게 할 수 있다. 데이터 시각화는 우리의 눈을 속일 수 있는 위험한 요소를 갖고 있기 때문에 이런 점을 주의해야한다. 정량적 지표를 표현할 때는 반드시 기준점을 0으로 맞출 필요가 있다.Line plotLine Plot은 전반적인 데이터의 추세를 보는 것이 주 목적이다. 따라서 반드시 데이터가 기준점이 0에 맞춰질 필요는 없다. 오히려 0에 맞추게 될 경우 데이터의 추세 흐름을 압축시켜버려 눈에 잘 보이지 않는 문제가 발생할 수 있다. 그래서 일반적으로 .set_ylim() y축의 범위를 조절하는 경우가 많다.line plot의 간격line plot은 데이터의 추세성을 보여주는 만큼 비례지표를 보여주는 것보다 x축 데이터의 간격이 중요하다. 따라서 x축을 주의해서 조절할 필요가 있다.2, 3, 5, 7, 9는 데이터의 간격이 2와 3에서 무너지는 현상이 나타난다. 이를 막고자 해당 데이터는 표현을 단순히 marker를 통해 나타내는 방식을 사용하는 것이 좋다.물론 x축 값을 살려야 할 경우 축 간격을 살려낼 필요가 있다.보간법Line plot의 성질상 데이터와 데이터를 연결해서 그래프를 표현한다. 이에 따라 결측치가 존재하면 전체적인 데이터의 추세가 무너질 수 있다. 따라서 시계열분야는 이전부터 다양한 방법으로 결측치를 처리하는 방식을 연구했다. 대표적인 방법은 결측치 보간법인데, forward fill, backward fill과 같은 방식도 많이 사용하지만 선형 보간법을 많이 사용한다.from scipy.interpolate import make_interp_spline, interp1d문제는 보간법은 실제 있는 데이터가 아닌 없는 데이터를 추가하는 것이므로 전반적인 데이터 분석에서 데이터 분포를 해칠 수 있다. 따라서 실질적인 분석에서는 활용을 지양하는 것이 좋다.이 외에도 sklearn의 KNNimputer와 같은 방식을 쓸 수도 있다. KNN방식으로 상위 N개의 유사 데이터의 값을 활용하는 방식이다.보간법 추가 자료 링크Scatter plotScatter Plot은 점을 활용해 2~3가지의 feature의 관계를 알기 위해 사용하는 그래프이다. 한글 이름으로는 산점도라고도 한다. 보통 단순히 scatter plot만 사용하는 경우는 별로 없고 다양한 그래프를 섞어서 많이 사용한다. scatter plot에서 크게 주의할 것은 많이 없지만 데이터의 전반적인 관계성을 주로 보기때문에 추세선을 잘 활용하는 것도 좋다.가장 많이 활용되는 부분은 데이터의 군집화를 시각화하는 과정에서 많이 사용한다.출처 : https://www.kaggle.com/maksimeren/covid-19-literature-clusteringScatter plot의 목적Scatter plot에서 확인할 필요가 있는 정보는 크게 3가지이다. 앞서 언급한 데이터 군집, 데이터간의 간격, 이상치(outlier) 이다.특히 데이터 군집과 이상치는 주요하게 볼 가치가 있는 정보이다. 데이터 군집은 앞서 간단히 설명하였고 이상치 값은 데이터 전처리 과정에서 주로 처리하는 것 중 하나이고 이 데이터들은 전바적인 데이터의 경향성을 해치는 경우가 많다." }, { "title": "[BoostCamp AI Tech / Level 1 - Data Viz] Day11 - Introduction", "url": "/posts/day11_viz_ot/", "categories": "NAVER BoostCamp AI Tech, Level 1 - Data Viz", "tags": "NAVER, BoostCamp, AI Tech, Basic, Data visualization", "date": "2022-02-03 13:00:00 +0900", "snippet": "Data Viz : IntroductionIntroduction출처: https://boostlabs.com/blog/10-types-of-data-visualization-tools/데이터 시각화(Data Visualization) 는 ML/DL을 진행하기 이전에 사용하는 데이터의 전반적인 분포를 확인하고 전처리를 진행하기 위해 필요한 과정이다. 시각화는 데이터를 바라보는 관점이 중요한데, 관점뿐 아니라 여러가지를 고려해서 시각화를 진행해야한다.시각화의 요소 목적 데이터 시각화를 진행하기에 앞서 왜 시각화를 진행하는가? 가 중요하다. 주식 데이터라면 주가의 흐름을 보기 위해서, 의료 데이터라면 어떤 환자의 특성이 질병과 관련이 높은지 등을 보게된다. 통계적인 수치가 드러나는 내용들을 시각적으로 표현한다면 데이터가 직관적으로 더 와닿을 것이다. 독자 이 시각화 자료를 누가 읽는가? 도 중요한 시각화의 요소이다. 연구자가 보는 것과 기업 매니저가 보는 시각은 당연히 다를 것이다. 많은 데이터 시각화는 일반적인 사람들 혹은 대회 참가자들에게 제공되는 시각화인 경우가 많아 일반화된 시각화를 많이 진행한다. 데이터 스토리 방법 디자인개인적으로 위에 언급한 요소들 중 주의깊게 고려하는 것은 데이터와 디자인이다. 데이터의 특성에 따라 표현해야하는 방식이 달라지기도 하고 바라봐야하는 관점도 달라지기 때문이다. 또한 디자인은 데이터를 조작할 수 있는 요소이기 때문에 매우 주의를 기울여야하는 부분이다. 통계의 함정이라는 말이 존재하듯이 데이터를 시각화한 결과는 그래프의 축, 원형 데이터의 비율과 같은 것들을 조절하면 데이터 수치를 건드리지 않고 사람들에게 데이터를 조작한 것과 같은 효과를 주게된다.찬성이 82.9%지만 마치 50%정도로 보이게 표현하고 있다. (출처: MBN 판도라)디자인은 특히 데이터를 바라보는 가장 중요한 요소인데, 우리는 객관적인 시각을 갖고 있다는 착각을 하고 있기 때문이다. 하지만 우리의 눈은 생각보다 객관적이지 않다. 사람을 많은 에너지를 소모하는 것을 피하는 방향으로 진화했기 때문에 직관적인 표현을 우선으로 인식한다. 그렇기 때문에 시각화를 진행하는 사람은 이와 같은 것을 반드시 고려해야한다. 우리의 눈은 생각보다 부정확하다.데이터데이터의 종류에 맞춰 시각화를 보는 관점도 다른데, 데이터셋의 형태에 따라 어떤 point로 시각화를 할 지 알아보자정형 데이터정형데이터는 가장 자주 접하게되는 데이터셋이다. 테이블 형태로 주어지는 데이터이며 일반적으로 csv, tsv 형태로 데이터가 제공된다. Row가 1개의 item, Column이 attribute(feature)를 의미한다. 시각화 Point 통계적 특성과 feature간의 관계 데이터 간의 관계 데이터 간 비교 시계열 데이터시계열 데이터는 시간의 흐름을 갖는 데이터이다. 시간적 흐름의 특징을 잘 살려야하고 계절성, 추게와 같은 특징을 살려야한다. 시각화 Point 추세(Trend) 계절성(Seasonal) 주기성(Cycle) 지리/지도 데이터지도 데이터는 단순 지도 위치만의 의미보다는 주위의 정보를 활용하는 것이 중요하다. 시각화 Point 주변 정보의 활용 거리, 경로 등의 정보 관계 데이터출처 : http://www.martingrandjean.ch/network-visualization-shakespeare/관계 데이터는 그래프로 표현하는 데이터에 많이 활용하는 경우가 많다. 그래프의 특수한 형태인 트리의 형태로 표현하는 경우가 많으며 객체간의 관계를 나타낸다. 시각화 Point 객체는 Node로 관계는 Link로 표현 크기, 색, 수 등으로 가중치를 표현 휴리스틱하게 노드를 배치 관계 데이터에서 포함관계가 존재한다면 Tree, Treemap 등으로 표현하기도 한다. " }, { "title": "[Elastic Search] Elasticsearch의 구조", "url": "/posts/elk_structure/", "categories": "Data Engineering, ELK", "tags": "Data Engineering, ELK, Elastic Search, Search Engine", "date": "2022-02-02 00:00:00 +0900", "snippet": "Elasticsearch 구조지난번에 기본적인 ELK stack의 설치와 실행을 확인했습니다. 이번에는 엘라스틱서치의 기본 구조부터 알아봅시다. 이번 내용은 이론적인 내용이 강해서 재미가 없을 수도 있습니다. 기본적으로 엘라스틱서치는 논리적 구조와 물리적 구조로 구분할 수 있습니다.논리적 구조우선 논리적 구조 로는 도큐먼트(document), 타입(type), 필드(field), 인덱스(index), 매핑(mapping)로 구성됩니다.1. 도큐먼트 (Document) 도큐먼트는 엘라스틱서치의 데이터 최소 단위입니다. JSON 오브젝트 하나를 일컫습니다. 이 형태는 MongoDB와 같은 NoSQL에서도 사용을 하고 있습니다. 도큐먼트는 내부에 여러가지 필드로 구성되어 있습니다. 특이한 점이라면 도큐먼트 내부의 필드 구성을 도큐먼트로 할 수 있습니다. 이를 Embedded documents라고 MongoDB에서는 말합니다.2. 타입 (Type) 여러 개의 document가 모여서 하나의 type을 형성합니다. 하지만 elasticsearch 7.0버전 이후부터 mapping type, 일명 타입이 삭제가 됩니다. 이는 엘라스틱서치의 인덱싱 방식과 어느정도 연관이 있습니다. 엘라스틱 서치가 빠른 검색 속도를 갖는 이유는 엘라스틱서치의 인덱싱 방식때문입니다. 여러 도큐먼트에는 다양한 필드가 존재합니다. 이때 RDB와 다르게 엘라스틱서치는 다른 타입에 존재하는 동일한 이름의 필드는 동일한 Lucene (루씬)필드에 지원합니다. 이렇게 될 경우 타입이라는 제약때문에 필드의 삭제, 수정이나 필드의 공통부분이 부족한 경우에 Lucene의 효율적 데이터 압축기능을 저해한다는 이유로 타입은 삭제가 되었고 인덱스가 역할을 대신합니다. (이해하지 못하셔도 괜찮습니다. 어차피 설명에서는 타입을 활용을 할 예정입니다.) 3. 필드 (Field) 필드는 도큐먼트의 데이터 타입입니다. 일반적으로 RDB의 column과 유사하다고 설명하지만 엄밀히는 틀린 말입니다. 엘라스틱서치의 빠른 검색 속도가 보장되는 이유는 필드에 저장된 데이터 토큰을 살려서 인덱싱이 진행되기 때문입니다. 실제 공식 문서에 따르면 도큐먼트를 검색 토큰으로 변환하여 해당 토큰이 저장된 도큐먼트를 연결합니다. 이를 역 인덱스 (Inverted Index)라고 합니다.위의 도큐먼트가 저장되어 있다고 가정하면 엘라스틱서치는 아래와 같이 인덱싱을 진행합니다.결과적으로 완전히 새로운 단어가 추가되는 경우가 아니면 데이터가 추가되어도 탐색하는 값이 늘어나지 않는 경우가 발생합니다. 따라서 굉장히 빠른 속도로 검색이 가능합니다.4. 인덱스 (Index) 인덱스는 인덱싱과정의 결과물을 말합니다. 도큐먼트들이 모여있는 집합체를 일컫기도 합니다. 엘라스틱서치에서는 인덱스라는 단어가 많이 사용되기 때문에 데이터의 저장단위는 인디시즈(indices)라고 말합니다. 인덱스는 기본적으로 샤드 (shard)라는 단위로 구분되어 저장됩니다. 샤드의 자세한 설명은 물리적 구조 설명에서 설명드리겠습니다. 7.x 버전에서는 디폴트로 1개의 샤드로 구성이 됩니다. 이 1개의 샤드는 프라이머리 샤드(primary shard)라고 부릅니다. 6.x버전에서는 5개의 샤드로 구성이 되었습니다. 기본세팅을 기준으로 말씀드리면 분산저장을 할 경우 5개의 프라이머리 샤드와 1개의 레플리카 샤드(Replica shard)로 구성됩니다. 자세한 설명은 이어지는 물리적 구조에서 설명드리겠습니다.물리적 구조물리적 구조 는 노드 (node), 샤드 (shard)로 구성됩니다. 이 2가지를 설명하기에 앞서 모든 것의 기반이 되는 클러스터 (cluster)를 설명하겠습니다.0. 클러스터 (Cluster) 클러스터는 엘라스틱서치의 가장 큰 시스템 단위입니다. 최소 1개 이상의 노드를 갖고 있으며 각 노드는 샤드로 구성됩니다. 서로 다른 클러스터는 독립적으로 유지되며 여러 서버가 하나의 클러스터를 구성하기도 하고 한개의 서버가 여러 개의 클러스터를 관리할 수도 있습니다.1. 노드 (Node) 노드는 엘라스틱서치의 클러스터에 있는 일종의 서버입니다. 노드에서 반드시 1개는 마스터 노드 (Master node) 역할을 수행합니다. 그리고 도큐먼트를 관리하는 데이터 노드 (Data node) 가 존재합니다. 이 외에도 더 존재하지만 설명은 나중에 진행하겠습니다. 마스터 노드(Master node) 는 노드 추가/제거, 인덱스 생성/삭제, 인덱스 메타 데이터 관리, 샤드의 위치와 같은 클러스터의 전반적인 정보와 상태를 관리하는 역할을 담당합니다. 마스터 노드 설정은 elasticsearch.yml에서 할 수 있습니다. 디폴트로 node.master: true로 설정된 경우 모든 노드가 마스터 노드의 후보(마스터 후보 노드)가 됩니다. 이 경우 현재 마스터 노드가 특정 이유로 동작을 중지하는 경우 클러스터가 동작을 중지하는 것을 막기 위해 마스터 후보 노드 중 1개가 마스터 노드가 됩니다. 마스터 후보 노드들은 이미 마스터 노드의 정보를 모두 공유하기 때문에 바로 이어서 마스터 노드의 역할을 수행할 수 있습니다. 하지만 클러스터의 규모가 커지며 노드와 샤드의 수가 많아지는 경우 모든 노드가 마스터 노드의 정보를 공유하는 것은 클러스터 자체에 과부하를 일으킬 위험이 존재합니다. 마스터 노드는 많은 정보를 담고 있는데 모든 노드가 방대한 양의 정보를 저장, 관리하면 그만큼의 용량이 증가하니까요. 이를 위해 마스터 후보 노드가 되지 않는 노드는 node.master를 false로 설정하여 관리해줍니다. 데이터 노드(Data node) 는 실제로 인덱싱이 된 데이터를 저장한 노드입니다. 이때, 마스터 후보 노드와 데이터 노드는 엄격하게 분리하여 각자의 역할을 수행하도록 하는 것이 중요합니다.그래서 마스터 후보 노드들은 node.data: false로 설정하여 마스터 노드의 역할만 수행하게 합니다. 이를 통해 마스터 노드는 데이터 저장에는 관여하지 않고 클러스터 관리에 집중하며 데이터 노드는 단순 데이터 처리 작업에만 집중할 수 있습니다. 실제 분산 저장이 되는 샤드가 위치하는 노드가 데이터 노드입니다. 또한 데이터 처리 작업이 집중적으로 일어나기 때문에 리소스 모니터링이 필요합니다. 2. 샤드 (Shard) 샤드는 인덱싱된 데이터가 여러 개의 부분으로 인덱스 내부에 분산 저장되어 있습니다. 이런 분산 저장이 엘라스틱서치의 핵심 저장방식입니다. 이렇게 나눠진 부분을 샤드라고 부릅니다. 엘라스틱 서치는 기본적으로 인덱스를 5개의 샤드로 나누어서 저장합니다. 이런 방식때문에 데이터의 크기를 수평적인 확장이 가능하고 작업이 여러 샤드에서 동시적으로 작업이 가능하여 병렬 작업이 가능합니다. (이는 NoSQL방식을 사용하는 많은 DB의 장점입니다.) 일반적으로 프라이머리 샤드(Primary shard) 와 레플리카 샤드(Replica shard) 로 구성됩니다. 프라이머리 샤드(Primary shard) 는 처음 생성된 샤드입니다. 데이터의 원본을 갖고 있으며 데이터 업데이트 요청이 들어올 경우 반드시 프라이머리 샤드에 요청이 수행됩니다. 그리고 해당 요청의 내용은 레플리카 샤드에 복제됩니다. 레플리카 샤드(Replica shard) 는 프라이머리 샤드에 문제가 발생할 경우 복구작업을 위한 샤드입니다. 프라이머리 샤드의 데이터가 무너질 경우 대신해서 프라이머리 샤드의 역할을 수행합니다. 복제본이라고 부르기도 합니다. 쉽게 이해를 하기위해 엘라스틱 가이드북 설명을 가져오겠습니다. 1개의 인덱스가 있을때, 5개의 샤드로 구성되고 4개의 노드로 이루어져 있다고 가정합니다. 이 경우 5개의 프라이머리 샤드와 5개의 레플리카 샤드가 4개의 노드에 분산되어 저장됩니다.출처:https://esbook.kimjmin.net/이때 같은 샤드와 복제본은 반드시 서로 다른 노드에 저장됩니다. 노드는 일종의 서버라고 했었죠? 만약 서버가 네트워크 문제같은 어떤 문제가 발생하여 작동을 정지하면 해당 노드의 정보를 어떻게 처리할까요?출처:https://esbook.kimjmin.net/위처럼 3번 노드가 작동을 중지하면 0번과 4번 샤드의 데이터를 잃게 됩니다. 이때 클러스터의 동작 순서는 다음과 같이 흘러갑니다.1) 클러스터는 우선 유실된 노드의 복구를 기다립니다. 만약 노드가 복구된다면 그대로 진행하면 되지만 타임아웃이 지날 경우 노드의 복구가 어렵다고 판단되면 샤드 복구 작업에 들어갑니다.2) 우선 복제본을 잃은 샤드를 확인합니다. 0번과 4번 샤드는 복제본이 사라져 1개의 샤드만 남아있습니다. 이 샤드의 레플리카 샤드를 다른 노드에 저장합니다.3) 이렇게되면 모든 샤드의 복구가 완료하여 전체 샤드의 개수가 유지되고 노드 유실이 발생해도 데이터 무결성과 가용성을 보장합니다.출처:https://esbook.kimjmin.net/최종적으로는 다시 위의 사진처럼 데이터 복구가 완료됩니다. 만약 프라이머리 샤드가 유실되는 경우에는 기존에 존재하는 레플리카 샤드가 프라이머리 샤드로 승격되고 다른 노드에 해당 샤드를 복제하는 과정이 수행됩니다.이렇게 길고 긴 엘라스틱서치 구조를 알아봤습니다. 이 내용은 사실 어렵고 재미없지만 상당히 중요합니다. 구조를 알아야 엘라스틱서치의 활용도를 최대한으로 끌어낼 수 있기 때문입니다. 그래서 좀 지루하더라도 잘 읽어보시면 좋겠습니다.다음시간에는 본격적인 엘라스틱서치 사용을 알아보겠습니다." }, { "title": "[BoostCamp AI Tech] Day10", "url": "/posts/day10/", "categories": "NAVER BoostCamp AI Tech, Review", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech", "date": "2022-01-28 18:00:00 +0900", "snippet": "Day10 Review당신은 오늘 하루 어떻게 살았나요? 심화포스팅 작성 torch.nn.Module 정리오늘의 피어세션 심화 포스팅 도리토스 : Back Propagation 계산과정 써리 : p-value / p-values 사용시 주의할 점 오늘 하지 못한 것들 심화과제내일은 어떤 것을 할까?마무리 이번주 회고 깊게 파보는 게 제일 중요한 것 같다. 이번주에 torch.nn.Module의 심화 포스팅을 진행한게 좋은 것 같다. 서로 자신의 관심분야를 자세하게 파서 설명해주니까 또 하나의 소규모 강의가 있는 것 같다. 조금 더 코딩 연습을 더 하면 좋겠는데 개인적으로 연습이나 내용을 정리할 것들을 진행해봐야겠다. MLOps나 일부 데이터 엔지니어링 관련 내용들을 추가로 주말이나 시간날때 공부하는 것이 좋을 것 같다. " }, { "title": "[BoostCamp AI Tech / 심화포스팅] torch.nn.Module 뜯어먹기", "url": "/posts/module/", "categories": "NAVER BoostCamp AI Tech, Level 1 - Python/PyTorch", "tags": "NAVER, BoostCamp, AI Tech, Python, Basic, PyTorch", "date": "2022-01-28 00:00:00 +0900", "snippet": "torch.nn.Module 뜯어먹기IntroductionPyTorch로 Machine Learning 모델링과 학습을 한다면 대부분 nn.Module을 상속해서 사용할 것이다. 그래서 대부분의 기능이 nn.Module에서 구현된 코드와 연결된 것이 많다. 좀 더 좋은 코드를 만들고 이해하고자 부스트캠프 조원들과 심화 포스팅을 하기로 했는데, 내가 하기로 한 심화 포스팅 주제는 nn.Module을 뜯어먹기 이다.많은 함수 분석을 하겠지만 모든 함수를 분석하진 않을 것이다. 그리고 외부에 보이는 메서드만 말고 내부에서 동작하는 메서드도 분석할 것이다.torch.nn.Module torch.nn.Module은 Nueral Network의 base class 역할을 한다. 우리의 모델은 nn.Module의 sub class로 구현이 된다. Module의 클래스에는 변수로 dump_patches와 _version, training, _is_full_backward_hook이 선언되어있다. 이 attribute들은 이후 다른 메서드들에서 사용이 된다.1. __init__(self)def __init__(self) -&amp;gt; None: torch._C._log_api_usage_once(&quot;python.nn_module&quot;) self.training = True self._parameters: Dict[str, Optional[Parameter]] = OrderedDict() self._buffers: Dict[str, Optional[Tensor]] = OrderedDict() self._non_persistent_buffers_set: Set[str] = set() self._backward_hooks: Dict[int, Callable] = OrderedDict() self._is_full_backward_hook = None self._forward_hooks: Dict[int, Callable] = OrderedDict() self._forward_pre_hooks: Dict[int, Callable] = OrderedDict() self._state_dict_hooks: Dict[int, Callable] = OrderedDict() self._load_state_dict_pre_hooks: Dict[int, Callable] = OrderedDict() self._modules: Dict[str, Optional[&#39;Module&#39;]] = OrderedDict() 개인적으로 여러군데에 사용되는 데 정확한 역할이 뭔지 찾지 못한 것이 있는데, torch._C에 있는 함수들의 원형을 찾기가 어려웠다. 이 외에는 대부분은 변수를 설정하는 역할을 한다. self.training은 이후 eval()함수와 train()함수에서 사용되는 훈련 세팅 여부를 결정한다. 우리가 세팅한 nn.Linear, Conv같은 레이어들은 nn.Module 기반이므로 self._modules에 {레이어 변수명 : 레이어 종류}형태로 저장한다. self._modules와 같이 self._parameters도 Parameter변수가 자동적으로 저장된다.2. forward()forward: Callable[..., Any] = _forward_unimplementeddef _forward_unimplemented(self, *input: Any) -&amp;gt; None: &quot;&quot;&quot; .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class:`Module` instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. &quot;&quot;&quot; raise NotImplementedErrordef _call_impl(self, *input, **kwargs): forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward) ... forward는 반드시 모든 subclass에서 오버라이딩을 통해 재현을 해줘야한다. 만약 구현이 되지 않는다면 NotImplementedError를 발생시킨다. _forward_unimplemented에 적힌 추가적인 설명에 따르면 우리가 정의한 함수는 등록된 hook들을 신경쓰며 수행하지만 정의한 함수가 Module instance를 호출하게되는 경우 hook을 무시하게된다고 한다.영어 자체가 해석하기 어렵게 적혀있어서…. 틀린 이해일 수 있다. _forward_unimplemented는 forward를 정의하지 않을 경우에만 호출된다. _call_impl함수에 따르면 가장 첫단계로 torch._C.get_tracing_state()를 통해 조건을 확인하고 self.forward의 구현부를 가져온다.3. apply(self, fn)def apply(self: T, fn: Callable[[&#39;Module&#39;], None]) -&amp;gt; T: for module in self.children(): module.apply(fn) fn(self) return self self의 children()을 통해 named_children을 호출하고 이는 yield 구문을 통해 module과 name을 반환해주는데, 이를 활용해 후위순회로 fn을 모듈에 적용한다.4. dump_patches, _version dump_patches와 _version은 module의 변화 상태를 기록하는 역할을 하는 것으로 보인다. 새로운 parameter와 buffer가 module에 추가/제거되면 충돌을 일으키는 역할을 dump_patches가 하는데, 이때 _load_from_state_dict가 _version의 번호를 비교하여 적절한 수행을 한다.5. eval(), train()def eval(self: T) -&amp;gt; T: return self.train(False)def train(self: T, mode: bool = True) -&amp;gt; T: if not isinstance(mode, bool): raise ValueError(&quot;training mode is expected to be boolean&quot;) self.training = mode for module in self.children(): module.train(mode) return self eval과 train은 현재 모델의 훈련 상태를 설정한다. eval은 모든 module의 self.training을 False로 만들고 train은 인자로 들어온 상태에 대해 self.training을 세팅하는 역할을 한다.apply도 그렇고 의외로 재귀 구문으로 module에 함수를 적용하는 패턴이 많이 발견된다.6. extra_repr(), __repr__()def extra_repr(self) -&amp;gt; str: return &#39;&#39;def __repr__(self): # We treat the extra repr like the sub-module, one item per line extra_lines = [] extra_repr = self.extra_repr() # empty string will be split into list [&#39;&#39;] if extra_repr: extra_lines = extra_repr.split(&#39;\\n&#39;) child_lines = [] for key, module in self._modules.items(): mod_str = repr(module) mod_str = _addindent(mod_str, 2) child_lines.append(&#39;(&#39; + key + &#39;): &#39; + mod_str) lines = extra_lines + child_lines main_str = self._get_name() + &#39;(&#39; if lines: # simple one-liner info, which most builtin Modules will use if len(extra_lines) == 1 and not child_lines: main_str += extra_lines[0] else: main_str += &#39;\\n &#39; + &#39;\\n &#39;.join(lines) + &#39;\\n&#39; main_str += &#39;)&#39; return main_str extra_repr 그 자체는 큰 의미가 있는 함수는 아니다. 이를 분석하기 위해서는 __repr__을 분석해야한다. __repr__에서 시작하자마자 self.extra_repr을 우선적으로 호출한다. 그 후 내용이 있다면 \\n을 기준으로 분리를 하고 일련의 과정을 통해 출력문을 설정한다.7. register_forward(_pre)_hook(self, hook) register_forward_hook과 register_forward_pre_hook은 둘다 객체의 attribute인 self._forward_hooks와 self._forward_pre_hooks에 입력으로 들어온 hook을 저장한다. forawrd_hook 코드 설명에 따르면 매 forward() 호출 후 출력에 대해 hook을 수행한다고 한다. output, input을 모두 수정할 수 있지만 forward가 호출된 이후이므로 input의 수정이 영향을 미치지 않는다. (원문 : The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after forward is called.) 실제 자세한 구동은 뒤에 서술할 _call_impl에서 과정을 설명하겠다. forward_pre_hook 코드 설명에 따르면 forward() 호출 이전에 invoke된다. input 수정이 가능하고 return이 가능하다. 단일 value를 return 하는 경우 tuple로 값을 wrapping한다. 8. register_full_backward_hook(self, hook) 객체의 attribute인 self._is_full_backward_hook을 True로 변경한다. 이후 self._backward_hooks에 hook을 등록한다. 입력에대한 gradient가 계산될 때마다 hook을 호출한다. hook은 argument들을 수정하면 안되지만 선택적으로 (Optionally) grad_input대신 사용할 new gradient의 반환이 가능하다. input과 output을 수정할 경우 에러를 발생시킨다.(원문 : Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error.)9. __call__, _call_impl(*input, **kwargs) 객체가 호출될 때 수행되는 magic method 함수이다. 이 부분 코드에 의해 단순히 객체가 call됨에도 forward 연산이 수행된다. 세세하게 뜯어보긴하겠지만 모든 코드를 해석하진 않을 것이다.__call__ : Callable[..., Any] = _call_impldef _call_impl(self, *input, **kwargs): forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward) # If we don&#39;t have any hooks, we want to skip the rest of the logic in # this function, and just call forward. if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks or _global_forward_hooks or _global_forward_pre_hooks): return forward_call(*input, **kwargs) # Do not call functions when jit is used full_backward_hooks, non_full_backward_hooks = [], [] if self._backward_hooks or _global_backward_hooks: full_backward_hooks, non_full_backward_hooks = self._get_backward_hooks() if _global_forward_pre_hooks or self._forward_pre_hooks: for hook in (*_global_forward_pre_hooks.values(), *self._forward_pre_hooks.values()): result = hook(self, input) if result is not None: if not isinstance(result, tuple): result = (result,) input = result bw_hook = None if full_backward_hooks: bw_hook = hooks.BackwardHook(self, full_backward_hooks) input = bw_hook.setup_input_hook(input) result = forward_call(*input, **kwargs) if _global_forward_hooks or self._forward_hooks: for hook in (*_global_forward_hooks.values(), *self._forward_hooks.values()): hook_result = hook(self, input, result) if hook_result is not None: result = hook_result if bw_hook: result = bw_hook.setup_output_hook(result) # Handle the non-full backward hooks if non_full_backward_hooks: var = result while not isinstance(var, torch.Tensor): if isinstance(var, dict): var = next((v for v in var.values() if isinstance(v, torch.Tensor))) else: var = var[0] grad_fn = var.grad_fn if grad_fn is not None: for hook in non_full_backward_hooks: wrapper = functools.partial(hook, self) functools.update_wrapper(wrapper, hook) grad_fn.register_hook(wrapper) self._maybe_warn_non_full_backward_hook(input, result, grad_fn) return result torch._C._get_tracing_state()의 상태가 False면 self.forwad()를 사용 hook이 없다면 앞서 할당한 foward_call attribute를 바로 반환한다. if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks or _global_forward_hooks or _global_forward_pre_hooks): return forward_call(*input, **kwargs) hook이 등록된 경우의 순서는 다음과 같다. 우선 backward와 관련된 hook들을 미리 확인하여 저장한다. full_backward_hooks, non_full_backward_hooks = [], [] if self._backward_hooks or _global_backward_hooks: full_backward_hooks, non_full_backward_hooks = self._get_backward_hooks() pre_hook들을 확인하고 존재한다면 input에 hook을 적용한다. if _global_forward_pre_hooks or self._forward_pre_hooks: for hook in (*_global_forward_pre_hooks.values(), *self._forward_pre_hooks.values()): result = hook(self, input) if result is not None: if not isinstance(result, tuple): result = (result,) input = result bw_hook으로 input에 hook을 설정하고 forward를 진행한다. bw_hook = None if full_backward_hooks: bw_hook = hooks.BackwardHook(self, full_backward_hooks) input = bw_hook.setup_input_hook(input) result = forward_call(*input, **kwargs) 그 후 forward_hook과 관련된 내용들을 확인한다. 만약 값들이 존재한다면 앞서 연산한 forward의 결과에 forward_hook들을 적용한다. 이런 이유로 input을 변경해도 forward값에 영향을 주지 못하는 것으로 보인다. if _global_forward_hooks or self._forward_hooks: for hook in (*_global_forward_hooks.values(), *self._forward_hooks.values()): hook_result = hook(self, input, result) if hook_result is not None: result = hook_result 그 후 앞서 backward_hook을 확인해서 저장하는 bw_hook에 값이 있다면 앞서 계산한 forward의 연산 결과에 setup_output_hook을 사용해 설정을 해주는 것으로 보인다. 해당 메서드에 대한 설명이 자세히 나와있지 않아서 자세한 설명이 어렵다… " }, { "title": "[BoostCamp AI Tech] Day9", "url": "/posts/day9/", "categories": "NAVER BoostCamp AI Tech, Review", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech", "date": "2022-01-27 23:40:00 +0900", "snippet": "Day9 Review당신은 오늘 하루 어떻게 살았나요? PyTorch Day4 torch.nn.Module 정리오늘의 피어세션 멘토링 : 공부할 때 Tip drawio 라는 프로그램을 활용해 필요한 이미지 생성가능 수식이 예쁘게 만들 수 있다. overleaf.com 여러 자료들을 오픈소스로 사용가능 허들을 낮춰주는 역할 chaper, section 등 자동 분류 텍스트를 템플릿화 해주고, 템플릿을 텍스트화 해줌. {enumerate} 번호가 있는 목차 {itemize} 블랫이 있는 목차 수식은 $$보다 [ ] 권장 attention is all you need (Transformer 논문) 예시 blog 자료에 오타가 있는 것에 주의 kocw에서 대학 강의를 들을 수 있다. 시각화 : https://jehyunlee.github.io/오늘 하지 못한 것들 심화과제내일은 어떤 것을 할까? 심화과제마무리 심화과제 같은 코드적인 이해를 좀 늘려보자…" }, { "title": "[BoostCamp AI Tech / PyTorch] Day9 - Hyper-parameter Tuning", "url": "/posts/day9_torch8/", "categories": "NAVER BoostCamp AI Tech, Level 1 - Python/PyTorch", "tags": "NAVER, BoostCamp, AI Tech, Python, Basic, PyTorch, Hyperparameter", "date": "2022-01-27 13:00:00 +0900", "snippet": "PyTorch : Hyper-parameter TuningIntroduction 모델 성능을 좋게 만드는 방법은 크게 3가지가 있다. 모델 변경 data check hyperparameter tuning 모델을 변경하는 것은 사실 요즘 좋은 backbone 모델이 있어서 큰 의미는 없다. 가장 좋은 성능 향상을 보이는 것은 data preprocessing인데, 이 부분이 요즘 가장 화두가 되는 MLOps와 관련이 깊다. Hyper parameter tuning은 좋은 성능을 보일 것이라 생각했지만 요즘은 별로 큰 향상을 보이지는 못한다.Hyper-parameter Tuning 모델 스스로 학습하지 않는 값을 사람이 직접 지정하며 모델의 성능을 확인한다. learning rate, 모델 크기, optimizer 등등 요즘은 AutoML의 발달로 이 부분도 많이 축소가 되고 있다. 가장 기본적인 방법론에는 Grid Search와 Random Search가 있다.Random Search for Hyper-parameter Optimization Grid Search란 왼쪽 그림처럼 해당하는 경우의 수 조합을 모두 확인하는 방식이다. 내 기억으론 GridSearchCV라는 함수를 쓰면 TensorFlow에서 튜닝을 했는데 토치는 될지 모르겠다. 또 다른 방법은 Random으로 가장 좋은 조합을 찍어내는 것이다. 과거에 자주 사용하는 방식은 Random Search로 탐색을 하다가 잘 나오는 부분에서 Grid Search를 진행하는 방식을 썼다. 요즘은 베이지안 방식을 많이 사용하는데 이 방법은 BOHB 2018에서 참고할 수 있다.Ray Spark를 개발한 연구실에서 개발한 multi-node multi processing 지원 모듈이다. 머신러닝의 병렬 처리에서 자주 사용하기 위해 만들어졌다. Ray Settingsfrom ray import tunefrom ray.tune import CLIReporterfrom ray.tune.schedulers import ASHASchedulerdata_dir = # your directoryload_data(data_dir)config = { &#39;&#39;&#39; your hyper-parameter tuning configuration &#39;&#39;&#39;}scheduler = ASHASceduler(metric=metric, mode=mode, max_t=max_num_epochs, grace_period=1, reduction_factor=2)reporter = CLIReporter(metric_colums=metric_columns)result = tune.run( partial(train_cifar, data_dir=data_dir), resource_per_trial={&#39;cpu&#39;:cpu_num, &#39;gpu&#39;:gpus_per_tiral}, config=config, num_samples=num_samples, scheduler=scheduler, progress_reporter=reporter) config에 우리가 테스트할 hyper-parameter 조합을 설정한다. scheduler가 중요한데, 스케쥴러를 세팅하는 방식에 따라 어떤 값을 체크하고 어떻게 파라미터 조합을 짤 지를 결정한다." }, { "title": "[BoostCamp AI Tech / PyTorch] Day9 - Multi-GPU 학습", "url": "/posts/day9_torch7/", "categories": "NAVER BoostCamp AI Tech, Level 1 - Python/PyTorch", "tags": "NAVER, BoostCamp, AI Tech, Python, Basic, PyTorch, GPU", "date": "2022-01-27 11:00:00 +0900", "snippet": "PyTorch : Multi-GPU 학습Introduction Single GPU란 GPU가 1개가 존재하는 것이고 Multi GPU란 1개의 컴퓨터에 여러개의 GPU가 존재하는 것을 의미한다. 일반적으로 Node라고 표현하면 1대의 컴퓨터를 의미한다. 요즘 많은 컴퓨터는 Single Node Multi GPU 방식을 사용한다. 대용량 서버실은 Multi Node, Multi GPU방식을 쓴다. NVIDIA에서 이런 Multi GPU를 위한 TensorRT라는 기술을 공개하기도 했다.Parallel 다중 GPU에서 학습을 분사하는 방법은 2가지가 있다. 모델의 병렬화 데이터 병렬화 모델을 나누는 방식은 이전부터 많이 활용했었고 대표적으로는 AlexNet이 있다. 요즘은 큰 모델을 생성하는 추세에 맞춰 모델을 나누는 방식을 사용한다. 하지만 모델 병렬화는 모델의 병목과 파이프라인 구성의 어려움으로 쉽지 않은 과제이다. Model Parallel 모델 병렬화 과정에서는 forward와 backward의 수행과정이 서로 다른 GPU간에 병렬적으로 수행해야한다. 만약 하나의 수행이 진행되는 동안 다른 GPU가 작업을 진행하지 않는다면 이는 병렬화의 의미가 많이 없다.class ModelParallelResNet(ResNet): def __init__(self, *args, **kwargs): super(ModelParallelResNet,, self).__init__( Botleneck, [3, 4, 6, 3], num_classes=num_classes, *args, **kwargs) self.seq1 = nn.Seqeuntial(....).to(&#39;cuda:0&#39;) self.seq2 = nn.Sequential(....).to(&#39;cuda:1&#39;) self.fc.to(&#39;cuda:1&#39;) def forward(self, x): &#39;&#39;&#39; your forward function &#39;&#39;&#39; x = self.seq2(self.seq1(x).to(&#39;cuda:1&#39;)) return self.fc(x.view(x.size(0), -1)) seq1과 seq2를 각각 GPU에 할당해준다. 이후 forward 과정에서 적절히 forward를 수행하고 한쪽의 값을 다른 GPU에 다시 할당하는 방식을 쓰면 된다. 그러면 모델을 각자 사용해서 연결하는 방식을 쓰는 것인데, 이러면 병목이 발생해서 잘 쓰지 않기도 한다.Data Parallel 모델을 분할하듯이 데이터를 분할해서 각 GPU가 데이터를 처리하는 것이다. mini-batch 방식과 유사하지만 한번에 동시에 여러 GPU가 작업을 수행한다. PyTorch에서는 2가지 방식을 제공한다. DataParallel : 데이터를 분배한 후 다시 하나의 GPU에 합쳐 평균을 계산한다. 이 방법은 GPU 1개가 떠앉는 overhead가 크다는 문제가 있다. GPU 병목현상이 발생할 수 있다. DistributedDataParallel DataParallel의 단점을 해결하고자 나온 방식이다. 각 CPU와 GPU를 매핑하여 개별적인 연산을 진행하고 연산결과만을 합쳐 평균을 계산한다. 단순 평균치 연산만 이뤄지므로 overhead가 적다. 기본적으로 DataParallel로 하고 개별적 연산의 평균을 낸다. 기본 세팅import torch# DataParallelparallel_model = torch.nn.DataParallel(model)# DistributedDataParallelsampler = torch.utils.data.distributed.DistributedSampler(train_data)shuffle = Falsepin_memory = Truetrain_loader = torch.utils.data.DataLoader(train_data, batch_size=20, shuffle=shuffle, pin_memory=pin_memory, num_workers=3,sampler=sampler) 메인 코드import torchdef main(): n_gpu = torch.cuda.device_count() torch.multiprocessing.spawn(main_worker, nprocs=n_gpus, args=(n_gpus, ))def main_worker(gpu, n_gpus): &#39;&#39;&#39; your settings &#39;&#39;&#39; batch_size = int(batch_size/n_gpus) num_worker = int(num_worker/n_gpus) # set multiprocessing protocol model = MODEL torch.cuda.set_device(gpu) model = model.cuda(gpu) model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[gpu])from multiprocessing import Pooldef f(x): return # your functionif __name__ == &#39;__main__&#39;: with Pool(4) as p: print(p.map(f, [1,2,3]))" }, { "title": "[Elastic Search] WSL을 통한 ELK 설치 및 실행", "url": "/posts/elk_install/", "categories": "Data Engineering, ELK", "tags": "Data Engineering, ELK, Elastic Search, Search Engine", "date": "2022-01-27 00:00:00 +0900", "snippet": "들어가며…오늘부터 본격적으로 ElasticSearch와 Kibana, Logstash를 활용한 Bigdata processing과 visualization을 설명하겠습니다. 블로그 메인에도 있고 전체적인 포스팅의 분포를 보시면 알 수 있듯이 저는 Bigdata와 DataScience에 관심이 많습니다. 그래서 관련 공부도 많이 하고 있죠. 상당히 힘듭니다…ㅠㅠ사실 작년 초부터 카카오 인재영입을 SNS로 팔로우하며 정보를 모으고 있던 와중 카카오 데이터 사이언티스트/플랫폼 개발자 채용 공고를 보았습니다. (취직을위해 알아봤다기보단 뭐가 필요한지 알기 위해서 입니다.)해당 공고에서 요구했던 사항은 통계 지식, python등 많이 있었고 그 중에도 elasticsearch가 있었습니다. 물론 그때는 이제 막 1학년을 마친 뭣 모르는 때라 모든 게 다 필요할 것이라 생각했지만 지금 생각해보면 ELK는 플랫폼 개발자의 역량에 가까운 것 같네요. 물론 데이터의 시각화와 분석에 ELK를 사이언티스트도 필요하다고 생각합니다. 그래서 아무것도 모르고 “elasticsearch? 이름도 뭔가 멋지고… 알아보니까 카카오, 네이버 같은 검색사이트들이 많이 쓰네?? 공부하자!!!”라는 생각으로 공부에 뛰어 들었습니다.하지만 아시는 분들은 아시겠지만 이 ELK라는 것이 애초에 연식이 그렇게 오래된 기술이 아닙니다.제 나이보다 어려요..그렇다보니 검색을 해도 자세한 한글 포스트도 많이 없고 이게 또 ELK stack이 원래는 각자 독립적인 개발이었다가 합쳐진 거라 공식 문서도 딱히 그렇게 친절하진 않습니다.(물론 다양한 OS환경에 따라 코드가 다 적혀있는 건 맘에 들더라구요)그래서 제가 공부하며 얻은 지식들을 이번 ELK 시리즈에 녹여낼 생각입니다.ELK StackELK Stack, 우선 꽤 낯선 이름입니다. 일반적으로 Elastic Stack이라고도 부릅니다. Elasticsearch, Logstash, Kibana로 이어지는 하나의 데이터의 검색, 분석, 시각화를 도와주는 것을 ELK Stack이라고 합니다. 요즘은 위의 3개에 추가로 Filebeats도 추가해서 많이 사용되는 추세입니다. 엘라스틱 서치는 현재 국내외 많은 기업들이 데이터 로깅과 처리에 사용을 합니다. 요즘처럼 빅데이터가 중요한 시기에 아주 적합한 스택이라고 말합니다. 성능자체가 굉장히 우수하기 때문에 많은 양의 데이터 처리가 용이하고 분산 처리가 가능하다는 장점이 존재하기 때문입니다.네이버 클라우드 플랫폼의 ELK stack네이버는 클라우드 플랫폼 자체에 Hadoop Ecosystem과 ELK stack을 활용하고 있습니다. 일반적으로 Hadoop, Kafka, Spark를 통해 데이터를 저장을 하면 logstash를 통해 로그 및 이벤트 데이터를 수집합니다. 이렇게 수집된 로그 데이터를 elasticsearch는 의미있는 방법으로 검색이 가능하게 만들어 줍니다. 그 후 데이터들을 활용해서 데이터 시각화 및 분석을 위해 kibana를 활용하는 파이프라인이 일반적으로 구축됩니다.&quot;그렇다면 왜? 엘라스틱서치를 검색엔진 사이트들은 많이 쓸까요?&quot;이유는 단순합니다. 많은 양의 데이터를 빠르게 처리하기 때문입니다. 엘라스틱서치는 자바루씬을 기반의 엔진이 구동됩니다. 그래서 빠른 검색시간을 추구합니다. 일반적으로 검색에 소요하는 시간을 1초 정도로 추구한다고 알려져있습니다. 또한 log기록을 실시간으로 처리하는 상황에서 상당히 효과가 좋습니다. 엘라스틱서치의 핵심 개념 중 하나가 바로 Near Realtime으로 거의 실시간에 근접하는 검색 시간을 갖습니다. 그리고 클러스터와 노드를 활용한 데이터 관리를 합니다. 이 방식을 통해 연합 인덱싱과 데이터 식별을 할 수 있는 장점이 있습니다.그리고 무엇보다 빅데이터에 최적화인 JSON형식으로 도큐먼트를 저장하며 RESTful API를 통해 통신합니다.그렇다면 이제 ELK를 설치하러 가봅시다!ELK 설치ELK 설치에 앞서 일반적인 윈도우 환경보단 WSL을 활용한 Ubuntu환경에서 진행하겠습니다. 제가 맥북을 갖고 있지만 이전에 맥북에서 잘 돌아갔는데 무슨 이유에서인지 맥북상의 elasticsearch가 자꾸 동작을 못하네요. 결국 윈도우의 WSL환경으로 진행합니다.WSL 2 설치 및 환경 세팅은 아래 블로그를 참고했기 때문에 링크 남겨둡니다.윈도우10에 우분투 설치하는 방법WSL 세팅이 끝나고 우분투도 잘 설정이 되었다면 우분투 서버를 실행하면 아래 사진처럼 나옵니다.이제 엘라스틱서치를 설치하기 전에 엘라스틱 서치는 자바 기반으로 구동되기 때문에 자바를 설치해줘야 합니다. 아래의 두 명령어를 쳐줍시다.$ sudo apt update &amp;amp;&amp;amp; sudo apt upgrade$ sudo apt install deafult-jdk아마 조금 오래된 글이나 강의에서는 자바 8버전이 필요하다고 말이 나옵니다. 하지만 자바 8이 2019년 이후로 지원을 종료했기때문에 다음 LTS버전인 자바 11의 지원여부가 문제였습니다. 찾아본 결과 자바 11으로도 가능하게 업데이트가 이미 되었다고 합니다. 문제 없이 위의 코드로 진행하여 자바 11으로 설치하셔도 괜찮습니다.Support for Java 11$ java --versionopenjdk 11.0.11 2021-04-20OpenJDK Runtime Environment (build 11.0.11+9-Ubuntu-0ubuntu2.20.04)OpenJDK 64-Bit Server VM (build 11.0.11+9-Ubuntu-0ubuntu2.20.04, mixed mode, sharing)$ javac --versionjavac 11.0.11위의 쉘 코드를 작성했을때 코드 바로 아래에 똑같이 출력이 나타나면 설치가 완료된 것입니다. 보시는 시기에 따라 자바 버전은 변경될 수도 있습니다.이제 본격적으로 elasticsearch, kibana, logstash, filebeats를 설치해봅시다아래 링크는 엘라스틱서치 다운로드 사이트입니다. 아래로 들어가서 직접 파일을 다운 받지 않고 링크를 통해 다운 받을 겁니다.Download Elasticsearch사진처럼 DEB X86 64를 우클릭해서 링크 주소 복사를 해줍시다. 이건 각 OS와 아키텍쳐에 맞춰서 선택해주셔야 합니다. 만약 본인이 ARM기반의 노트북이면 옆에 있는 AARCH로 설치하셔야합니다. 그 후 아래의 코드를 우분투 쉘에 입력해주세요. 참고로 링크 복붙은 마우스 우클릭하시면 붙여넣기가 됩니다.$ wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.13.3-amd64.deb동일한 방식으로 키바나, 로그 스태시, 파일 비츠도 설치합시다. 키바나, 로그 스태시, 비츠는 위의 사이트에서 상단의 제품으로 가시면 아래 처럼 ELK stack부분이 나타납니다. 해당 부분을 클릭하시면 설치가 가능합니다.제품의 ELASTIC (ELK) STACK을 누르면우측 상단에 ELK Stack이 모두 나옵니다. logstash, kibana는 동일 방식으로 설치합니다.파일 비츠의 경우 좀 더 찾아야 합니다. 설치 방법은 동일합니다. 하지만 Beats로 들어가서 아래의 다운로드를 누르면 아래처럼 다양한 비츠들이 나옵니다. 사실 파일 비츠는 비츠의 일종이기 때문에 여기서 Filebeat만 설치를 합니다.위의 사진에서 처럼 FIlebeat를 설치하시면 동일하게 진행이 가능합니다.이제 설치가 모두 끝났다면 제대로 돌아가는지 확인 해야겠죠?이번엔 우분투 쉘에서 엘라스틱 서치와 키바나를 실행해봅시다.$ sudo service elasticsearch start$ sudo service kibana start엘라스틱 서치가 정상적으로 돌아가는지 확인하는 방법은 2가지가 있습니다. 쉘을 통한 방법과 웹을 통한 방법입니다.첫번째로 쉘을 통해 확인하는 방법은 쉘에 아래 코드를 입력해봅니다.$ curl -X GET localhost:9200엘라스틱서치는 일반적으로 9200 포트에, 키바나는 5601포트에 열리게 됩니다. 위의 코드로 9200포트의 접속 값을 GET방식을 통해 가져왔을 때 아래처럼 나타나면 엘라스틱서치 정상적으로 동작하는 것입니다.두번째 방법은 웹 페이지 링크에 localhost:9200을 입력해서 위의 사진과 유사하게 나타나면 정상적으로 실행되고 있는겁니다.이제 키바나가 정상적으로 동작하는지 확인해봅시다.이번엔 웹페이지에 localhost:5601로 접속해봅시다. 아래처럼 어떤 사이트가 나타나면 키바나가 정상적으로 동작하고 있는겁니다.이렇게 해서 ELK를 대략적으로 알아봤고 ELK Stack의 설치까지 진행해봤습니다.본격적인 ELK 활용은 다음 시간부터 해보겠습니다!" }, { "title": "[BoostCamp AI Tech] Day8", "url": "/posts/day8/", "categories": "NAVER BoostCamp AI Tech, Review", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech", "date": "2022-01-26 23:40:00 +0900", "snippet": "Day8 Review당신은 오늘 하루 어떻게 살았나요? PyTorch Day3 nn.Module 일부 분석오늘의 피어세션 코드리뷰에서 Parameter변수에 있는 data attribute를 직접 연산하면 왜 autograd가 적용되지 않을까?오늘 하지 못한 것들 심화과제내일은 어떤 것을 할까? 심화과제 torch.nn.Module 정리마무리 좀 더 정리를 체계적으로 할 필요가 있는거 같은데.. 일단 nn.Module 분석이나 하자" }, { "title": "[BoostCamp AI Tech / PyTorch] Day8 - Monitoring tools for PyTorch", "url": "/posts/day8_torch6/", "categories": "NAVER BoostCamp AI Tech, Level 1 - Python/PyTorch", "tags": "NAVER, BoostCamp, AI Tech, Python, Basic, PyTorch, tensorboard, WandB, monitoring", "date": "2022-01-26 19:00:00 +0900", "snippet": "PyTorch : Monitoring tools for PyTorchMachine Learning Monitoring Tools 머신러닝 모델 학습의 모니터링 툴은 다양하게 존재한다. 대표적인 것이 Tensorboard와 weight &amp;amp; biases이다.Tensorboard 원래는 TensorFlow의 프로젝트로 만들어진 시각화 도구이다. 학습 그래프, metric과 같은 다양한 수치들에 대해 시각화를 지원해준다. TensorFlow project의 일환이었으나 이제는 다양한 DL 프레임워크들이 연결해서 사용하고 있다. 물론 PyTorch도 마찬가지다. Tensorboard 값 scalar : metric 등 상수값의 epoch를 표시 graph : 모델의 computational graph vytl histogram Image Tensorboard in Colabfrom torch.utils.tensorboard import SummaryWriterimport numpy as npPATH = &quot;project&quot;exp = f&quot;{PATH}/ex1&quot;writer = SummaryWriter(exp)for n_iter in range(100): writer.add_scalar(&#39;Loss/train&#39;, np.random.random(), n_iter) writer.add_scalar(&#39;Loss/test&#39;, np.random.random(), n_iter) writer.add_scalar(&#39;Accuracy/train&#39;, np.random.random(), n_iter) writer.add_scalar(&#39;Accuracy/test&#39;, np.random.random(), n_iter)writer.flush() Tensorboard 기본 세팅이다. 보통 로그를 기록하는 파일 양식은 project이름/실험명의 형식으로 저장한다. SummaryWriter 객체는 기록을 진행하는 객체이다. 이 객체에 scalar등 위에 언급한 값들을 입력할 수 있다. writer.add_scalar()는 scalar값을 기록하는 역할을 한다. 파라미터에서 n_iter는 x축에 들어가는 값을 의미한다. 코랩에서 텐서보드를 실행하기 위해서는 매직커맨드를 활용해야한다. 참고로 safari에서는 잘 안된다. 크롬을 권장한다. %load_ext tensorboard % tensorboard --logdir {logs_base_dir} 위의 코드를 실행하면 코랩 자체에서도 텐서보드 확인이 가능하고 포트상으로는 6006포트에 활성화된다. Weight and BiasesWeight and Biases 머신러닝 실험에서 자주 활용되는 도구이다. 다른 사람들과 실험을 라이브로 공유하기에 최적화된 도구이다. MLOps의 대표적인 툴로 자주 사용되는 중이다.WandB in Colab!pip instal wandb -qimport wandbwandb.init(project=&quot;my_project_name&quot;, entity=&quot;user_name&quot;) 위처럼 연결하면 wandb에 연결된다. 솔직히 코랩연결보단 그냥 페이지에서 보는게 나은 것 같다.EPOCHS = 100BATCH_SIZE = 64LEARNING_RATE = 0.001config={&quot;epochs&quot;: EPOCHS, &quot;batch_size&quot;: BATCH_SIZE, &quot;learning_rate&quot; : LEARNING_RATE}wandb.init(project=&quot;my_project_name&quot;, config=config)# wandb.config.batch_size = BATCH_SIZE# wandb.config.learning_rate = LEARNING_RATE# config={&quot;epochs&quot;: EPOCHS, &quot;batch_size&quot;: BATCH_SIZE, &quot;learning_rate&quot; : LEARNING_RATE}for e in range(1, EPOCHS+1): &#39;&#39;&#39; your train valid code &#39;&#39;&#39; train_loss = epoch_loss/len(train_dataset) train_acc = epoch_acc/len(train_dataset) print(f&#39;Epoch {e+0:03}: | Loss: {train_loss:.5f} | Acc: {train_acc:.3f}&#39;) wandb.log({&#39;accuracy&#39;: train_acc, &#39;loss&#39;: train_loss}) 자신의 훈련을 위한 모델의 parameter를 config로 설정해서 전달해준다. 해당 모델의 훈련값이 wandb.log 코드를 통해 wandb에 전달되어 라이브로 기록되는 것을 알 수 있다.PyTorch Lightning Logging PyTorch Lightning에도 모델 학습 기록을 하는 모듈이 존재한다. 물론 기록을 확인하는 것은 tensorboard를 쓰는 것으로 보인다. 앞서 파이토치 라이트닝을 간단히 소개한 적이 있는데, pytorch lightning은 Trainer객체를 활용해서 모델의 학습이 진행된다.그렇기 때문에 deafult 폴더가 아닌 다른 폴더에 기록을 전달하고 싶다면 Trainer에 TensorBoardLogger를 파라미터로 전달해 줄 필요가 있다.TensorBoardLogger말고 다른 로거들을 사용할 수도 있다.from pytorch_lightning import Trainer# Automatically logs to a directory# (by default ``lightning_logs/``)trainer = Trainer()# To see your logs# tensorboard --logdir=lightning_logs/ # If you wand custom loggerfrom pytorch_lightning import loggers as pl_loggerstb_logger = pl_loggers.TensorBoardLogger(&quot;logs/&quot;)trainer = Trainer(logger=tb_logger)Automatic Logging log()를 사용하면 라이트니의 자동 로그 기능을 사용할 수 있다. training_step의 구현부에 추가로 log를 작성해주면 되는 것으로 보인다. log의 파라미터를 조정해서 매뉴얼로 조작이 가능하다. one_step : 현재 단계의 metric을 기록 training_step()의 deafult는 True on_epoch : epoch 종료시 자동으로 누적해서 기록 prog_bar : 훈련의 진행률을 표시해준다. logger : Trainer에 전달한 사용자 정의 로거에 기록할지 여부를 결정한다. def training_step(self, batch, batch_idx): self.log(&quot;my_metric&quot;, x)# or a dictdef training_step(self, batch, batch_idx): self.log(&quot;performance&quot;, {&quot;acc&quot;: acc, &quot;recall&quot;: recall})# manual loggerdef training_step(self, batch, batch_idx): self.log(&quot;my_loss&quot;, loss, on_step=True, on_epoch=True, prog_bar=True, logger=True) log를 이용해서 기록할 수도 있지만 직접 기록할 로그를 설정할 수 있다.def training_step(self): ... # the logger you used (in this case tensorboard) tensorboard = self.logger.experiment tensorboard.add_image() tensorboard.add_histogram(...) tensorboard.add_figure(...) 이 외에 자세한 사용법은 공식문서를 통해 확인할 수 있다." }, { "title": "[BoostCamp AI Tech / PyTorch] Day8 - 모델 불러오기", "url": "/posts/day8_torch5/", "categories": "NAVER BoostCamp AI Tech, Level 1 - Python/PyTorch", "tags": "NAVER, BoostCamp, AI Tech, Python, Basic, PyTorch", "date": "2022-01-26 16:00:00 +0900", "snippet": "PyTorch : 모델 불러오기Introduction 최근 추세는 pre-trained 모델을 데이터에 맞추는 fine-tuning을 진행한다. 또한 모델의 학습 결과를 공유하고 저장할 필요가 있다.모델 저장 및 불러오기1. model.save 학습의 결과를 저장하기 위한 함수이다. 모델과 파라미터를 저장한다. Earky Stopping이나 중간중간 모델의 최적 결과를 위해 최선의 결과모델을 선택한다. 모델을 외부로 공유해서 학습의 재연성을 향상할 수 있다.import torchimport ostorch.save(model.state_dict(), os.path.join(MODEL_PATH, &quot;model.pt&quot;))new_model = NewModel()new_model.load_state_dict(torch.load(os.path.join(MODEL_PATH, &quot;model.pt&quot;)))torch.save(model, os.path.join(MODEL_PATH, &quot;model.pt&quot;))model = torch.load(os.path.join(MODEL_PATH, &quot;model.pt&quot;)) state_dict : 모델의 파라미터를 표시해주는 역할 모델 파라미터를 불러와서 모델에 저장하는 역할 model.pt : 과거에는 pth의 형식으로 저장을 많이 했으나 요즘은 pt 형식으로 저장한다. 같은 모델에서 파라미터만 load해서 사용하는 경우가 많다.2. checkpoints 학습 중간에 결과를 저장해서 최선의 결과를 선택한다. earlystopping과 같은 기법으로 학습 결과를 저장한다. epoch, loss외 metric을 함께 저장해서 확인하는 경우가 많다.torch.save({ &#39;epoch&#39;: e, &#39;model_state_dict&#39;: model.state_dict(), &#39;optimizer_state_dict&#39;: optimizer.state_dict(), &#39;loss&#39;: epoch_loss}, f&quot;saved/model_name.pt&quot;)checkpoint = torch.load(PATH)model.load_state_dict(checkpoint[&#39;model_sate_dict&#39;])optimizer.load_state_dict(checkpoint[&#39;optimizer_state_dict&#39;])epoch = checkpoint[&#39;epoch&#39;]loss = checkpoint[&#39;loss&#39;]Pre-trained model, Transfer learning 최근 CV와 NLP 분야에서는 Bert, ImageNet과 같은 Pre-trained model을 사용하는 추세이다. 이 모델들은 다량의 데이터로 학습을 시킨 모델들이다. pre-trained model에 목적을 위한 데이터를 fine-tuning을 하는 방식을 많이 사용한다. backbone architecture가 잘 학습된 모델에서 일부만 재학습하는 방식을 수행한다. CV는 TrochVision을 사용하고 NLP는 HuggingFace가 표준이다. 나도 Bert때문에 huggingface에 들어가본 기억이 있다. Frezzing사진출처 : SpotTune: Transfer Learnong through Adaptive Fine-tuning pretrained model 활용시 다른 task를 학습시키기 때문에 일부의 레이어를 고정시키고 남은 레이어만 학습을 진행한다.import torchvision.models as modelsimport torchimport torch.nn as nnclass NewNN(nn.Module): def __init__(self): super(NewNN, self).__init__() self.vgg19 = models.vgg19(pretrained=True) # pre-trained load self.fc = nn.Linear(1000, 1) # linear layer 추가 def forward(self, x): x = self.vgg19(x) return self.fc(x)model = NewNN()for param in model.parameters(): param.requires_grad = Falsefor param in model.fc.parameters(): param.requires_grad = True torchvision의 vgg19에 새로운 linear layer를 추가하는 코드이다. 하단부에 parameter를 처리하는 부분에서 모든 레이어를 일단 require_grad를 False로 하고 linear layer인 fc의 파라미터의 require_grad를 True`로 바꾼다." }, { "title": "[BoostCamp AI Tech] Day7", "url": "/posts/day7/", "categories": "NAVER BoostCamp AI Tech, Review", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech", "date": "2022-01-25 23:40:00 +0900", "snippet": "Day7 Review당신은 오늘 하루 어떻게 살았나요? PyTorch Day2 Day7 내용 블로그 정리 기본과제 all clear오늘의 피어세션 팀원들과 스터디 내용 결정 매주 주제를 정하고 심화 포스팅 오늘 하지 못한 것들 없음~ all clear내일은 어떤 것을 할까? torch.nn.Module 분석계획 및 일부 분석 심화과제…?마무리 배워보고 싶던 파이토치!! 열심히 뜯어먹어서 갈아 마셔야겠다!" }, { "title": "[BoostCamp AI Tech / PyTorch] Day7 - Datasets &amp; Dataloaders", "url": "/posts/day7_torch4/", "categories": "NAVER BoostCamp AI Tech, Level 1 - Python/PyTorch", "tags": "NAVER, BoostCamp, AI Tech, Python, Basic, PyTorch", "date": "2022-01-25 19:00:00 +0900", "snippet": "PyTorch : Datasets &amp;amp; DataloadersPyTorch Data 구조1. Data 데이터는 데이터 그 자체를 의미한다. 정제되거나 전처리, 누적이 이미 다 되어 들어오는 경우도 많지만 아닌 경우도 있다. 보통 데이터 엔지니어 통해서 데이터가 정리가 되어 오는 경우가 많다.2. Dataset Dataset class는 이미 구현되어 있는 것을 상속에서 세팅하는 경우가 많다. __init__(), __len()__, __getitem()__과 같이 필수적으로 생성해줘야하는 것들이 존재한다. __init__() : 데이터를 로드하는 방법을 세팅하는 부분이다. 일반적으로 X, y를 지정하거나 다른 메서드들에 사용하는 attribute들을 세팅해주는 경우가 많다. __len()__ : 흔히 어떤 데이터셋을 len(dataset)으로 처리했을 때 어떤 값을 반환할 지를 구현하는 부분이다. __getitem()__ : 파라미터에 self와 함께 idx가 들어오는 경우가 일반적이다. 이 매직메서드는 우리가 일반적으로 []으로 데이터에 접근할 때 어떻게 반환하는 가를 구현하는 부분이다. map-style이라고도 한다. transforms transforms는 전처리하는 부분을 다루고 있다. dataloader에 transform 파라미터가 존재하는데 이 부분에 들어갈 것들을 처리한다. ToTensor(), CenterCrop() 같은 것들이 들어간다. 보통 실제 구현하는 경우는 잘 없다…DataLoader 우리가 구현한 모델에 data feeding을 하는 부분이다. batch단위로 분리해주고 data shuffle, sampling 등을 진행해서 model에 데이터를 제공한다.Dataset Class 앞서 간단히 말했지만 데이터의 입력 형태를 결정하는 클래스이다. 데이터의 종류에 따라 입력을 다르게 정의해야한다. 이미지, 텍스트, 오디오등 데이터의 형식이 가장 중요한 포인트이다. import torchfrom torch.utils.data import Datasetclass MyDataset(Dataset): def __init__(self, data, labels): self.X = data self.y = labels def __len__(self): return len(self.y) def __getitem__(self, idx): label = self.y[idx] data = self.X[idx] return data, label 이때 __getitem__의 형식도 자유자재로 데이터에 맞춰 결정해주면 좋다. classification 문제는 dict type으로 반환하는 경우가 많다. 모든 것을 데이터 생성시점에 처리할 필요는 없다. 데이터를 한번에 불러오면 그 자체가 무거워질 수 있기 때문이다. image data의 경우 Tensor변화는 학습에 필요한 시점에 변환한다. 팀원들과의 협업에서 데이터 셋의 표준 처리방법을 제공 및 공유하는 것이 좋다.DataLoade classDataLoader(dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=0, collate_fn=None, pin_memory=False, drop_last=False, timeout=0, worker_init_fn=None, multiprocessing_context=None, generator=None, *, prefetch_factor=2, persistent_workers=False) DataLoader는 여러 데이터를 묶어서 model에 feed하는 역할을 한다. Data의 batch를 생성하는 역할을 한다. 학습직전 (GPU feed전) 데이터의 변환을 책임진다. 주로 tensor변환, batch 처리가 메인 업무이다. 주목할 파라미터 sampler : data 추출기법을 설정하는 부분이다. collate_fn : variable length(가변길이)의 데이터를 batch 단위로 처리할 때 남은 데이터에 대해 padding을 설정해준다. 보통 sequence data에 많이 사용한다. data = [&#39;apple&#39;, &#39;dog&#39;, &#39;cat&#39;, &#39;banana&#39;, &#39;tomato&#39;]labels = [1, 0, 0, 1, 1]MyData = MyDataset(data, labels)MyDataLoader = DataLoader(MyData, batch_size=2, shuffle=True)next(iter(MyDataLoader))# [(&#39;apple&#39;, &#39;tomato&#39;), tensor([1, 1])]MyDataLoader = DataLoader(MyData, batch_size=2, shuffle=True)for dataset in MyDataLoader: print(dataset)&#39;&#39;&#39;[(&#39;dog&#39;, &#39;apple&#39;), tensor([0, 1])][(&#39;cat&#39;, &#39;banana&#39;), tensor([0, 1])][(&#39;tomato&#39;,), tensor([1])]&#39;&#39;&#39;" }, { "title": "[BoostCamp AI Tech / PyTorch] Day7 - AutoGrad &amp; Optimizer", "url": "/posts/day7_torch3/", "categories": "NAVER BoostCamp AI Tech, Level 1 - Python/PyTorch", "tags": "NAVER, BoostCamp, AI Tech, Python, Basic, PyTorch", "date": "2022-01-25 14:00:00 +0900", "snippet": "PyTorch : AutoGrad &amp;amp; OptimizerIntroduction 다양한 논문들이 제공하고 있는 모델들은 레이어들의 연속체이다. 이런 레이어들은 일종의 블록이랑 같다고 생각하면 편하다. ResNet이나 transformer처럼 여러개의 레이어를 합쳐서 큰 규모의 레이어를 구성할 수도 있다. 이렇게 모델을 구현할 때 pytorch의 torch.nn.Module을 사용한다.Model 만들기torch.nn.Module 딥러닝을 구성하는 Layer의 base class이다. input, output은 선택적으로 정의하는 경우가 많지만 forward, backward는 직접 정의하는 경우가 많다. backward에서 AutoGrad를 통해 weight에 대한 미분을 진행한다. 학습의 대상이 되는 weight나 bias같은 파라미터들도 정의한다. 모델에서 사용한 그래프는 forwar에서는 연산을 위한 공식에 입력값들을 적용시킨다. backward에서는 loss 값을 계산하여 각 parameter에 맞춰 편미분을 진행해서 parameter update를 진행한다.nn.Parameter Tensor 객체의 상속 객체이다. 실제로 torch.nn.Parameter의 소스코드 링크에 들어가면 torch.Tensor를 상속하고 있다. 일반적으로 nn.Module내에서 attribute가 되면 required_grad=True로 지정되어 학습의 대상이 되는 Tensor이다. 직접 Parameter를 지정할 일은 잘 없다. Linear, Conv같은 구현된 레이어들에 이미 Parameter로 정의된 변수들이 있기 때문에 직접 정의할 일은 없다.class MyLinear(nn.Module): def __init__(self, in_features, out_features, bias=True): super().__init__() self.in_features = in_features self.out_features = out_features self.weights = nn.Parameter( torch.randn(in_features, out_features) ) self.bias = nn.Parameter(torch.randn(out_features)) def forward(self, x : Tensor): return x @ self.weights + self.biasx = torch.randn(3, 4)fc = MyLinear(4, 12)fc(x)&#39;&#39;&#39;tensor([[ 1.0335, -2.9261, 0.1745, 3.4936, -0.1151, 2.7729, 1.3236, -0.8619, -2.3663, 1.0498, -3.7553, 2.7351], [ 1.9231, -0.9690, 0.5683, 2.5309, 2.3729, 0.3830, -3.4628, 1.4042, -3.5674, -3.6533, -0.6249, 1.2501], [ 4.5484, -1.5726, 5.9213, 0.3171, -1.4282, 0.9741, -2.0159, -1.4090, -2.4930, -1.4857, -1.5419, -0.2907]], grad_fn=&amp;lt;AddBackward0&amp;gt;)&#39;&#39;&#39; 이 코드는 간단한 선형식인 $XW+b$를 구현한 모델이다. 여기서 nn.Parameter의 역할은 torch.Tensor와 출력자체는 동일하게 나타난다. print(torch.nn.Parameter(torch.Tensor([1, 2, 3]))) print(torch.Tensor([1, 2, 3])) &#39;&#39;&#39; Parameter containing: tensor([1., 2., 3.], requires_grad=True) tensor([1., 2., 3.]) &#39;&#39;&#39; 근데 우리가 구현한 모델의 메소드 중 parameters()라는 메소드가 있는데, 이 메소드를 수행하면 Parameter로 선언된 변수의 학습된 파라미터들 값을 볼 수 있다. 하지만 Tensor로 구현하면 torch.nn.Module에 있는 파라미터 리스트에 등록되지 않아 확인할 수 없다. 또 한가지 특이한 점은 단순히 객체를 호출했는데 값을 보내주는 forward가 수행된 것을 알 수 있다. 이는 torch.nn.Module의 내부 코드원리 때문인데 nn.Module의 공식문서에서 _call_impl부분을 보면 self.forward를 호출하는 것을 알 수 있다. 참고로 _call_imp은 함수 호출시 동작하는 함수인 __call__의 구현부이다.Backward lyaer에 있는 Parameter들의 미분을 수행한다. forward의 결과인 predict 값($\\hat{y}$)과 실제 값 $y$의 차이인 loss function의 값을 활용하여 미분을 수행한다. 이 과정에서는 AutoGrad를 통해 미분을 진행한다. 계산된 값으로 Parameter의 업데이트를 진행한다.import torchfrom torch.autograd import Variableclass LinearReg(torch.nn.Module): def __init__(self, inputSize, outputSize): super(LinearReg, self).__init__() self.linear = torch.nn.Linear(inputSize, outputSize) def forward(self, x): out = self.linear(x) return outinput_size = 1 # takes variable &#39;x&#39; output_size = 1 # takes variable &#39;y&#39;lr = 0.01 epochs = 100model = LinearReg(input_size, output_size)criterion = torch.nn.MSELoss() optimizer = torch.optim.SGD(model.parameters(), lr=lr)for epoch in range(epochs): inputs = Variable(torch.from_numpy(x_train)) labels = Variable(torch.from_numpy(y_train)) optimizer.zero_grad() outputs = model(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() print(f&#39;epoch {epoch}, loss { loss.item() }&#39;) optimizer.zero_grad()는 기존의 gradient를 학습에 영향이 가지 않게 초기화하는 과정이다. outputs = model(inputs)은 forward가 호출되므로 $\\hat{y}$을 계산하는 과정이다. loss = criterion(outputs, labels)는 오차값을 계산하는 loss function 처리구간이다. 여기서는 Mean Squared Error를 사용했다. loss.backward()와 optimizer.step()부분이 중요하다. loss.backward()는 backward에서 미리 선언한 optimizer로 AutoGrad 과정을 통해 미분을 진행한다. 그리고 optimizer.step()은 weight를 update하는 부분이다. backward도 Module단계에서 직접 지정이 가능한데 잘 그러지는 않는다. 보통 backward와 optimizer는 오버라이딩으로 진행된다. " }, { "title": "[BoostCamp AI Tech / PyTorch] Day6 - PyTorch 프로젝트 구조 이해하기", "url": "/posts/day6_torch2/", "categories": "NAVER BoostCamp AI Tech, Level 1 - Python/PyTorch", "tags": "NAVER, BoostCamp, AI Tech, Python, Basic, PyTorch", "date": "2022-01-24 19:00:00 +0900", "snippet": "PyTorch : PyTorch 프로젝트 구조 이해하기Introduction 많이들 Jupyter notebook을 통해 ML/DL 입문을 하고 코드를 접한다. 하지만 Jupyter notebook에는 한계가 존재한다. 배포 및 공유의 어려움 $\\rightarrow$ 재현의 어려움과 실행순서의 꼬임 이를 해결하고자 Deep Learning 코드를 하나이 프로그램으로 작성할 필요가 있다. 개발의 용이성 확보, 유지보수 향상 앞서 Python의 OOP 개념을 배운 것이 여기써 쓰인다.PyTorch Template 파이토치의 템플릿의 종류는 다양한데 그 중 일부를 소개하겠다. 사용자의 필요에 따라 수정하여 사용한다. train, data_loader, model, config, logging, metric, utils 등 다양한 모듈로 분리하여 프로젝트를 사용한다. 실제로 많은 기업들과 대학원 연구실에서 템플릿을 활용하는 방식을 사용한다.PyTorch Template Projectpytorch-template/│├── train.py - main script to start training├── test.py - evaluation of trained model│├── config.json - holds configuration for training├── parse_config.py - class to handle config file and cli options│├── new_project.py - initialize new project with template files│├── base/ - abstract base classes│ ├── base_data_loader.py│ ├── base_model.py│ └── base_trainer.py│├── data_loader/ - anything about data loading goes here│ └── data_loaders.py│├── data/ - default directory for storing input data│├── model/ - models, losses, and metrics│ ├── model.py│ ├── metric.py│ └── loss.py│├── saved/│ ├── models/ - trained models are saved here│ └── log/ - default logdir for tensorboard and logging output│├── trainer/ - trainers│ └── trainer.py│├── logger/ - module for tensorboard visualization and logging│ ├── visualization.py│ ├── logger.py│ └── logger_config.json│ └── utils/ - small utility functions ├── util.py └── ... PyTorch Template Project는 아마도 많이 사용하는 형태의 템플릿이 아닐까 싶다. train.py, test.py train 및 test가 실제로 실행되는 코드이다. 템플릿의 핵심이 되는 코드들 중 하나이다. config.json, parse_config.py config.json은 모델의 학습 또는 데이터 관련 등 ML 프로젝트의 설정을 담고 있는 파일이다. parse_config.py는 ConfigParser라는 클래스가 작성되어있다. @classmethod def from_args(cls, args, options=&#39;&#39;): &quot;&quot;&quot; Initialize this class from some cli arguments. Used in train, test. &quot;&quot;&quot; for opt in options: args.add_argument(*opt.flags, default=None, type=opt.type) if not isinstance(args, tuple): args = args.parse_args() if args.device is not None: os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = args.device if args.resume is not None: resume = Path(args.resume) cfg_fname = resume.parent / &#39;config.json&#39; else: msg_no_cfg = &quot;Configuration file need to be specified. Add &#39;-c config.json&#39;, for example.&quot; assert args.config is not None, msg_no_cfg resume = None cfg_fname = Path(args.config) config = read_json(cfg_fname) if args.config and resume: # update new config for fine-tuning config.update(read_json(args.config)) # parse custom cli options into dictionary modification = {opt.target : getattr(args, _get_opt_name(opt.flags)) for opt in options} return cls(config, resume, modification) 핵심적인 코드 중 하나인데 입력으로 들어온 argument들을 파싱하여 ConfigParser 객체로 반환하는 역할을 한다. 여기에 같이 구현된 mangling 함수 중 __getitme__이 있는데 def __getitem__(self, name): &quot;&quot;&quot;Access items like ordinary dict.&quot;&quot;&quot; return self.config[name] 이 함수는 ConfigParser 객체에 한해서 []로 딕셔너리 접근을 가능하게 만들어주는 역할을 한다. data_loader/, data/ 이 폴더에는 data_loader.py와 학습을 위한 데이터가 들어있다. data_loader는 모델 학습에서 중요하게 쓰이는데 이 부분의 구현이 된 부분이다. model/ 이 폴더는 우리가 학습에 사용할 모델의 구조가 구현된 부분이다. 물론 모델만 있는 것은 아니지만 대부분 model.py에 새로운 모델을 클래스로 구현해서 train.py나 test.py에 적용하여 모델을 생성한다. trainer/ 가장 핵심이 되는 파일이다. model.py에 존재하는 모델을 사용해서 학습을 본격적으로 진행하는 코드이다. train.py에 있는 main 함수에 다음과 같은 코드가 있다. def main(config): ... trainer = Trainer(model, criterion, metrics, optimizer, config=config, device=device, data_loader=data_loader, valid_data_loader=valid_data_loader, lr_scheduler=lr_scheduler) trainer.train() 여기서 trainer가 trainer.py에 구현된 Trainer클래스의 객체를 사용한다. utils/ 프로젝트에 사용되는 다양한 짜잘짜잘한 함수들을 구현한 파일이 있다. PyTorch LightningPyTorch Lightning PyTorch Lightning은 간단하게 말하면 TensorFlow의 Keras 같은 존재이다. PyTorch의 nn.Module을 상속해서 모델을 생성하는 방식을 많이 사용하지만 Multi-GPU나 TPU, 분산학습 등 복잡한 실험환경에서는 코드가 길어지는 경우가 발생한다.이를 해결하고자 좀 더 간결하고 집중하고자 하는 곳에 더 집중할 수 있게 도움을 주는 것이 PyTorch Lightning의 역할이다. 아마 sci-kit learn에서 model을 만들고 model.fit으로 한방에 학습을 시켜본 기억이 있을 것이다.PyTorch Lightning은 이렇게 코드 자체를 획기적으로 줄여주는 역할을 한다.# ----------------# TRAINING LOOP# ----------------num_epochs = 1for epoch in range(num_epochs): # TRAINING LOOP for train_batch in mnist_train: x, y = train_batch logits = pytorch_model(x) loss = cross_entropy_loss(logits, y) print(&#39;train loss: &#39;, loss.item()) loss.backward() optimizer.step() optimizer.zero_grad() # VALIDATION LOOP with torch.no_grad(): val_loss = [] for val_batch in mnist_val: x, y = val_batch logits = pytorch_model(x) val_loss.append(cross_entropy_loss(logits, y).item()) val_loss = torch.mean(torch.tensor(val_loss)) print(&#39;val_loss: &#39;, val_loss.item())이랬던 코드를# trainmodel = LightningMNISTClassifier()trainer = pl.Trainer()trainer.fit(model)이렇게 만들 수 있다.PyTorch Lightning 써보기 참고링크 Model 구현 class Classifier(pl.LightningModule): def __init__(self): super().__init__() self.model = nn.Sequential( nn.Flatten(), nn.Linear(28 * 28, 64), nn.BatchNorm1d(64), nn.ReLU(inplace=True), nn.Linear(64, 64), nn.BatchNorm1d(64), nn.ReLU(inplace=True), nn.Linear(64, 10) ) def forward(self, x): pass def training_step(self, batch, batch_idx): pass def validation_step(self, batch, batch_idx): pass def test_step(self, batch, batch_idx): pass def configure_optimizers(self): pass PyTorch Lightning을 활용한 모델은 위와 같은 구성으로 구현하면된다. training_step과 configure_optimizers는 반드시 구현해야한다. def __init__(self) 이 부분은 모델 그래프를 구현하는 곳이다. def forward() def forward(self, x): return self.model(x) 모델의 추론 결과를 제공할 때 사용한다. nn.Module처럼 반드시 정의하는 부분은 아니지만 다른 메서드 구현때 편리하므로 구현하는 것이 좋다. def training_step() def training_step(self, batch, batch_idx): x, y = batch logits = self(x) loss = F.cross_entropy(logits, y) return loss training_step은 학습 루프의 body이다. 이 메소드에는 argumetn에 train dataloader가 제동하는 batch와 batch index가 주어지고 이를 활용해 학습 loss를 계산한다. PyTorch Lightning은 CPU, GPU 세팅을 따로 설정하지 않아도 trainer 설정에 맞춰 자동으로 typecasting을 해준다. configure_optimizers def configure_optimizers(self): optimizer = Adam(self.parameters()) return optimizer configure_optimizers는 모델의 최적 파라미터를 찾을 때 사용할 optimizer와 scheduler를 구현한다. 학습모델이 여러개면 리스트로 리턴을 해주면된다. 여기서는 모델이 1개이므로 Adam만 사용했다. 학습 및 테스트pl.seed_everything(args.seed) # 재생산을 위한 랜덤시드 고정# dataloadersdataset = MNIST(&#39;&#39;, train=True, download=True, transform=transforms.ToTensor())train_dataset, val_dataset = random_split(dataset, [55000, 5000])test_dataset = MNIST(&#39;&#39;, train=False, download=True, transform=transforms.ToTensor())train_loader = DataLoader(train_dataset, batch_size=args.batch_size)val_loader = DataLoader(val_dataset, batch_size=args.batch_size)test_loader = DataLoader(test_dataset, batch_size=args.batch_size)# modelmodel = Classifier()# trainingtrainer = pl.Trainer(max_epochs=args.n_epochs, gpus=args.n_gpus)trainer.fit(model, train_loader, val_loader)trainer.test(test_dataloaders=test_loader) Keras에서 MNIST를 학습하고 테스트할 때와 유사한 코드를 보여주고 있다. 위에서 모델을 구현해두면 간단하게 모델을 학습, 테스트 할 수 있는 것을 볼 수 있다." }, { "title": "[BoostCamp AI Tech / PyTorch] Day6 - Introduction to PyTorch and Basics", "url": "/posts/day6_torch1/", "categories": "NAVER BoostCamp AI Tech, Level 1 - Python/PyTorch", "tags": "NAVER, BoostCamp, AI Tech, Python, Basic, PyTorch", "date": "2022-01-24 15:00:00 +0900", "snippet": "PyTorch : Introduction to PyTorch ans Basics딥 러닝 프레임워크1. Tensorflow 딥러닝 프레임워크의 선두주자’였’던 TensorFlow다. ‘였’던 이라는건 지금은 아마 PyTorch로 넘어간 걸로 알고있다. TF의 아버지는 구글이다보니 초기부터 굉장한 밀어주기가 있어서 지금도 많은 소스들이 존재한다. 실제로 2020년에도 점유율은 35.1%정도를 차지하고 있다. 이런 텐서플로우를 보좌해주는 친구가 존재하는데 그게 바로 Keras다. 텐서플로우에 코드를 하나하나 다 적는거보다는 layer나 optimizer같은 것들을 쉽게 조작할 수 있게 도와주는 역할을 케라스가 한다. 소스자체도 많고 관련 오픈소스들이 많다보니 실제 서비스 구축에서 많이 사용된다. Define and Run Tensorflow의 model graph 형성방법은 static graph이다. 즉 모델을 우선 구성하고 학습을 진행하는 것이다. 텐서플로우는 production, cloud, multip GPU에서 강점을 보인다.2. PyTorch 딥러닝 프레임워크계의 신흥강자인 파이토치이다. 파이토치의 아버지도 텐서플로우 못지 않게 빠방한데, 바로 페이스북이다. 텐서플로우와 다르게 내부 코드를 하나하나 동작하면서 학습을 돌릴 수 있기 때문에 연구목적으로 많이 사용된다. Define by Run (Dynamic Computational Graph, DCG) PyTorch는 텐서플로우와 다르게 실행을 하면서 모델 그래프를 그린다. 이런 방식은 디버깅의 편의성을 가져오기 때문에 연구 목적으로 쓰기 가장 훌륭하다. PyTorchimport torch1. Numpy PyTorch에는 Numpy의 ndarray와 유사한 역할을 하는 Tensor가 존재한다. Tensor를 생성하는 방식도 Numpy와 동일하다. import torch import numpy as np # numpy array n_array = np.arange(10).reshape(2, 5) # tensor t_array = torch.FloatTensor(n_array) # array to tensor data = [[3, 5], [10, 5]] x_data = torch.tensor(data) # ndarray to tensor nd_array_ex = np.array(data) tensor_array = torch.from_numpy(nd_array_ex) GPU tensor라는 특수한 자료형이 존재한다. 이는 GPU환경 즉, cuda에서 돌리기 위한 특수 자료형이다. 기본적인 numpy의 연산이 대부분 활용된다. data = [[3, 5, 20], [10, 5, 50], [1, 5, 10]] x_data = torch.tensor(data) x_data.flatten() # tensor([ 3, 5, 20, 10, 5, 50, 1, 5, 10]) torch.ones_like(x_data) # tensor([[1, 1, 1], # [1, 1, 1], # [1, 1, 1]]) x_data.shape # torch.Size([3, 3]) x_data.dtype # torch.int64 pytorch는 GPU에 올려서 사용이 가능하다. if torch.cuda.is_available(): x_data_cuda = x_data.to(&#39;cuda&#39;) else: x_data_cuda = x_data.to(&#39;cpu&#39;) Tensor를 다루는 방법도 numpy와 유사한데 주의해야할 것이 있다. view tensor에는 reshape와 view가 모두 존재하는데 둘의 역할은 둘다 차원을 변환해주는 것은 같다. 하지만 약간의 차이가 존재하는데 reshape는 메모리의 구조와 간련이 있어 reshape 동작과 함께 copy가 발생하지만 view는 copy가 아닌 기존의 데이터를 그대로 사용한다. import torch a = torch.zeros(3, 2) b = a.view(2, 3) a.fill_(1) print(b) # tensor([[1., 1., 1.], # [1., 1., 1.]]) a = torch.zeros(3, 2) b = a.t().reshape(6) a.fill_(1) print(b) # tensor([0., 0., 0., 0., 0., 0.]) squeeze, unsqueezq squeeze는 tensor의 차원을 한단계 낮추는 함수이다. unsqueeze는 반대로 tensor의 차원을 axis에 맞춰 높여주는 역할을 한다. import torch tensor_ex = torch.rand(size=(2, 1, 2)) tensor_ex.squeeze() # tensor([[0.5461, 0.9128], # [0.4809, 0.8499]]) torch.mm, torch.matmul pytorch에도 행렬연산이 가능하다. 이때 행렬곱은 torch.mm을 사용하는 것을 권장한다. torch.dot은 벡터의 내적 연산만을 진행하고 행렬 연산은 지원하지 않는다. torch.matmul도 행렬곱으로 동작하지만 이는 broadcasting이 발생하기 때문에 계산결과의 이해가 어렵다. torch.mm은 broadcasting을 진행하지 않으므로 충분히 이해가되는 계산 연산이 이뤄진다. 2. AutoGrad ML/DL formula nn.functional 모듈을 활용하여 다양한 수식 변환을 지원한다. 간단한 예로는 softmax와 one_hot encoding이 있다. import torch import torch.nn.functional as F tensor = torch.FloatTensor([0.5, 0.7, 0.1]) h_tensor = F.softmax(tensor, dim=0) y = torch.randint(5, (7, 5)) y_label = y.argmax(dim=1) torch.nn.functional.one_hot(y_label) &quot;&quot;&quot; tensor([[1, 0, 0, 0, 0], [1, 0, 0, 0, 0], [0, 0, 0, 0, 1], [0, 0, 1, 0, 0], [1, 0, 0, 0, 0], [0, 1, 0, 0, 0], [1, 0, 0, 0, 0]]) &quot;&quot;&quot; AutoGrad PyTorch의 핵심은 자동민부을 지원한다. 이를 통해 backward 함수에 사용된다. w = torch.tensor(2.0, requires_grad=True) # 미분 대상에는 require_grad=True 설정 y = w**3 z = 12 * y + 2 z.backward() w.grad # tensor(144.) AutoGrad는 편미분도 가능하다. " }, { "title": "[Programmers] 카카오 블라인드 2022 - k진수에서 소수 개수 구하기", "url": "/posts/prg92335/", "categories": "Algorithm, Programmers", "tags": "Algorithm, Programmers, 구현, Kakao", "date": "2022-01-23 00:00:00 +0900", "snippet": "카카오 블라인드 2021 - k진수에서 소수 개수 구하기Kakao Blind Recruit 2022 - k진수에서 소수 개수 구하기문제 Comment문제를 보면 뭔가 조건이 많습니다. 근데 잘 보면 조건은 1개만 확인하면 충분히 풀리는 문제입니다.특별히 코멘트를 할 내용은 없습니다. 바로 들어가봅시다.문제 풀이문제에서 소수와 관련된 판별힌트를 주고 있습니다. 0P0처럼 소수 양쪽에 0이 있는 경우 P0처럼 소수 오른쪽에만 0이 있고 왼쪽에는 아무것도 없는 경우 0P처럼 소수 왼쪽에만 0이 있고 오른쪽에는 아무것도 없는 경우 P처럼 소수 양쪽에 아무것도 없는 경우 단, P는 각 자릿수에 0을 포함하지 않는 소수입니다. 예를 들어, 101은 P가 될 수 없습니다.예를 든 것을 보면 437674를 3진수로 표현한 211020101011애서 위의 조건에 해당하는건 211, 2, 11인데 211은 P0, 2는 0P0, 11은 0P라고 설명하고 있습니다.근데 조금 뭔가 이상하지 않나요? 과연 조건이 저렇게 4개일 이유가 있을까요?사실 소수를 판별하는 조건은 딱 1개입니다. 이전의 0이 나온 후로 0이 나왔을때까지 취합된 숫자가 소수인지 아닌지만 판별하면 끝입니다. 결국 위에서 제시한 조건은 0을 기준으로 끊은 수가 소수인가? 입니다.이제 조건을 파악했으니 구현을 해봅시다.코드#include &amp;lt;string&amp;gt;#include &amp;lt;vector&amp;gt;#include &amp;lt;cstring&amp;gt;#include &amp;lt;algorithm&amp;gt;#include &amp;lt;iostream&amp;gt;#include &amp;lt;cmath&amp;gt;using namespace std;typedef long long ll;bool isPrime(ll num) { if (num == 1) return false; if (num == 2) return true; ll lim = sqrt(num); for (ll i = 2; i &amp;lt;= lim; i++) { if (num % i == 0) return false; } return true;}string changeNumber(int num, int k) { string ret = &quot;&quot;; int rem = 0; while(num != 0) { rem = num % k; num /= k; ret += to_string(rem); } reverse(ret.begin(), ret.end()); return ret;}int solution(int n, int k) { int answer = 0; string number = &quot;&quot;; if (k == 10) number = to_string(n); else number = changeNumber(n, k); string tmp = &quot;&quot;; char curr = &#39;\\0&#39;; for (int i = 0; i &amp;lt; number.size(); i++) { curr = number[i]; if (curr == &#39;0&#39;) { if (tmp == &quot;&quot;) continue; if (isPrime(stoll(tmp))) answer++; tmp = &quot;&quot;; }else tmp += curr; } if (tmp != &quot;&quot;) { if (isPrime(stoll(tmp))) answer++; } return answer;}소수판별시에 에라토스테네스의 체를 쓰시는 경우가 있는데 이 경우 segmentation fault 오류가 발생합니다.근데 제 기억으론 당시에 제가 에라토스를 쓴걸로 기억하는데 왜 안되는지는 모르겠네요…..단순 소수 판별 써도 괜찮습니다. 물론 소수판별시 좀 더 시간을 줄이고자 제곱근까지만 체크하는 것이 가장 좋습니다.또한 워낙 값이 큰 경우가 나올 수 있으므로 자료형에도 주의해야합니다." }, { "title": "[Programmers] 카카오 블라인드 2022 - 신고 결과 받기", "url": "/posts/prg92334/", "categories": "Algorithm, Programmers", "tags": "Algorithm, Programmers, 구현, Kakao", "date": "2022-01-23 00:00:00 +0900", "snippet": "카카오 블라인드 2021 - 신규 아이디 추천Kakao Blind Recruit 2022 - 신고 결과 받기문제 Comment카카오 블라인드 2022 코딩 테스트 문제가 프로그래머스에 공개되었습니다.합격 커트라인은 4.5솔이었다고 알려져있습니다. 아마 1~4번 문제를 풀고 6번 문제의 정확성 테스트 통과가 합격 커트로 판단됩니다.저도 작년에 코테에 참여를 했는데 1~3번 + 6번 0.5를 했는데 4번 문제가 풀이를 진행한 방법과 다르게 일부 테케에서 틀렸다고 나와서 아쉽게 3.5솔을 한 기억이 있네요.어쨌든 1번 문제부터 차근차근 알아봅시다.문자열을 다루고 있는 문제인만큼 파괴력이 쎈 파이썬을 활용했습니다.문제 풀이1번 문제는 항상 그렇게 어렵지는 않습니다. 이번 문제는 무지성 2중포문을 작성하면 id_list가 1000, report가 200000이라 시간초과가 날 위험이 높습니다. 어차피 id_list에 주어진 유저로만 ban list가 생성될 것이기 때문에 각 유저가 몇번의 신고를 받는지를 기록하고 탐색을 진행하며 k이상에 해당하는 유저만으로 구성된 리스트를 생성했습니다. 이후 각 유저들이 신고한 유저의 리스트를 파악하고 밴목록에 올라간 유저만큼 카운팅을 진행하면 됩니다. 코드def solution(id_list, report, k): answer = [] user_list = {} banned_list = {} for _id in id_list: user_list[_id] = [] banned_list[_id] = 0 for rep in report: user1 = rep.split()[0] user2 = rep.split()[1] if user2 not in user_list[user1]: banned_list[user2] += 1 user_list[user1].append(user2) ban_list = [] for id, count in banned_list.items(): if count &amp;gt;= k: ban_list.append(id) for id, lists in user_list.items(): cnt = 0 for ban in ban_list: if ban in lists: cnt += 1 answer.append(cnt) return answer사실 시간적으로는 결국 2중 포문을 활용해서 문제가 발생할 위험이 높습니다.트리를 활용한 방식을 사용하면 탐색 시간을 좀 더 줄일 수 있지 않을까 생각합니다." }, { "title": "[BoostCamp AI Tech] Day5", "url": "/posts/day5/", "categories": "NAVER BoostCamp AI Tech, Review", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech", "date": "2022-01-21 23:40:00 +0900", "snippet": "Day5 Review당신은 오늘 하루 어떻게 살았나요? Backpropagation 심화과제 Day4 내용 블로그 정리 Pandas 강의 듣기오늘의 피어세션 팀원들이랑 수식관련 내용 스페셜 피어세션에서 다른 팀원분이랑 관련 내용이랑 생각을 자세히 말할 수 있어서 좋았음. 확실히 분석팀은 통계적 검증이 필요할 것 모델의 정확성에 의존하는 분석가는 필요가 없음 오늘 하지 못한 것들 Day4 내용 블로그 정리내일은 어떤 것을 할까? 남은 Day4 정리 Backpropagation 심화과제 연구마무리 다음주부터는 파이토치! 더 열정적으로하자!" }, { "title": "[BoostCamp AI Tech / Level 1 - Python/PyTorch] Day5 - Pandas", "url": "/posts/day5_1_pandas/", "categories": "NAVER BoostCamp AI Tech, Level 1 - Python/PyTorch", "tags": "NAVER, BoostCamp, AI Tech, Python, Basic, pandas", "date": "2022-01-21 00:00:00 +0900", "snippet": "Python : Pandas데이터 로드 import pandas as pd 일반적으로 pandas는 read_xxx() 함수를 활용해 데이터를 불러온다.Series 데이터프레임에서 하나의 column vector data object를 series라고 한다. (index, data) 한 쌍으로 이루어진 데이터 객체 astype을 통해 type을 일괄적으로 변경가능 index를 기준으로 series가 생성되므로 값이 index보다 적으면 NaN으로 채워짐DataFrame1. 개념 pd.DataFrame(data, columns) index와 column으로 구성 series를 모아서 만든 data table이다. R에서 존재하는 data.frame을 본 따서 만든 것 dictionary type으로 생성하는 경우가 많음2. indexing loc : index location으로 (index의 이름으로 탐색) iloc : index number example loc s = pd.Series(np.nan, index=[49,48,47,46,1,2,3,4,5]) s.loc[:3] &#39;&#39;&#39; &amp;lt;output&amp;gt; 49 NaN 48 NaN 47 NaN 46 NaN 1 NaN 2 NaN 3 NaN dtype: float64 &#39;&#39;&#39; iloc s.iloc[:3] &#39;&#39;&#39; &amp;lt;output&amp;gt; 49 NaN 48 NaN 47 NaN dtype: float64 &#39;&#39;&#39; 3. transpose 일반적인 행렬의 transpose와 동일4. delete column 데이터프레임에서 특정 컬럼을 제거하는 방법은 2가지가 있다. del df[column] : memory상에서 해당 컬럼을 제거 df.drop(column, axis=1) : df는 유지되므로 다시 할당해줘야 적용 5. to_csv() df.to_csv(filename, index) 일반적으로 많이 데이터프레임을 저장하는 형태이다. index 파라미터는 일반적으로 False로 선언하여 저장한다.Selection, drop1. selection df[colum] : column 1개를 반환 (series type) df[[columns]] : column을 1개 이상 반환 (dataframe type) boolean index와 fancy index도 사용가능 loc[[index], [columnn]] 으로 특정 position data를 가져올 수 있음 iloc[:row, :col] : 특정 행, 열의 position data를 가져올 수 있음2. reset_index reset_index(drop, inplace) drop : 기존 index의 제거 여부 (True, False) inplace : 기존의 df를 변경할지 여부 3. drop df.drop([rows]) df.drop([columns], axis=1)lambda, map, apply1. map example : df.column.map({&quot;male&quot;:0, &quot;female&quot;:1})2. replace example : df.column.replace({&quot;male&quot;:0, &quot;female&quot;:1})3. apply 특정 series 전체에 특정 함수를 적용 내장연산함수(built-in)도 사용가능함4. applymap 데이터프레임 모든 값에 함수를 적용함Built-in function df.describe() df[column].unique() sum, mean … isnull 과 sum을 활용해 결측치의 개수를 확인 가능 df.sort_values(by=[column], ascending=True) df.corr(), df.cov() df[column].value_counts()Groupby split -&amp;gt; apply -&amp;gt; combine 순서로 연산을 진행 df.groupby(기준 columns)[적용 column].연산() group으로 묶인 data를 multi-index인 경우 unstack을 사용하면 matrix형태로 바꿀 수 있음 reset_index를 하면 groupby를 풀어줌 example base dataframe ipl_data = { &quot;Team&quot;:[ &quot;Riders&quot;, &quot;Riders&quot;, &quot;Devils&quot;, &quot;Devils&quot;, &quot;Kings&quot;, &quot;Kings&quot;, &quot;Kings&quot;, &quot;Kings&quot;, &quot;Riders&quot;, &quot;Royals&quot;, &quot;Royals&quot;, &quot;Riders&quot;, ], &quot;Rank&quot;:[1,2,2,3,3,4,1,1,2,4,1,2], &quot;Year&quot;:[2014, 2015, 2014, 2015, 2014, 2015, 2016, 2017, 2016, 2014, 2015, 2017], &quot;Points&quot;:[876, 789, 863, 673, 741, 812, 756, 788, 694, 701, 804, 690] } df = pd.DataFrame(ipl_data) groupby h_index = df.groupby([&quot;Team&quot;, &quot;Year&quot;])[&quot;Points&quot;].sum() h_index unstack() h_index.unstack() &#39;&#39;&#39; &amp;lt;output&amp;gt; Year 2014 2015 2016 2017 Team Devils 863.0 673.0 NaN NaN Kings 741.0 812.0 756.0 788.0 Riders 876.0 789.0 694.0 690.0 Royals 701.0 804.0 NaN NaN &#39;&#39;&#39; groupby를 split형태로 사용하면 key-value 형태로 추출가능 추출된 group정보는 3가지 유형의 apply가 가능 aggregate : 요약통계 (agg) transformation : 정보변환 (transgorm) filteration : 필터링기능 (filter) pivot table df.pivot_table(data, index colums) 특정 구성의 데이터를 재구성하는 방법Merge &amp;amp; concat merge : sql의 join과 유사 concat : 비슷한 방법으로 합침 example pd.merge(df1, df2, how=&quot;method&quot;, on=&quot;col&quot;) pd.concat([df1, df2], ignore_index=True, axis=1/0) " }, { "title": "[BoostCamp AI Tech] Day4", "url": "/posts/day4/", "categories": "NAVER BoostCamp AI Tech, Review", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech", "date": "2022-01-20 23:00:00 +0900", "snippet": "Day4 Review당신은 오늘 하루 어떻게 살았나요? AI Math Chapter 5 ~ 10 MLE 심화과제오늘의 피어세션 팀 강의 리뷰 레퍼런스 정리오늘 하지 못한 것들 Backpropagation 심화과제내일은 어떤 것을 할까? Backpropagation 심화과제 Day4 내용 블로그 정리 Pandas 강의 듣기마무리 수학이 좀 어렵지만 집중하자.내가 얻어갈 것은 모델 구현능력과 MLOps" }, { "title": "[BoostCamp AI Tech / Level 1 - AI Math] Day4 - RNN 첫 걸음", "url": "/posts/day4_6_RNN/", "categories": "NAVER BoostCamp AI Tech, Level 1 - AI Math", "tags": "NAVER, BoostCamp, AI Tech, AI Math, math, Deep Learning, RNN", "date": "2022-01-20 18:00:00 +0900", "snippet": "AI Math : RNN 첫걸음Sequence 데이터1. 개념 소리, 문자열, 주가와 같이 일련의 흐름이 존재하는 데이터 시계열(time series) 데이터는 시간 순서에 따라 나열하므로 sequence data에 속한다. sequence data는 독립동등분포(i.i.d) 가정을 위배하는 경우가 많으므로 순서가 변경되거나 과거 정보의 손실이 발생하면 데이터의 확률분포가 변경된다.2. sequence data handling\\[\\begin{aligned}\\begin{matrix}P(X_1, ..., X_t) &amp;amp;=&amp;amp; P(X_t | X_1, ... X_{t-1})P(X_1, ..., X_{t-1}) \\\\&amp;amp;=&amp;amp; P(X_t | X_1, ... X_{t-1})P(X_{t-1}|X_1, ..., X_{t-2})P(X_1, ..., X_{t-2}) \\\\&amp;amp;=&amp;amp; \\prod_{s=1}^{t}P(X_s | X_{s-1}, ..., X_1)\\end{matrix}\\end{aligned}\\] 기본적인 원리가 이전 시퀀스의 정보를 가지고 앞으로 발생할 데이터의 확률분포를 다루기 위해 조건부확률을 이용할 수 있다는 것이다.\\[\\begin{aligned}X_{t} \\sim P(X_t | X_{t-1}, ..., X_1) \\quad\\quad \\\\X_{t+1} \\sim P(X_{t+1} | X_{t}, X_{t-1}, ..., X_1) \\end{aligned}\\] 시퀀스 데이터는 데이터의 길이가 가변적이기 때문에 모델 자체도 가변길이에 대응할 수 있는 모델로 설계되어야한다. 조건부에 들어가는 데이터의 길이를 조절하는 방식의 모델을 자기회귀(AR)모델이라고 말한다.\\[\\begin{aligned}X_{t} \\sim P(X_t | {\\color{orange}{X_{t-1}, ...}}, X_1) \\quad\\quad \\\\X_{t+1} \\sim P(X_{t+1} | {\\color{orange}{X_{t}, X_{t-1}}}, ..., X_1) \\end{aligned} \\quad \\text{Let } {\\color{orange}{orange}} \\text{ is } \\tau\\] 고정된 길이 $\\tau$ 만큼의 시퀀스만 사용하는 경우를 $AR(\\tau)$ (Autoregressive Model) 자기회귀모델이라 한다.\\[\\begin{aligned}X_{t} \\sim P(X_t | X_{t-1}, H_t) \\quad \\\\X_{t+1} \\sim P(X_{t+1} | X_{t}, H_{t+1}) \\end{aligned} \\quad\\quad H_t = \\text{Net}_{\\theta}(H+{t-1}, X_{t-1})\\] 바로 이전의 정보를 제외한 나머지 정보들을 $H_t$라는 잠재변수로 인코딩해서 활용하는 잠재 AR모델이다.Recurrent Neural Network (RNN)1. RNN 기본원리\\[\\begin{aligned}\\mathbf{H}_t = \\begin{cases}\\sigma(\\mathbf{X}_t\\mathbf{W}^{(1)} + \\mathbf{b}^{(1)}) &amp;amp;&amp;amp;&amp;amp;&amp;amp; \\text{MLP} \\\\\\sigma(\\mathbf{X}_t\\mathbf{W}^{(1)}_{X} + \\mathbf{H}_{t-1}\\mathbf{W}^{(1)}_{H} + \\mathbf{b}^{(1)}) &amp;amp;&amp;amp;&amp;amp;&amp;amp; \\text{RNN}\\end{cases} \\\\\\\\\\mathbf{O} = \\mathbf{HW}^{(2)} + \\mathbf{b}^{(2)} \\quad\\quad\\quad\\quad\\quad\\quad\\quad \\\\ \\end{aligned}\\] 변수 $\\mathbf{H}$ : 잠재변수 $\\sigma$ : 활성화함수 $\\mathbf{W}$ : 가중치행렬 $\\mathbf{b}$ : bias RNN의 기본적인 형태는 MLP와 유사한 모형이다. 단, 기본적인 MLP는 이전 값을 활용할 수 없기에 이전 잠재변수를 활용한다. 즉, 이전 순서의 잠재변수가 현재의 입력으로 들어온다는 것이다. 2. RNN의 역전파 RNN의 역전파는 단순 역전파와는 다른 형태로 진행된다. 근본적으로 편미분을 통해 backpropagation을 하는 것은 같지만 단순 역전파를 하기엔 무리가 있다. RNN의 역전파는 Backpropagation Through Time (BPTT)라고 한다.3. 수식을 통한 BPTT\\[\\begin{aligned}L(x, y, w_h, w_o) = \\sum_{t = 1}^{T}l(y_t, o_t) \\quad\\quad\\quad h_t = f(x_t, h_{t-1}, w_h) \\text{ and } o_t = g(h_t, w_o)\\end{aligned}\\] BPTT는 각 가중치행렬에 들어가는 미분값을 모두 계산하고 한번에 update를 진행한다. 우선 각 시점에 대해 prediction과 label의 차를 계산하는 $l(y_t,o_y)$를 위와 같이 정의한다.\\[\\partial_{w_{h}}L(x, y, w_h, w_o) = \\sum_{t=1}^{T}\\partial_{w_h}l(y_t, o_t) = \\sum_{t=1}^T\\partial_{o_t}l(y_t, o_t)\\partial_{h_t}g(h_t , w_h)[\\partial_{w_h}h_t]\\] $W_h$에 대해 손실함수를 편미분을 진행한다. 이때 왜 $O_t$와 $h_t$에 대해 편미분이 진행되냐면 $w_h$로는 바로 $o_t$를 편미분할 수 없다. 즉 아래와 같은 과정이 발생한다.\\[\\frac{\\partial L}{\\partial w_h} = \\frac{\\partial L}{\\partial o_t}\\frac{\\partial o_t}{\\partial h_t}\\frac{\\partial h_t}{\\partial w_h}\\] 기본적인 gradient descent 원리인 chain rule에 의해 동작하기 때문에 추가적인 편미분 표현이 나타난 것이다. 이후 단순히 여기서만 끝나는 것이 아닌 $h_t$도 $w_h$를 포함한 함수이므로 속미분에 의해 $W_h$에 의해 편미분을 진행해야한다.\\[\\partial_{w_h}h_t = \\partial_{w_h}f(x_t, h_{t-1}, w_h) + \\sum_{i=1}^{t-1}\\left( \\prod_{j=i+1}^{t} \\partial_{h_{j-1}}f(x_j, h_{j-1}, w_h)\\partial_{w_h}f(x_i, h{i-1}, w_h) \\right)\\] 이 과정에서도 $h_t$에 있는 또다른 $h_{t-1}$에 의해 다시 연속적인 속미분이 진행된다. 이런 결과로 계속해서 값이 곱해지는데 이 경우 대부분 가중치는 소수점을 갖는 경우가 많기 때문에 곱연산이 많을수록 0에 수렴하게된다. 이를 기울기 소실 (vanishing gradient)라고 한다.기울기 소실의 해결책 기울기 소실이 발생하는 이유는 너무 과도한 재귀형태의 연산이 발생하기 때문이다. 기울기 소실의 해결책으로는 시퀀스의 길이를 끊어버리는 truncated BPTT를 사용한다. 또한 이를 근본적으로 해결하고자 LST, GRU와 같은 네트워크들이 등장했다." }, { "title": "[BoostCamp AI Tech / Level 1 - AI Math] Day4 - CNN 첫 걸음", "url": "/posts/day4_5_CNN/", "categories": "NAVER BoostCamp AI Tech, Level 1 - AI Math", "tags": "NAVER, BoostCamp, AI Tech, AI Math, math, Deep Learning, CNN", "date": "2022-01-20 17:00:00 +0900", "snippet": "AI Math : CNN 첫걸음Convolution 연산 이해하기1. MLP\\[h_{i} = \\sigma\\left( \\sum_{j=1}^{p} W_{ij}x_{j} \\right)\\] 기존의 다층 신경망(MLP)은 fully connected 구조이기 때문에 각 성분 $h_{i}$에 대응하는 가중치 행 $\\mathbf{W}_{i}$가 필요 이런 이유로 성분의 수가 많아지면 가중치행렬의 크기와 parameter의 크기가 너무 크다는 문제가 발생 2. Convolution\\[h_{i} = \\sigma\\left( \\sum_{j=1}^{k}V_{j}x_{i+j-1} \\right)\\] convolution 연산의 기본원리는 기존의 가중치행렬을 사용하는 것이 아닌 kernel을 활용하는 구조 kernel은 고정된 가중치 행렬 convolution 연산의 수학적 의미는 신호를 커널을 통해 국소적 증폭 또는 감소하여 정보를 필터링 하는 것\\[\\begin{aligned}\\text{continuous} \\quad [f * g](x) = \\int_{\\mathbb{R}}f(z)g(x \\pm z)dz = \\int_{\\mathbb{R}}f(x \\pm z)g(z)dz = [g * f](x) \\\\\\text{discrete} \\quad [f * g](x) = \\sum_{a\\in\\mathbb{Z}^{d}}f(a)g(i \\pm a) = \\sum_{a\\in\\mathbb{Z}^{d}}f(i \\pm a)g(a) = [g * f](i) \\quad\\quad\\end{aligned}\\] CNN에서 사용되는 연산은 convolution이라 하지 않고 cross-correlation이라 한다. 단순히 1차원에서만 가능한 연산이 아니고 다차원에서도 가능한 연산이다. 데이터 종류에 따른 convolution 연산 음성/text : 1-D 영상 : 2-D, 3-D 2차원 Convoluton 이해하기\\[\\text{2-D Conv} \\quad\\quad [f * g](i, j) = \\sum_{p, q}f(p, q)g(i+p, j+q)\\] 단순히 수식만으로는 convolution 연산을 이해하기는 어렵다. 간단한 예제를 통해 2차원 convolution을 이해해보자. 커널은 항상 동일하게 주어진다. 이후 입력값에 대해 커널을 겹쳐서 각 행렬값을 element-wise 방식으로 연산한다. 간단히 말하면 입력벡터 위를 커널이 지정된 step-wise에 맞춰 이동하는 것이다, 이렇게 연산한 방식으로 나온 결과는 다음과 같다. 여기서 step-wise가 1이라면 다음은 [1, 2, 4, 5 ] 와 커널의 element-wise 값을 계산한다. convolution 연산에서 가장 중요한 것은 입력의 크기와 커널의 크기를 통해 출력의 크기를 계산하는 것이다. 이를 알아야 데이터의 축소, 확장을 자유자재로 연산할 수 있기 때문이다.\\[\\begin{aligned}O_{H} = H - K_{H} + 1 \\; \\\\ O_{W} = W - K_{W} + 1\\end{aligned}\\] 예를 들어 입력이 28X28이고 커널이 3X3이라면 2D-Conv 연산을 진행하면 26X26이 된다. 채널이 여러개인 2차원 입력인 경우 2차원 convolution을 채널 개수만큼 적용한다.\\[\\sum_{i=1}^{n}{K_{i} \\text{Input}_{i}} \\quad\\quad K_{i} : \\text{i-th Kernel}\\] 위의 연산은 좀 더 단순하게 보면 직육면체 블록같은 텐서의 커널와 텐서의 입력으로 convolution 연산을 하는 것과 같다.Convolution 역전파\\[\\begin{aligned}\\begin{matrix} \\frac{\\partial}{\\partial x}[f * g](x) &amp;amp;=&amp;amp; \\frac{\\partial}{\\partial x}\\int_{\\mathbb{R}^{d}}f(y)g(x-y) dy \\\\ &amp;amp;=&amp;amp; \\int_{\\mathbb{R}^{d}}f(y)\\frac{\\partial g}{\\partial x}(x - y)dy \\\\ &amp;amp;=&amp;amp; [f * g&#39;](x)\\end{matrix}\\end{aligned}\\] Convolution 연산은 커널이 모든 입력 데이터에 공통으로 적용되기 때문에 역전파를 계산할 때도 convolution 연산이 나오게 된다. 역시나 수식 자체보다는 그림으로 이해하는게 편하다.\\[o_i = \\sum_{j}w_{j}x_{i+j-1}\\] CNN의 기본 convolution 연산은 커널의 이동에 기반한 element-wise 값들의 연산이다.결국 backpropagation을 위해서는 각 출력에 들어온 특정 입력($x_i$)과 특정 가중치 ($W_i$)를 활용해야한다. backpropagation의 기본원리는 출력에 들어온 값을 활용해서 미분을 통해 가중치를 갱신하는 것이다. 각 convolution 출력에 맞는 loss의 미분값을 계산했다고 하자. $x_3$은 커널 $W_1$, $W_2$, $W_3$을 한번씩 거치게 된다. 물론 $x_3$과 특정 커널의 곱 결과는 서로 다른 출력을 보이기 때문에 각 출력에 맞는 미분값 $\\sigma$를 각 커널에 곱한 것은 $x_3$이 기여한 결과이다. $\\delta_1 \\color{green}{w_{3}}$ + $\\delta_2 \\color{blue}{w_{2}}$ + $\\delta_3 \\color{red}{w_{1}}$ 이렇게 계산된 미분값을 가중치 업데이트에 활용하는데, 각 가중치에는 각 출력에 알맞은 미분값 $\\delta$와 해당 값을 연산하는데 기여한 $x_i$의 곱연산을 활용하여 업데이트한다. 사실 결과적으로보면 역으로 convolution 연산을 하는 것과 같다. 커널의 gradient update : $(\\color{red}{\\delta_3 x_e}, \\color{blue}{\\delta_2 x_3}, \\color{green}{\\delta_1 x_3})$\\[\\frac{\\partial L}{\\partial w_i} = \\sum_{j}\\delta_{j}x_{i+j-1}\\] 최종적으로 커널의 각 가중치에 사용되는 loss function의 편미분 연산값은 각 커널 가중치를 사용한 모든 convolution 연산에 사용된 입력과, 해당 출력의 미분값의 곱들의 합이 된다. 적고 나서 다시 읽어봤는데 겁나 헷갈린다." }, { "title": "[BoostCamp AI Tech / Level 1 - AI Math] Day4 - 딥러닝 학습방법 이해하기", "url": "/posts/day4_4_deeplearning/", "categories": "NAVER BoostCamp AI Tech, Level 1 - AI Math", "tags": "NAVER, BoostCamp, AI Tech, AI Math, math, Deep Learning", "date": "2022-01-20 16:00:00 +0900", "snippet": "AI Math : 딥러닝 학습방법 이해하기신경망 (Neural Network)\\[\\begin{aligned}\\begin{matrix}\\begin{bmatrix}- &amp;amp; \\mathbf{o}_{1} &amp;amp; - \\\\- &amp;amp; \\mathbf{o}_{2} &amp;amp; - \\\\&amp;amp; \\vdots &amp;amp; \\\\- &amp;amp; \\mathbf{o}_{n} &amp;amp; -\\end{bmatrix}&amp;amp; = &amp;amp;\\begin{bmatrix}- &amp;amp; \\mathbf{x}_{1} &amp;amp; - \\\\- &amp;amp; \\mathbf{x}_{2} &amp;amp; - \\\\&amp;amp; \\vdots &amp;amp; \\\\- &amp;amp; \\mathbf{x}_{n} &amp;amp; -\\end{bmatrix} &amp;amp;\\begin{bmatrix}w_{11} &amp;amp; w_{12} &amp;amp; \\cdots &amp;amp; w_{1p} \\\\w_{21} &amp;amp; w_{22} &amp;amp; \\cdots &amp;amp; w_{2p} \\\\\\vdots &amp;amp; \\vdots &amp;amp; &amp;amp; \\vdots \\\\w_{d1} &amp;amp; w_{d2} &amp;amp; \\cdots &amp;amp; w_{dp}\\end{bmatrix}&amp;amp;+&amp;amp;\\begin{bmatrix}| &amp;amp; | &amp;amp; \\cdots &amp;amp; | \\\\b_{1} &amp;amp; b_{2} &amp;amp; \\cdots &amp;amp; b_{p} \\\\| &amp;amp; | &amp;amp; \\cdots &amp;amp; |\\end{bmatrix} \\\\\\\\\\mathbf{O} &amp;amp;&amp;amp; \\mathbf{X} &amp;amp; \\mathbf{W} &amp;amp;&amp;amp; \\mathbf{b} \\\\(n \\times p) &amp;amp;&amp;amp; (n \\times d) &amp;amp; (d \\times p) &amp;amp;&amp;amp; (n \\times p)\\end{matrix}\\end{aligned}\\] 행렬의 연산원리에 따르면 d차원의 입력이 w를 통해 p차원 출력으로 나오게된다. 이런 형태의 선형모델을 비선형 방식으로 표현하면 신경망의 형태로 표현하면된다. 위의 식은 d개의 변수로 p개의 선형 모델 을 만들어서 p개의 잠재변수를 설명하는 모델이라 생각할 수 있다. 위의 식을 층의 형태로 표현하면 각 $x$들과 $o$들에 연결된 화살표들은 $W_{ij}$라고 생각하면된다.이렇게 나타난 출력 ($O$)를 비선형 함수인 softmax 함수에 넣게되면 신경망의 구성이 완성된다.\\[\\text{softmax}(\\mathbf{o}) = \\left( \\frac{exp(o_{1})}{\\sum_{k=1}^p exp(o_{k})}, \\cdots , \\frac{exp(o_{p})}{\\sum_{k=1}^p exp(o_{k})} \\right)\\] $\\text{softmax}(\\mathbf{o}) = \\text{softmax}(\\mathbf{Wx} + \\mathbf{b})$softmax softmax 함수는 모델의 출력을 확률로 해석하게 변환하는 역할을 함 분류문제를 풀 때 선형모델과 softmax를 결합한 함수로 예측 학습과정에는 softmax로 학습 추론과정에서는 one_hot 함수를 활용 $\\text{onehot}(\\mathbf{o})$ softmaxdef softmax(vec): denumerator = np.exp(vec - np.max(vec, axis=-1, keepdims=True)) numerator = np.sum(denumerator, axis=-1, keepdims=True) val = denumerator / numerator return valvec = np.array([[1, 2, 0], [-1, 0, 1], [-10, 0, 10]])softmax(vec)&quot;&quot;&quot;array([[2.44728471e-01, 6.65240956e-01, 9.00305732e-02], [9.00305732e-02, 2.44728471e-01, 6.65240956e-01], [2.06106005e-09, 4.53978686e-05, 9.99954600e-01]])&quot;&quot;&quot; one_hotdef one_hot(val, dim): return [np.eye(dim)[_] for _ in val]def one_hot_encoding(vec): vec_dim = vec.shape[1] vec_argmax = np.argmax(vec, axis=-1) return one_hot(vec_argmax, vec_dim)def softmax(vec): denumerator = np.exp(vec - np.max(vec, axis=-1, keepdims=True)) numerator = np.sum(denumerator, axis=-1, keepdims=True) val = denumerator / numerator return valvec = np.array([[1, 2, 0], [-1, 0, 1], [-10, 0, 10]])print(one_hot_encoding(vec))print(one_hot_encoding(softmax(vec)))&quot;&quot;&quot;[array([0., 1., 0.]), array([0., 0., 1.]), array([0., 0., 1.])][array([0., 1., 0.]), array([0., 0., 1.]), array([0., 0., 1.])]&quot;&quot;&quot;신경망을 수식으로 분해해보자\\[\\mathbf{H} = (\\sigma(\\mathbf{z}_1), \\cdots, \\sigma(\\mathbf{z}_n)) \\quad \\sigma(\\mathbf{z}) = \\sigma\\left(\\mathbf{Wx} + \\mathbf{b}\\right)\\] 신경망 = 선형모델 + activation functionActivation function $\\sigma$ $\\mathbb{R}$위에 정의된 비선형 함수 딥러닝의 개념에서 가장 중요 활성함수를 활용하지 않으면 선형모형과 차이가 없기 때문에 반드시 사용해야한다. 각 출력값 $\\mathbf{z}$ 에 적용하여 새로운 잠재벡터 $\\mathbf{H}$ 를 생성함 softmax도 활성함수의 일종 sigmoid, tanh는 전통적으로 많이 사용한 활성함수지만 딥러닝에선 ReLU를 많이 쓴다. 활성함수의 종류1-Layer NN 단일층을 활용하는 신경망이다. 각 출력에 대해 비선형 활성함수를 적용한다. $\\mathbf{H}$는 활성함수 출력의 집합이다.2-Layer NN 1개의 층을 더 쌓아올린 신경망이다. 1차로 쌓은 층에서 나온 잠재벡터 $\\mathbf{H}$에서 가중치 행렬 $\\mathbf{W}^{(2)}$와 $\\mathbf{b}^{(2)}$를 통해 선형변환을 진행 파라미터는 $(\\mathbf{W}^{(2)}, \\mathbf{W}^{(1)})$Multi-Layer Perceptro (MLP) MLP의 파라미터는 $L$개의 가중치 행렬 $\\mathbf{W}^{(L)}, …, \\mathbf{W}^{(1)}$로 구성된다. $l = 1, …L$까지 순차적인 신경망의 계산을 순전파 (forward propagation)라고 한다.층을 여러개 쌓는 이유 이론상 2층 신경망으로도 연속함수를 근사하는 것이 가능함 (universal approximation theorem) 층이 싶을수록 목적함수를 근사하는데 필요한 뉴런(노드)으 숫자가 훨씬 빨리 줄어 들어 효율적 학습이 가능 층이 얇으면 뉴런의 수가 기하급수적으로 늘어나서 넓은 (wide) 신경망이 필요역전파 알고리즘 (Back propagation) 딥러닝은 역전파(backpropagation) 알고리즘을 이용하여 각 층에 사용된 파라미터 $ \\{ \\mathbf{W}^{(l)}, \\mathbf{b}^{(l)} \\}^{L}_{l=1} $ 를 학습 경사하강법을 사용해 학습을 진행하는데, 각 가중치의 gradien vector를 계산해서 적용한다. 손실함수는 $\\mathcal{L}$이라 했을때 역전파는 $\\sigma\\mathcal{L}/\\sigma\\mathbf{W}^{(l)}$ 정보를 계산할 때 사용된다. 각 층 parameter의 gradient vector는 윗층부터 역순으로 계산한다. 이전 층에서 계산된 gradient를 밑으로 전달한다. 편미분의 연쇄법칙 원리에의해 이전 층의 gradient값이 필요하다. 역전파 알고리즘 원리 역전파 알고리즘은 합성함수 미분법인 연쇄법칙(chain-rule)기반 자동미분(auto-differentiation)을 사용 각 뉴런에 해당하는 값을 텐서(tensor)라고 한다. 각 텐서는 메모리에 저장되어야 역전파 알고리즘이 동작이 가능하다. 각 노드의 텐서 값을 컴퓨터가 기억해야 미분 계산이 가능하다. " }, { "title": "[BoostCamp AI Tech / Level 1 - AI Math] Day4 - 베이즈 통계학 맛보기", "url": "/posts/day4_3_bayesian/", "categories": "NAVER BoostCamp AI Tech, Level 1 - AI Math", "tags": "NAVER, BoostCamp, AI Tech, AI Math, math, Statistics, Bayesian", "date": "2022-01-20 16:00:00 +0900", "snippet": "AI Math : 베이즈 통계학 맛보기조건부확률\\[\\begin{aligned}P(A \\cap B) = P(B)P(A|B) \\quad\\quad\\quad\\quad\\\\P(B | A) = \\frac{P(A \\cap B)}{P(A)} = P(B)\\frac{P(A \\cap B)}{P(A)}\\end{aligned}\\] 베이즈 정리 : 조건부확률을 이용하여 정보를 갱신한다. A라는 신규정보가 주어지면, $P(B)$로부터 $P(B|A)$를 계산베이즈 정리1. 개념\\[P(\\theta | D) = P(\\theta)\\frac{P(D | \\theta)}{P(D)}\\] $P(\\theta|D)$ : 사후확률 (posterior) data 관찰시 이 가설이 성립할 확률이다. $P(\\theta)$ : 사전확률 (prior) modeling 이전에 주어진 확률 $P(D | \\theta)$ : likelihood $P(D)$ : evidence = 관측상태 Real True False Predict True True Positive (TP) False Positive (FP) False False Negative (FN) True Negative (TN) 민감도 (Recal) = $\\frac{\\text{TP}}{\\text{TP + FN}}$ 특이도 (Specificity) = $\\frac{\\text{TN}}{\\text{TN + FN}}$ 1종오류 : False Positive 실제로는 거짓인데 참이라고 판단하는 경우 1종오류는 당장의 위험성은 적은 오류이다. 예를 들어 암에 걸렸다고 판단했더니 암에 걸리지 않았던 것이다. 물론 병원은 병원비로 고소 좀 먹겠지만 사람 목숨 하나 살아났으니 그나마 다행이다. 2종오류 : False Negative 실제로는 참인데 거짓이라 판단하는 경우 2종오류는 상당히 심각한 오류이다. 예를 들어 암에 걸린 환자가 있는데 암이 아니라고 판단한 것이다. 이 환자는 초기에 암을 발견하여 치료를 하여 충분히 살 수 있었음에도 시간이 흘러 말기암이 되어 발견하게 되어 손 쓸수도 없을 수 있는 것이다. 그래서 의료분야는 2종오류를 최소화하는 것을 목적으로 한다. 2. 예시를 통한 베이즈 정리 이해 COVID-99 발병률이 10%이다. 어떤 진단키트의 성능이 실제 질병에 걸렸을 때 검진확률은 99%, 걸리지 않았을 때 오진할 확률이 1%이다. 만약 환자 A가 양성이 나왔을 때 실제 COVID-99에 걸렸을 확률은?$D$는 covid-99에 걸렸다고 밝혀진 집단, $\\theta$는 실제로 걸린 집단을 생각하면된다.여기서 발병률은 이미 우리에게 주어진 데이터이다. 즉, 사전에 주어진 데이터 ($P(\\theta)$) 이다. 이 정보를 기반으로 우리는 걸렸다고 판단한 집단 중 실제로 걸린 집단일 확률을 업데이트할 것이다.여기서 실제 걸린 경우에 확진이라고 판단하는 경우는 조건부확률로는 $P(D|\\theta)$, 걸리지 않은 환자에 대해 양성이라고 판단하는 경우는 $P(D|\\neg\\theta)$ (위양성율)이다.이때 이 검진키트에 기반한 evidence, 즉 우리가 확인하고자 하는 집단은 다음과 같은 확률을 같게된다.\\[P(D) = \\sum P(D|\\theta)P(\\theta) = 0.99 \\times 0.01 + 0.01 \\times 0.9 = 0.108\\]이를 활용해 베이즈 정리를 통해 이 진단키트가 양성이라고 한 경우 실제 환자가 양성인 확률을 계산하면 다음과 같다.\\[P(\\theta | D) = 0.1 \\times \\frac{0.99}{0.108} \\approx 0.916\\]이때 만약 $P(D)$에서 $P(D | \\neg\\theta)$ 의 값이 증가하면 베이즈 정리의 분모값이 작아지므로 진단키트의 전체 정확도는 현저히 떨어진다. 예를 들어 $P(D | \\neg\\theta)$ 이 10%라면 $P(\\theta| D)$ 의 값은 0.524로 떨어진다.3. 정보의 갱신과정\\[\\begin{aligned}P(\\theta | D) = P(\\theta)\\frac{P(D|\\theta)}{P(D)} \\rightarrow P(\\theta | D&#39;) = P(\\theta)\\frac{P(D&#39;|\\theta)}{P(D&#39;)}\\end{aligned}\\]앞에서 수행한 문제를 가져와보자. $P(D|\\neg\\theta)$ (위양성율)이 10%인 경우 $P(D | \\neg\\theta)$ 는 0.524였다. 이 환자를 한번 더 검사를 했는데 양성이 나왔다. 그렇다면 이 환자가 진짜 covid-99에 걸렸을 확률은 얼마나 될까?의미가 있는 행동일까?라고 생각이 들 수 있으나 이는 의미가 있는 행동이다. 왜냐면 우리에게 주어진 evidence 상황이 바뀌었기 때문이다. 앞선 case는 발병률이 99%인 상황이지만 이제는 기반 Data의 분포가 변경되었기 때문이다. 우리가 확인하는 데이터 분포는 1차 검사에서 양성이 나온 경우로 변경되었다. 즉 $P(\\theta)$ = 0.524가 되었기 때문에 $P(D’)$를 다시 계산해야한다.\\[P(D&#39;) = 0.99 \\times 0.524 + 0.1 \\times 0.476 \\sim 0.566\\]이를 활용해 다시 베이즈 정리에 기반해서 환자가 2차 검사결과에서 진짜 양성인 확률을 구해보자\\[P(\\theta | D) = 0.524 \\times \\frac{0.99}{0.566} \\approx 0.917\\]무려 0.917까지 올라갔다. 이게 수식을 따라가면서 보면 이해가 조금 어려울 수 있다. 좀 더 간단하게 설명하면 한 진단키트가 있을때 이 진단키트로 여러번 검사했을때 양성이 많이 나온다면 우리는 직감적으로 양성이라고 판단하는 것과 비슷하다. (물론 엄밀히 말하면 좀 다르긴한데 대충 느낌상….)" }, { "title": "[BoostCamp AI Tech / Level 1 - AI Math] Day4 - 통계학 맛보기", "url": "/posts/day4_2_statistics/", "categories": "NAVER BoostCamp AI Tech, Level 1 - AI Math", "tags": "NAVER, BoostCamp, AI Tech, AI Math, math, Statistics", "date": "2022-01-20 14:00:00 +0900", "snippet": "AI Math : 통계학 맛보기모수 통계적 모델링이란 가정을 통해 확률분포를 추정하는 것이 목표이다. 하지만 유한한 data로 모집단의 분포를 알아내는 것은 어렵기 때문에 “근사적”추정을 하는데 이때 방법으로는 2가지 방법이 있다. parametric : 선험적으로 분포를 가정하고 모수를 추정한다. non-parametric : 모델구조 + 모수개수를 활용 1. Parametric 확률분포가정 데이터가 2개의 case만 존재 : 베르누이 분포 ($Bernoulli$) n개의 이산 데이터 : 카테고리 분포 (categorical) [0, 1]사이의 값 : 베타분포 ($\\text{Beta}(\\alpha, \\beta)$) 0 이상의 값 : 감마분포 ($\\text{Gamma}$), 로그정규분포 $\\mathbb{R}$ 전체의 값 : 정규분포, 라플라스 분포 모수추정\\[\\begin{aligned} \\bar{X} = \\frac{1}{N}\\sum_{i=1}^{N} x_{i} \\quad,\\quad \\mathbb{E}[\\bar{X}] = \\mu \\quad\\quad\\quad\\\\ s^2 = \\frac{1}{N-1}\\sum_{i=1}^{N}(X_i - \\bar{X})^2 \\quad,\\quad \\mathbb{E}[S^2] =\\sigma^2 \\end{aligned}\\] 표집분포 (Sampling distribution) 통계량들이 존재하는 확률분포이다. sample distribution이랑은 다르다! 주의!! 표집분포는 N이 클수록 정규분포에 근사한다. 최대가능도추정법 (Maximum Likelihood Estimator, MLE)\\[\\begin{aligned}\\hat{\\theta}_{MLE} = \\text{argmax}_{\\theta} L(\\theta ; \\mathbf{x}) = \\text{argmax} P(\\mathbf{x} | \\theta) \\quad\\quad\\quad \\\\L(\\theta ; \\mathbf{x}) = \\prod_{i=1}^{n}P(\\mathbf{x}_{i} | \\theta) \\Rightarrow \\log L(\\theta ; \\mathbf{x}) = \\sum\\log P(\\mathbf{x}_{i} |\\theta)\\end{aligned}\\] 이론적으로 가장 가능성이 높은 모수를 추정 Likelihood에 로그를 연산한 log likelihood를 일반적으로 많이 활용 데이터 규모가 커질경우 계산이 어려워짐 곱셈보다 덧셈이 오차율이 더 적음. 딥러닝의 MLE 가중치 $\\theta = (\\mathbf{W}^{(1)}, …, \\mathbf{W}^{(L)})$ 분류문제에서 softmax는 categorical distribution의 모수를 모델링함 원핫 벡터형태의 정답 레이블 $\\mathbf{y} = (y_1, …, y_k)$ 를 관찰 data로 활용하면 softmax MLE계산 \\[\\hat{\\theta}_{MLE} = \\underset{\\theta}{\\text{argmax}}\\frac{1}{n}\\sum_{i=1}^{n}\\sum_{k=1}^{K}y_{i,k}\\log(MLP_{\\theta}(\\mathbf{x}_{i})_{k})\\] 확률분포의 거리 기계학습의 손실함수들은 model의 학습확률분포와 데이터의 관찰 확률분포의 거리로 유도한다. 거리 계산 함수 총 변동거리 쿨백-라이블러 발산 바슈타인 거리 1. 쿨백-라이블러 발산 (KL Divergence)\\[\\begin{aligned}\\mathbb{KL}(P||Q) = \\sum_{\\mathbf{x}\\in\\chi} P(\\mathbf{x})\\log\\left( \\frac{P(\\mathbf{x})}{Q(\\mathbf{x})} \\right) \\quad,\\quad (\\text{discrete}) \\\\ \\mathbb{KL}(P||Q) = \\int_{\\mathbf{x}} P(\\mathbf{x})\\log\\left( \\frac{P(\\mathbf{x})}{Q(\\mathbf{x})} \\right) \\quad,\\quad (\\text{continuos})\\end{aligned}\\]쿨백-라이블러 발산을 분해할 수 있는데, 이를 분해하면 다음과 같다.\\[\\begin{aligned}\\begin{matrix}\\mathbb{KL}(P || Q) &amp;amp;=&amp;amp; -\\mathbb{E}_{\\mathbf{x} \\sim P(\\mathbf{x})}[\\log Q(\\mathbf{x})] &amp;amp;+&amp;amp; \\mathbb{E}_{\\mathbf{x} \\sim P(\\mathbf{x})}[\\log Q(\\mathbf{x})] \\\\&amp;amp;&amp;amp;\\text{cross entropy}&amp;amp;&amp;amp; \\text{entropy}\\end{matrix}\\end{aligned}\\]여기서 정답레이블을 $P$, 모델의 예측을 $Q$라 두면 MLE는 쿨백-라이블러 발산을 최소화하는 것과 같음" }, { "title": "[BoostCamp AI Tech / Level 1 - AI Math] Day4 - 확률론 맛보기", "url": "/posts/day4_1_probability/", "categories": "NAVER BoostCamp AI Tech, Level 1 - AI Math", "tags": "NAVER, BoostCamp, AI Tech, AI Math, math, Probability theory", "date": "2022-01-20 11:00:00 +0900", "snippet": "AI Math : 확률론 맛보기Introduction 딥러닝은 확률론 기반의 기계학습 이론 회귀분석 : $L_{2}$-norm은 예측오차의 분산을 최소화하는 방향 분류문제 : Cross Entropy는 모델의 불확실성을 최소화하는 방향용어정의1. 데이터 공간 데이터 $(\\mathbf{x}, y) \\sim D$ $D$는 이론적 확률분포2. 확률변수 이산확률변수 (discrete random variable) 확률변수가 가질 수 있는 모든 경우의 수를 합한 것 \\[\\mathbb{P}(\\mathbf{x} \\in \\mathbf{A}) = \\sum_{\\mathbf{x} \\in \\mathbf{A}} P(X = \\mathbf{x})\\] 연속확률변수 (continuous random variable) 확률변수의 밀도 위에서 적분 정확한 확률을 구하는 것은 불가능하기 때문 \\[\\mathbb{P}(\\mathbf{x} \\in \\mathbf{A}) = \\int_{\\mathbf{A}} P({\\mathbf{x}}) d\\mathbf{x}\\]확률분포1. 결합분포 (joint distribution) 각 $X$와 $Y$에 따라 구분하여 동시에 고려한 분포를 형성$\\Rightarrow$ $P(\\mathbf{x}, y)$는 $D$를 모델링함 확률변수가 여러 개일때 함께 고려하는 확률분포2. 주변확률분포 (marginal distribution)\\[P(\\mathbf{x}) = \\sum_{y}P(\\mathbf{x}, y) \\text{ or } \\int_{y}P(\\mathbf{x}, y) dy\\] $\\mathbf{x}$에 대한 정보를 표현한 것 위에 있는 사진의 데이터를 기준으로 설명을 하면 나눠진 9개의 칸을 왼쪽부터 1 ~ 9라고 하면 X가 1일때 점의 개수, 2일때 점의 개수 같은 것을 주변확률분포라고 한다.3. 조건부확률분포\\[P(\\mathbf{x} | y)\\] 입력 $\\mathbf{x}$와 출력 y의 관계를 모델링한다. 특정 클래스 $y$가 주어졌을 때, $\\mathbf{x}$의 확률분포를 의미기대값이란?\\[\\begin{aligned}\\mathbb{E}_{\\mathbf{x} \\sim P(\\mathbf{x})}[f(\\mathbf{x})] = \\int_{\\chi} f(\\mathbf{x})P(\\mathbf{x})d\\mathbf{x} \\quad\\quad\\quad\\quad\\text{(continuous)}\\\\\\mathbb{E}_{\\mathbf{x} \\sim P(\\mathbf{x})}[f(\\mathbf{x})] = \\sum_{\\mathbf{x} \\in \\chi} f(\\mathbf{x})P(\\mathbf{x})d\\mathbf{x} \\quad\\quad\\quad\\quad\\quad\\text{(discrete)}\\end{aligned}\\] $f$에 다른 수식을 넣으면 다양한 통계량을 구할 수 있음 분산 : $\\mathbb{V}(\\mathbf{x}) = \\mathbb{E}_{\\mathbf{x} \\sim P(\\mathbf{x})}[(\\mathbf{x} - \\mathbb{E}[\\mathbf{x}])^2]$ 첨도 : $\\text{Skewness}(\\mathbf{x}) = \\mathbb{E}\\left[ \\left( \\frac{\\mathbf{x} - \\mathbb{E}[\\mathbf{x}]}{\\sqrt(\\mathbb{V}(\\mathbf{x}))} \\right)^3 \\right]$ 분산 \\(\\text{Cov}(\\mathbf{x}_1, \\mathbf{x}_2) = \\mathbb{E}_{\\mathbf{x}_1,\\mathbf{x}_2 \\sim P(\\mathbf{x}_1\\mathbf{x}_2)}[(\\mathbf{x}_1 - \\mathbb{E}[\\mathbf{x}_1])(\\mathbf{x}_2 - \\mathbb{E}[\\mathbf{x}_2])]\\) 조건부확률과 기계학습 $ P(y | \\mathbf{x}) $ : 입력 $\\mathbf{x}$ 가 들어왔을 때 정답이 $\\mathbf{y}$ 일 확률 로지스틱 회귀에서는 선형모델 + softmax를 사용 추출된 패턴을 기반으로 확률을 해석함 분류문제 $softmax(\\mathbf{w}\\phi + \\mathbf{b})$은 데이터 $\\mathbf{x}$ 로부터 추출된 특징패턴 $\\phi({\\mathbf{x}})$과 가중치행렬 $\\mathbf{W}$ 을 통해 조건부확률 $P(y |\\mathbf{x})$을 계산 회귀문제 조건부기대값 $\\mathbb{E}[y|\\mathbf{x}]$ 을 추정함 $\\mathbb{E}_{y \\sim P(y|\\mathbf{x})}[y|\\mathbf{x}] = \\int_y yP(y|\\mathbf{x})dy$ 왜 조건부기대값을 계산하는가? 공식연산 결과가 $L_{2}$-norm의 최소화 식과 일치함 몬테카를로 샘플링\\[\\mathbb{E}_{\\mathbf{x} \\sim P(\\mathbf{x})}[f(\\mathbf{x})] \\approx \\frac{1}{N}\\sum_{i=1}^{N}f(\\mathbf{x}^{(i)}) \\quad,\\quad \\mathbf{x}^{(i)} \\stackrel{iid}{\\sim} P(\\mathbf{x})\\] 확률분포를 모를때 기대값 계산을 위해 Monte Carlo Sampling이 필요함 example\\[\\begin{aligned} \\begin{matrix} \\int_{-1}^{1} e^{-x^2} dx = \\text{?} \\\\ \\frac{1}{2}\\int_{-1}^{1}e^{-x^{2}}dx \\approx \\frac{1}{N}\\sum_{i=1}^{N}f(x^{(i)}) \\quad,\\quad x^{(i)} \\sim U(-1, 1) \\end{matrix} \\end{aligned}\\] 이 과정에서 적분구간 길이가 2이므로 균등분포의 추출을 위해 2로 나눔 " }, { "title": "[BoostCamp AI Tech / Level 1 - AI Math] Day3 - 경사하강법 (매운맛)", "url": "/posts/day3_4_GD_hard/", "categories": "NAVER BoostCamp AI Tech, Level 1 - AI Math", "tags": "NAVER, BoostCamp, AI Tech, AI Math, math, Gradient Descent, 경사하강법", "date": "2022-01-19 22:00:00 +0900", "snippet": "AI Math : 경사하강법 (매운맛)Introduction 선형모델은 무어-펜로즈를 사용해서 계산이 가능하지만 Gradient Descent로 계산을 많이 함 목적식 : $|| \\mathbf{y} - \\mathbf{X}\\beta||_{2}$ $\\Rightarrow$ 목적식을 최소화하는 $\\beta$를 찾는 것이 목적이므로 $\\beta$로 편미분 진행한다. 무어-펜로즈 공식은 링크 참조 경사하강법 (Gradient Descent)1. 공식\\[\\begin{aligned} \\begin{aligned} \\text{(1)}\\quad\\quad\\quad\\quad\\;\\;\\; \\partial_{\\beta_{k}}||\\mathbf{y} - \\mathbf{X}\\beta||_{2} = \\partial_{\\beta_{k}}\\left\\{ \\frac{1}{n}\\sum^{n}_{i=1}\\left( y_{i} - \\sum_{j=1}^{d}X_{ij}\\beta_{j} \\right)^2 \\right\\}^{1/2} \\end{aligned} \\\\\\\\ \\begin{matrix} \\text{(2)}\\quad\\quad\\quad \\nabla_{\\beta} ||\\mathbf{y} - \\mathbf{X}\\beta||_{2} &amp;amp;=&amp;amp; (\\partial_{\\beta_{1}}||\\mathbf{y} - \\mathbf{X}\\beta||_{2}, ..., \\partial_{\\beta_{d}}||\\mathbf{y} - \\mathbf{X}\\beta||_{2} ) \\\\ &amp;amp;=&amp;amp; \\left( -\\frac{\\mathbf{X}^{\\intercal}_{\\cdot 1}(\\mathbf{y}-\\mathbf{X}\\beta)}{n||\\mathbf{y}-\\mathbf{X}\\beta||_{2}}, ..., -\\frac{\\mathbf{X}^{\\intercal}_{\\cdot d}(\\mathbf{y}-\\mathbf{X}\\beta)}{n||\\mathbf{y}-\\mathbf{X}\\beta||_{2}} \\right) \\\\ &amp;amp;=&amp;amp; -\\frac{\\mathbf{X}^{\\intercal}(\\mathbf{y}-\\mathbf{X}\\beta)}{n||\\mathbf{y}-\\mathbf{X}\\beta||_{2}} \\\\\\\\ &amp;amp;\\Downarrow&amp;amp; \\\\\\\\ \\beta^{(t+1)} &amp;amp; \\leftarrow &amp;amp; \\beta^{(t)} - \\lambda\\nabla_{\\beta}||\\mathbf{y} - \\mathbf{X}\\beta^{(t)}|| \\\\ &amp;amp;=&amp;amp; \\beta^{(t)} + \\frac{\\lambda}{n}\\frac{\\mathbf{X}^{\\intercal}(\\mathbf{y}-\\mathbf{X}\\beta^{(t)})}{||\\mathbf{y}-\\mathbf{X}\\beta^{(t)}||_{2}} \\\\ \\end{matrix} \\\\\\end{aligned}\\] 목적식을 $\\begin{Vmatrix}\\mathbf{y} - \\mathbf{X}\\beta^{(t)}\\end{Vmatrix}^{2}_{2}$ 를 쓴다면 gradient는 다음과 같다.\\[\\nabla_{\\beta}\\begin{Vmatrix}\\mathbf{y} - \\mathbf{X}\\beta\\end{Vmatrix}_{2}^{2} = -\\frac{2}{n}\\mathbf{X}^\\intercal (\\mathbf{y} - \\mathbf{X}\\beta)\\] $\\beta$로 편미분을 진행하는 의미를 좀 자세히 알아보자.미분을 하는 이유는 좌표계에서 함수값의 변화 방향성을 알기 위해서이다.위의 식에서 $\\mathbf{X}$는 다변수로 존재하는 벡터 형식의 데이터인데, $\\beta$는 $\\beta_{1}$, $\\beta_{2}$, …, $\\beta_{d}$가 모여있는 벡터라고 보면 된다. 각 $\\beta$가 담당하는 좌표계에서의 변수가 극솟값을 향하는 방향으로 연산을 하기위해 각 $\\beta$에 맞추어 편미분을 진행하는 것이다.좀 더 수식적으로 바라보면 아래와 같다.\\(\\begin{aligned}\\mathbf{X} = \\begin{bmatrix} x_{11} &amp;amp; x_{12} &amp;amp; ... &amp;amp; x_{1n} \\\\ x_{21} &amp;amp; x_{22} &amp;amp; ... &amp;amp; x_{2n} \\\\ \\vdots &amp;amp; \\vdots &amp;amp; &amp;amp; \\vdots \\\\ x_{m1} &amp;amp; x_{m2} &amp;amp; ... &amp;amp; x_{mn}\\end{bmatrix}\\quad\\beta = \\begin{bmatrix} \\beta_{1} \\\\ \\beta_{2} \\\\ \\vdots \\\\ \\beta_{n}\\end{bmatrix}\\quad\\mathbf{X}\\beta = \\begin{bmatrix} x_{11}\\beta_{1} &amp;amp; x_{12}\\beta_{2} &amp;amp; ... &amp;amp; x_{1n}\\beta_{n} \\\\ x_{21}\\beta_{1} &amp;amp; x_{22}\\beta_{2} &amp;amp; ... &amp;amp; x_{2n}\\beta_{n} \\\\ \\vdots &amp;amp; \\vdots &amp;amp; &amp;amp; \\vdots \\\\ x_{m1}\\beta_{1} &amp;amp; x_{m2}\\beta_{2} &amp;amp; ... &amp;amp; x_{mn}\\beta_{n}\\end{bmatrix}\\end{aligned}\\)여기서 $x_{ij}$는 $i$번째 데이터의 $j$번째 축값을 나타낸다고 생각하면 된다. 즉 한 줄의 행벡터는 n개 축을 갖는 좌표계에 있는 1개의 점을 의미한다.물론 여기서는 모든 $\\beta$값이 동일하다고 보는게 맞는 것으로 본다. 어떻게 위의 gradient 계산값이 나온 건지는 너무 길기 때문에 접은 글에 적어두겠다. gradient 공식 유도 위에서 편미분 값으로 활용된 $L_{2}$-norm을 풀어서 적으면 다음과 같다. $$ \\begin{aligned} \\begin{Vmatrix} \\mathbf{y} - \\mathbf{X}\\beta \\end{Vmatrix}_{2} = \\sqrt{(y_{1} - x_{1}\\beta_{1})^{2}+(y_{2} - x_{2}\\beta_{2})^{2}+...+(y_{d} - x_{d}\\beta_{d})^{2}} \\end{aligned} $$ 위의 $L_{2}$-norm을 $\\beta_{k}$에 대해 편미분을 하면 다음과 같이 계산된다. 고등학교때 배운 속미분을 활용하면된다. $$ \\begin{aligned} \\begin{matrix} \\partial_{\\beta_{k}} \\begin{Vmatrix} \\mathbf{y} - \\mathbf{X}\\beta \\end{Vmatrix}_{2} &amp;amp;=&amp;amp; \\partial_{\\beta_{k}}\\sqrt{(y_{1} - x_{1}\\beta_{1})^{2}+(y_{2} - x_{2}\\beta_{2})^{2}+...+(y_{d} - x_{d}\\beta_{d})^{2}} \\\\ &amp;amp;=&amp;amp; \\frac{1}{2}\\left( \\begin{Vmatrix} \\mathbf{y} - \\mathbf{X}\\beta \\end{Vmatrix}_{2} \\right)^{-1} \\times 2(-x_{k})(y_{k} - x_{k}\\beta_{k}) \\end{matrix} \\end{aligned} $$ 이렇게 계산된 편미분은 결과적으로 다음과 같은 값을 갖게된다. ($x_{k}는 1차원인 경우 이고$ n차원인 경우는 $\\mathbf{X}^{\\intercal}_{\\cdot k}$가 된다. 왜 전치행렬이 쓰이는지는 행렬을 작게 만들어서 차원비교 해보면 간단하게 확인 가능하다.) $$ \\left( -\\frac{\\mathbf{X}^{\\intercal}_{\\cdot 1}(\\mathbf{y}-\\mathbf{X}\\beta)}{n||\\mathbf{y}-\\mathbf{X}\\beta||_{2}}, ..., -\\frac{\\mathbf{X}^{\\intercal}_{\\cdot d}(\\mathbf{y}-\\mathbf{X}\\beta)}{n||\\mathbf{y}-\\mathbf{X}\\beta||_{2}} \\right) = -\\frac{\\mathbf{X}^{\\intercal}(\\mathbf{y}-\\mathbf{X}\\beta)}{n||\\mathbf{y}-\\mathbf{X}\\beta||_{2}} $$2. Gradient Descent AlgorithmInput : X, y, lr, TOutput : betaAlgorithm gradient_descent(lr, T) for t in range(T) error = y - X @ beta grad = -X.T @ error beta = beta - lr * grad eps 대신 T를 활용하여 학습횟수를 결정한다. learning rate (lr)과 학습횟수 (T)를 적절히 잘 조절하는 것이 중요하다.확률적 경사하강법 (Stochastic Gradient Descent : SGD) 경사하강법은 선형회귀같은 미분가능하고 볼록(convex)하면 수렴이 보장된다. $L_{2}$-norm 기반은 반드시 $\\beta$에 대해 볼록이 보장된다. 비선형회귀는 목적식이 볼록하지 않을 수도 있다. $\\Rightarrow$ 수렴이 보장되지 않음1. SGD\\[\\begin{matrix}\\theta^{(t+1)} &amp;amp;\\leftarrow&amp;amp; \\theta^{(t)} - \\hat{\\nabla_{\\theta} \\mathcal{L}}(\\theta^{(t)}) \\\\\\\\&amp;amp;\\Downarrow&amp;amp; \\\\\\\\\\beta^{(t+1)} &amp;amp;\\leftarrow&amp;amp; \\beta^{(t)} + \\frac{2 \\lambda}{b}\\mathbf{X}^\\intercal_{(b)}\\left( \\mathbf{y}_{(b)} - \\mathbf{X}_{(b)}\\beta^{(t)} \\right)\\end{matrix}\\] 모든 data가 아닌 1개 또는 일부 data를 sampling하여 gradient를 계산하는 방식 1개를 쓰면 SGD, 일부 data만 쓰면 mini-batch SGD라고 하지만 요즘 사용하는 SGD는 일반적으로 mini-batch SGD를 말한다. 이때, 모든 데이터 $(\\mathbf{X}, \\mathbf{y})$ 를 쓰지 않고 미니배치 데이터인 $(\\mathbf{X_{(b)}, \\mathbf{y}_{(b)}})$를 사용한다. 이는 모든 데이터를 계산하는 것보다 연산량을 줄여준다. 2. Mini-batch SGD는 일부 데이터를 확률적(Stochastic)으로 sampling을 진행하므로 계산때마다 gradient를 계산하는 목적식이 바뀐다.$\\Rightarrow$ local minimum을 탈출할 수 있다.$\\Rightarrow$ 볼록함수가 아닌 목적식에도 사용가능하므로 더 효율적이다. learning rate, 학습횟수와 추가로 mini-batch size도 같이 고려할 필요가 있다. 병렬 연산이 가능하다 CPU : 데이터 전처리 작업 + GPU 작업용 데이터 준비 GPU : 행렬연산 + model parameter update " }, { "title": "[BoostCamp AI Tech / Level 1 - AI Math] Day3 - 경사하강법 (순한맛)", "url": "/posts/day3_3_GD_easy/", "categories": "NAVER BoostCamp AI Tech, Level 1 - AI Math", "tags": "NAVER, BoostCamp, AI Tech, AI Math, math, Gradient Descent, 경사하강법", "date": "2022-01-19 21:00:00 +0900", "snippet": "AI Math : 경사하강법 (순한맛)미분 (Differentiation)\\[f&#39;(x) = \\lim_{h \\rightarrow 0}\\frac{f(x+h) - f(x)}{h}\\] 변수의 움직임에 따른 함숫값의 변화를 측정한다. ($x$, $f(x)$)에서의 접선의 기울기 최적화에 사용되는 연산 sympy 모듈을 활용하면 미분값 계산이 가능하다 sympy.diff() 접선의 기울기 : 어떤 방향으로 가야 함수값을 증가/감소하는 지를 알려주는 역할 편미분 (Partial Differentiation) \\[\\begin{aligned} \\partial_{x_{i}}f(\\mathbf{X}) = \\lim_{h \\rightarrow 0}\\frac{f(\\mathbf{X} + h\\mathbf{e}_{i}) - f(\\mathbf{X})}{h} \\\\\\\\ \\nabla f = \\left( \\partial_{x_{1}}, \\partial_{x_{2}}, ..., \\partial_{x_{d}} \\right) \\quad\\quad\\end{aligned}\\] 변수가 벡터와 같은 다변수인 경우 편미분을 활용 $\\nabla f$의 의미는 가장 빨리 증가하는 방향을 의미한다.따라서 $-\\nabla f$는 가장 빠르게 감소하는 방향을 의미한다. 변수별로 편미분을 계산한 gradient 벡터를 사용한다. n차원에서는 특정 극점으로 향하는 것 $-\\nabla f$는 극소점으로 향한다. 경사하강법 (Gradient Descent) 미분값을 빼는 계산을 하는 것을 경사하강법(Gradient Descent)라고 한다. 극솟값의 위치를 구함 목적함수를 최소화 극값에 도달한다면 $f’(x) = 0$이므로 gradient 연산 update를 중지한다. Gradient Descent Algorithm1. 단일변수Input : gradient, init, lr, epsOutput : varAlgorithm gradient_descent(lr, eps) var = init grad = gradient(var) while abs(var) &amp;gt; eps var = var - lr * grad grad = gradient(var) gradient : 미분계산함수 init : 초기화 값 lr : learning rate 미분의 update 속도를 결정 eps : 학습종료 조건 var = var - lt * grad 부분이 $x - \\lambda f’(x)$를 구현한 부분2. 다변수 (vector)Input : gradient, init, lr, epsOutput : varAlgorithm gradient_descent(lr, eps) var = init grad = gradient(var) while norm(var) &amp;gt; eps var = var - lr * grad grad = gradient(var) gradient : gradient($\\nabla$) 계산 함수 단일변수와 다르게 학습종료 기준은 norm을 계산한 수치로 설정Gradient Descent By Python1. 단일변수\\[f(x) = x^2 + 2x + 3 \\text{일 때 최소점을 찾는 코드}\\]import sympy as symimport numpy as npfrom sympy.abc import xdef func(val): fun = sym.poly(x**2 + 2*x + 3) return fun.subs(x, val), fundef func_gradient(fun, val): _, function = fun(val) diff = sym.diff(function, x) return diff.subs(x, val), diffdef gradient_descent(fun, init, lr=1e-2, eps=1e-5): cnt = 0 val = init diff, _ = func_gradient(fun, init) while np.abs(diff) &amp;gt; eps: val = val - lr * diff diff, _ = func_gradient(fun, val) cnt += 1 print(f&quot;함수 : {fun(val)[1]}, 연산횟수 : {cnt}, 최소점 : ({val}, {fun(val)[0]})&quot;)gradient_descent(fun=func, init=np.random.uniform(-2, 2))2. 다변수 (벡터)\\[f(x) = x^2 + 2y^2 \\text{일 때 최소점을 찾는 코드}\\]import sympy as symimport numpy as npfrom sympy.abc import x, ydef eval_(fun, val): val_x, val_y = val fun_eval = fun.subs(x, val_x).subs(y, val_y) return fun_evaldef func_multi(val): x_, y_ = val func = sym.poly(x**2 + 2*y**2) return eval_(func, [x_, y_]), funcdef func_gradient(fun, val): x_, y_ = val _, function = fun(val) diff_x = sym.diff(function, x) diff_y = sym.diff(function, y) grad_vec = np.array([eval_(diff_x, [x_, y_]), eval_(diff_y, [x_, y_])], dtype=float) return grad_vec, [diff_x, diff_y]def gradient_descent(fun, init, lr=1e-2, eps=1e-5): cnt = 0 val = init diff, _ = func_gradient(fun, init) while np.linalg.norm(diff) &amp;gt; eps: val = val - lr * diff diff, _ = func_gradient(fun, val) cnt += 1 print(f&quot;함수 : {fun(val)[1]}, 연산횟수 : {cnt}, 최소점 : ({val}, {fun(val)[0]})&quot;)pt = [np.random.uniform(-2, 2), np.random.uniform(-2, 2)]gradient_descent(fun=func_multi, init=pt)" }, { "title": "[BoostCamp AI Tech / Level 1 - AI Math] Day3 - Matrix", "url": "/posts/day3_2_matrix/", "categories": "NAVER BoostCamp AI Tech, Level 1 - AI Math", "tags": "NAVER, BoostCamp, AI Tech, AI Math, math, matrix", "date": "2022-01-19 16:30:00 +0900", "snippet": "AI Math : MatrixMatrix\\[\\begin{aligned}\\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_1 \\\\ \\mathbf{x}_2 \\\\ \\mathbf{x}_3 \\\\ \\vdots \\\\ \\mathbf{x}_n \\end{bmatrix} = \\begin{bmatrix} x_{11} &amp;amp; x_{12} &amp;amp; ... &amp;amp; x_{1m} \\\\ x_{21} &amp;amp; x_{22} &amp;amp; ... &amp;amp; x_{2m} \\\\ \\vdots &amp;amp; \\vdots &amp;amp; x_{ij} &amp;amp; \\vdots \\\\ x_{n1} &amp;amp; x_{n2} &amp;amp; ... &amp;amp; x_{nm} \\end{bmatrix} = (x_{ij})\\end{aligned}\\] 벡터를 원소로 갖는 2차원 배열 전치행렬 (Transpose)\\[\\mathbf{X}^\\intercal = (x_{ji})\\] 행렬곱\\[\\begin{aligned} \\mathbf{X}\\mathbf{Y} = \\left(\\sum_{k}x_{ik}y_{kj} \\right) \\end{aligned}\\] 행렬곱은 @를 사용하면 연산가능 행렬내적 실제로 행렬 내적을 계산한 것은 $\\text{tr}\\left(\\mathbf{X}\\mathbf{Y}^\\intercal\\right)$의 결과이다. \\[\\begin{aligned} \\mathbf{X}\\mathbf{Y}^\\intercal = \\left(\\sum_{k}x_{ik}y_{jk} \\right) \\end{aligned}\\] 행렬은 벡터공간에 사용되는 연산자와 같음 특정 벡터공간에서 다른 벡터공간으로 변환해주는 역할을 함 \\[\\begin{aligned} \\begin{bmatrix} z_1 \\\\ z_2 \\\\ z_3 \\\\ \\vdots \\\\ \\mathbf{z}_n \\end{bmatrix} = \\begin{bmatrix} a_{11} &amp;amp; a_{12} &amp;amp; ... &amp;amp; a_{1m} \\\\ a_{21} &amp;amp; a_{22} &amp;amp; ... &amp;amp; a_{2m} \\\\ \\vdots &amp;amp; \\vdots &amp;amp; \\ddots &amp;amp; \\vdots \\\\ a_{n1} &amp;amp; a_{n2} &amp;amp; ... &amp;amp; a_{nm} \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ ... \\\\ \\mathbf{x}_m \\end{bmatrix} \\\\\\\\ \\mathbf{Z} \\quad\\qquad\\qquad\\quad \\mathbf{A} \\quad\\qquad\\qquad \\mathbf{X}\\quad \\end{aligned}\\] 행렬곱은 패턴추출, 데이터 압축과 같은 경우에 사용 역행렬\\[\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I} = \\mathbf{A}^{-1}\\mathbf{A}\\] determinant가 0이 아닌 경우에 가능 np.linalg.inv를 활용하여 계산가능 역행렬 계산이 어려운 경우 유사 역행렬을 활용하여 계산 무어-펜로즈 역행렬 $\\mathbf{A}^{+}$\\[\\begin{aligned} n \\geq m \\quad : \\quad \\mathbf{A}^{+} = (\\mathbf{A}^\\intercal\\mathbf{A})^{-1}\\mathbf{A}^\\intercal \\\\ n \\leq m \\quad : \\quad \\mathbf{A}^{+} = \\mathbf{A}^\\intercal(\\mathbf{A}\\mathbf{A}^\\intercal)^{-1} \\end{aligned}\\] np.linalg.pinv를 활용하여 계산가능 유사 역행렬 활용 연립방정식의 해를 구하는 경우 활용 ($n \\leq m$) \\[\\begin{aligned} a_{11}x_{1} + a_{12}x_{2} + ... + a_{1m}x_{m} = b_{1} \\\\ a_{11}x_{1} + a_{12}x_{2} + ... + a_{1m}x_{m} = b_{1} \\\\\\\\ \\vdots \\qquad\\qquad\\qquad\\quad \\\\\\\\ a_{n1}x_{1} + a_{n2}x_{2} + ... + a_{nm}x_{m} = b_{n} \\\\ \\end{aligned} \\quad \\rightarrow \\quad \\begin{matrix} \\mathbf{A}\\mathbf{x} &amp;amp;=&amp;amp; \\mathbf{b} \\\\ \\Rightarrow&amp;amp; \\mathbf{x} &amp;amp;=&amp;amp; \\mathbf{A}^{+}\\mathbf{b} \\\\ &amp;amp; &amp;amp;=&amp;amp; \\mathbf{A}^\\intercal(\\mathbf{A}\\mathbf{A}^\\intercal)^{-1}\\mathbf{b} \\end{matrix}\\] 선형회귀식을 찾을 수 있음 ($n \\geq m$) : 방정식을 푸는 것은 불가능함 \\[\\begin{aligned} \\begin{bmatrix} - &amp;amp; \\mathbf{x}_{1} &amp;amp; - \\\\ - &amp;amp; \\mathbf{x}_{2} &amp;amp; - \\\\ &amp;amp; \\vdots &amp;amp; \\\\ - &amp;amp; \\mathbf{x}_{n} &amp;amp; - \\end{bmatrix} \\end{aligned} \\begin{aligned} \\begin{bmatrix} \\mathbf{\\beta_1} \\\\ \\mathbf{\\beta_2} \\\\ \\vdots\\\\ \\mathbf{\\beta_m} \\end{bmatrix} \\end{aligned} = \\begin{aligned} \\begin{bmatrix} \\mathbf{y_1} \\\\ \\mathbf{y_2} \\\\ \\vdots\\\\ \\mathbf{y_m} \\end{bmatrix} \\end{aligned} \\quad\\quad \\begin{aligned} \\mathbf{X}\\beta = \\hat{\\mathbf{y}} \\approx \\mathbf{y}\\quad\\quad\\quad\\quad\\quad\\quad \\\\ \\begin{matrix} \\Rightarrow &amp;amp; \\beta &amp;amp;=&amp;amp; \\mathbf{X}^{+}\\mathbf{y} \\\\ &amp;amp;&amp;amp;=&amp;amp; \\left(\\mathbf{X}^\\intercal\\mathbf{X} \\right)^{-1}\\mathbf{X}^\\intercal\\mathbf{y} \\end{matrix} \\end{aligned}\\] " }, { "title": "[BoostCamp AI Tech / Level 1 - Python/PyTorch] Day3 - Numpy", "url": "/posts/day3_1_numpy/", "categories": "NAVER BoostCamp AI Tech, Level 1 - Python/PyTorch", "tags": "NAVER, BoostCamp, AI Tech, Python, Basic, numpy", "date": "2022-01-19 00:00:00 +0900", "snippet": "Python : Numpy행렬의 표현\\[\\begin{aligned} \\begin{aligned} 2x_{1} + 2x_{2} + x_{3} = 9 \\\\ 2x_{1} - x_{2} + 2x_{3} = 6 \\\\ x_{1} - x{2} + 2x_{3} = 5 \\end{aligned} \\rightarrow \\begin{bmatrix} 2 &amp;amp; 2 &amp;amp; 1 &amp;amp; 9 \\\\ 2 &amp;amp; -1 &amp;amp; 2 &amp;amp; 6 \\\\ 1 &amp;amp; -1 &amp;amp; 2 &amp;amp; 5 \\end{bmatrix}\\end{aligned}\\]위와 같은 수식이 존재할 때 선형대수에서는 이를 행렬로 표현하는 방식을 많이 사용한다. 행렬을 코드로 나타내는 방법에는 2가지 방식이 있다. 리스트 표현 matrix = [[2, 2, 1], [2, -1, 2], [1, -1, 2]] constant = [9, 6, 5] 이 방식은 큰 matrix를 표현하기엔 메모리상 문제가 있음 처리속도의 문제가 발생 Numpy 활용 Numpy 리스트에 비해 빠르고, 메모리 효율적 반복문 없이 데이터 배열을 처리할 수 있음 C, C++, Fortran과 통합이 가능함 사용시 import numpy as np로 선언1. ndarray np.array(data, dtype) numpy array의 저장타입 리스트와 다르게 dynamic typing이 불가능하고 하나의 타입만 사용 가능 리스트는 item을 메모리주소로 static하게 저장하지만 ndarray는 data에 값을 직접적으로 메모리에 저장하기 때문에 연산속도가 빠르다. array의 shape에 따라서 이름을 다르게 부른다. Rank Name ex 0 scalar 7 1 vector [10, 10] 2 matrix [[10, 10], [15, 15]] 3 3-tensor [[[1,5,9],[2,6,10]],[[3,7,11],[4,8,12]]] n n-tensor … shape : numpy array의 dimension을 반환 (몇행 몇열의 구성인지) dtype : numpy array의 데이터 type을 반환 ndim : number of dimension (dimension 수만 반환) size : data의 개수 (각 차원별 데이터 수를 곱한 값)Handling Shape1. reshape ndarray.reshape(shape) element의 갯수는 동일하게 유지하고 shape만 변경 shape의 특정 dimension 값이 -1이면 알아서 값을 조정해줌 test_matrix = [[1, 2, 3, 4], [5, 6, 7, 8]] print(np.array(test_matrix)) print(np.array(test_matrix.reshape(4, 2))) &#39;&#39;&#39; &amp;lt;output&amp;gt; [[1 2 3 4] [5 6 7 8]] [[1 2] [3 4] [5 6] [7 8]] &#39;&#39;&#39; 2. flatten ndarray.flatten() 다차원 array를 1차원 array로 변환 test_matrix = [[1, 2, 3, 4], [5, 6, 7, 8]] print(np.array(test_matrix).flatten()) &#39;&#39;&#39; &amp;lt;output&amp;gt; [1 2 3 4 5 6 7 8] &#39;&#39;&#39; Indexing &amp;amp; Slicing indexing과 slicing은 파이썬의 list나 numpy array를 다룰때 모든 것이라 할만큼 중요하다. a[:, 2:] : 모든 row의 2열 이상의 데이터 a[1, 1:3] : 1행의 1~2열 데이터 a[1:3] : 1~2행의 데이터 a[:, ::2] : 모든 행의 시작부터 2단위의 열 데이터numpy creation1. arange np.arange(start, end, step) example np.arange(10) # array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) np.arange(0, 5, 0.5) # array([0. , 0.5, 1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5]) 2. ones, zeros, empty np.zeros(shape, dtype) : 0으로 가득한 ndarray np.ones(shape, dtype) : 1로 가득찬 ndarray np.empty(shape) : shape만 있고 빈 ndarray (값을 초기화하지 않음) example # ones np.ones((2, 3), dtype=float) # zeros np.zeros((2, 3)) # empty np.empty((10, 5)) 3. something_like np.ones_like(ndarray): ndarray랑 shape가 같고 1로 구성된 ndarray np.zeros_like(ndarray): ndarray랑 shape가 같고 0으로 구성된 ndarray np.empty_like(ndarray): ndarray랑 shape가 같고 초기화가 안된 값으로 구성된 ndarray4. identity np.identity(n=rows, dtype) : n x n의 단위행렬 example np.identity(n=3, dtype=np.int8) &#39;&#39;&#39; array([[1, 0, 0], [0, 1, 0], [0, 0, 1]], dtype=int8) &#39;&#39;&#39; 5. eye np.eye(N, M, k) : N x M 행렬에서 k가 시작 index인 대각성분이 1인 행렬 example np.eye(N=3, M=5, k=1) &#39;&#39;&#39; array([[0., 1., 0., 0., 0.], [0., 0., 1., 0., 0.], [0., 0., 0., 1., 0.]]) &#39;&#39;&#39; 6. diag np.diag(ndarray, k=시작index) : 대각 성분을 추출 example matrix = np.arange(9).reshape(3, 3) print(matrix) print(np.diag(matrix)) &#39;&#39;&#39; [[0 1 2] [3 4 5] [6 7 8]] [0 4 8] &#39;&#39;&#39; 7. random_sampling np.random.uniform(모수, 갯수) : uniform 분포 np.random.normal(모수, 갯수) : 정규 분포 이 외에 다양한 통계 분포를 쓸 수 있음Operation function1. sum sum() : 모든 원소의 합 sum의 인자로는 axis가 존재 axis = 1 : col 방향의 합 axis = 0 : row 방향의 합 축이 추가되면 새로 생기는 축이 항상 axis = 0을 나타냄 2. concatenate vstack() : vertical stack으로 위아래로 두 array를 합침 합치는 두 array는 반드시 열의 개수가 일치해야한다. hstack() : horizontal stack으로 좌우로 두 array를 합침 합치는 두 array의 행은 반드시 개수가 일치해야한다. concatenate(axis) axis=0 : 위아래로 합침 axis=1 : 좌우로 합침 Array operations element-wise 연산을 모두 지원함 +, -, /, * 로 행렬을 계산하면 각 위치의 원소로 연산 dot product : 행렬 곱 연산 (내적 연산) arr.dot(arr) transpose : 전치행렬 arr.T or arr.transpose() broadcasting shape이 다른 배열간의 연산을 해줌 \\(\\begin{align} \\begin{bmatrix} 1 &amp;amp; 2 &amp;amp; 3 \\\\ 4 &amp;amp; 5 &amp;amp; 6 \\end{bmatrix} + 3 = \\begin{bmatrix} 4 &amp;amp; 5 &amp;amp; 6 \\\\ 7 &amp;amp; 8 &amp;amp; 9 \\end{bmatrix} \\end{align}\\) Comparison np.where(condition, Tvalue, Fvalue) : codition에 만족하는 index를 반환하고 value가 주어지면 만족하는 위치에 해당되는 값으로 대체한다. np.argmax(ndarray), np.argmin(ndarray) : array의 최대/최소의 index를 반환 axis 기반으로 가능할 수 있음 argsort() : 값을 기반으로 정렬된 index를 반환 " }, { "title": "[Programmers] 카카오 블라인드 2021 - 신규 아이디 추천", "url": "/posts/prg72410/", "categories": "Algorithm, Programmers", "tags": "Algorithm, Programmers, 구현, Kakao", "date": "2022-01-19 00:00:00 +0900", "snippet": "카카오 블라인드 2021 - 신규 아이디 추천Kakao Blind Recruit 2021 - 신규 아이디 추천문제 Comment카카오 블라인드 코딩 테스트의 요즘 트렌드는 “구현”입니다.매년 인턴십, 블라인드 코테에 이런식으 조건을 부여하고 구현하는 문제들이 꼭 1문제씩은 출제합니다.이런 문제를 접근할 때는 문제상에 구현의 힌트를 많이 주기때문에 문제와 주어진 예시를 잘 파악하는 것이 좋습니다.문제 풀이문자열을 control하는 문제인 만큼 파이썬이 유리할 수도 있지만 C++로 접근해도 충분히 괜찮은 문제입니다.문제 풀이 접근에서 치환과 추가는 어렵지 않습니다.하지만 제거의 효율을 높이고자 제거되는 문자열은 ‘\\0’로 변경해주었습니다.~지금 생각해보면 그렇게 효율이 좋은것 같지는 않은 것 같기도…~C++의 문자열에서 ‘\\0’은 출력상에 표현하지 않기 때문입니다.단, 여기서 주의할 점은 string을 비교할 때, 중간에 들어간 \\0은 유지가 되므로 출력이 같은 문자열이 서로 다른 문자열로 인식됩니다.string str1 = &quot;abcd&quot;;string str2 = &quot;a\\0bcd&quot;;이런 경우 C++의 채점서버는 두 문자열이 다르다고 채점을 하고 심지어 두 문자열의 길이도 str1은 4, str2는 5로 반환합니다.따라서 길이와 관련된 작업이 있거나 제거 작업이 끝난 후에는 반드시 \\0을 빼고 문자열을 재구성할 필요가 있습니다.코드#include &amp;lt;string&amp;gt;#include &amp;lt;vector&amp;gt;#include &amp;lt;iostream&amp;gt;using namespace std;string reset(string input) { string ret = &quot;&quot;; for(int i = 0; i &amp;lt; input.size(); i++) { if(input[i] != &#39;\\0&#39;) ret += input[i]; } return ret;}string solution(string new_id) { string answer = &quot;&quot;; char m[3] = {&#39;-&#39;, &#39;_&#39;, &#39;.&#39;}; // step 1 for(int i = 0; i &amp;lt; new_id.size(); i++) { new_id[i] = tolower(new_id[i]); } // step 2 for(int i = 0; i &amp;lt; new_id.size(); i++) { bool flag = false; if(new_id[i] &amp;lt; &#39;a&#39; || new_id[i] &amp;gt; &#39;z&#39;) { if(new_id[i]-&#39;0&#39; &amp;gt;= 0 &amp;amp;&amp;amp; new_id[i]-&#39;0&#39; &amp;lt;= 9) continue; else { for(int j = 0; j &amp;lt; 3; j++) { if(new_id[i] == m[j]) { flag = true; break; } } if(!flag) { new_id[i] = &#39;\\0&#39;; } } } } new_id = reset(new_id); char prev = new_id[0]; // step 3 for(int i = 1; i &amp;lt; new_id.size(); i++) { if(prev == &#39;.&#39; &amp;amp;&amp;amp; new_id[i] == &#39;.&#39;) { prev = new_id[i]; new_id[i] = &#39;\\0&#39;; }else{ prev = new_id[i]; } } new_id = reset(new_id); // step 4 if(new_id[0] == &#39;.&#39;) new_id[0] = &#39;\\0&#39;; if(new_id[new_id.size() - 1] == &#39;.&#39;) new_id[new_id.size() - 1] = &#39;\\0&#39;; new_id = reset(new_id); // step 5 if(new_id.size() == 0) new_id += &#39;a&#39;; // step 6 if(new_id.size() &amp;gt; 15) { for(int i = 15; i &amp;lt; new_id.size(); i++) { new_id[i] = &#39;\\0&#39;; } } new_id = reset(new_id); // step 4 if(new_id[0] == &#39;.&#39;) new_id[0] = &#39;\\0&#39;; if(new_id[new_id.size() - 1] == &#39;.&#39;) new_id[new_id.size() - 1] = &#39;\\0&#39;; new_id = reset(new_id); // step 7 if(new_id.size() &amp;lt; 3) { while(new_id.size() != 3) { new_id += new_id[new_id.size() - 1]; } } answer = new_id; return answer;}step 6를 종료한 후 양 끝의 마침표를 제거해야하므로 step4를 반드시 다시 한 번 해주셔야합니다." }, { "title": "[BOJ] 1743 음식물 피하기", "url": "/posts/1743/", "categories": "Algorithm, BOJ", "tags": "Algorithm, BOJ, DFS", "date": "2022-01-19 00:00:00 +0900", "snippet": "https://www.acmicpc.net/problem/7576음식물이 땅에 놓여져 있으니까 그걸 피해가야합니다.가장 큰 음식물은 뭘까요??? 찾아봅시다.문제 핵심 IDEA쉬운 문제입니다.핵심 아이디어는 DFS입니다.가장 큰 음식물 덩어리를 찾기만 하면 되는 거죠.연결된 곳을 통해서 잘 탐색을 해주면 됩니다.생각흐름평소 문제처럼 탐색 기반이 되는 map을 제공하지 않습니다.하지만 확인할 수 있는 예시와 좌표그림이 있기 때문에 똑같이 map이 있다고 생각하고 진행하면 됩니다.첫번째 접근DFS의 가장 기본적인 코딩으로 문제를 풀었습니다.맨 처음에 좌표를 입력 받을 때 각 좌표를 벡터에 저장해두고 그 부분들을 모두 탐색하는 방식을 선택했습니다.그리고 각 좌표를 DFS로 탐색한 이후에 최대 카운트를 비교해서 최댓값 갱신만 해주면 됩니다.정답코드#include &amp;lt;iostream&amp;gt;#include &amp;lt;utility&amp;gt;#include &amp;lt;vector&amp;gt;using namespace std;int N, M;int dirx[4] = {1,0,-1,0};int diry[4] = {0,1,0,-1};int map[101][101];bool visit[101][101];typedef pair&amp;lt;int, int&amp;gt; P; // (r,c) = (j, i)vector&amp;lt;P&amp;gt; pos;int cnt = 0;int maxi = 0;void dfs(int r, int c) { cnt++; visit[r][c] = true; for (int i = 0; i &amp;lt; 4; i++) { int nr = r + dirx[i]; int nc = c + diry[i]; if (nr &amp;gt;= 0 &amp;amp;&amp;amp; nr &amp;lt;= N &amp;amp;&amp;amp; nc &amp;gt;= 0 &amp;amp;&amp;amp; nc &amp;lt;= M) { if (!visit[nr][nc] &amp;amp;&amp;amp; map[nr][nc] == 1) { dfs(nr, nc); } } }}int main() { int K; cin &amp;gt;&amp;gt; N &amp;gt;&amp;gt; M &amp;gt;&amp;gt; K; for (int i = 0; i &amp;lt; K; i++) { int r, c; cin &amp;gt;&amp;gt; r &amp;gt;&amp;gt; c; map[r][c] = 1; pos.push_back(P(r,c)); } for (int i = 0; i &amp;lt; pos.size(); i++) { cnt = 0; dfs(pos[i].first,pos[i].second); if (cnt &amp;gt; maxi) maxi = cnt; } cout &amp;lt;&amp;lt; maxi &amp;lt;&amp;lt; &quot;\\n&quot;;}깃허브 - https://github.com/cow-coding/algorithm/blob/master/BOJ/1743.cpp" }, { "title": "[BOJ] 15780 멀티탭 충분하니?", "url": "/posts/15780/", "categories": "Algorithm, BOJ", "tags": "Algorithm, BOJ", "date": "2022-01-19 00:00:00 +0900", "snippet": "https://www.acmicpc.net/problem/15780간단히 요약하면 멀티탭을 사용할 때 1개의 멀티탭에는 연달아서 끼우면 안됩니다.이러한 조건에 맞춰서 과연 주어진 학생수와 주어진 멀티탭으로 모두의 멀티탭 사용을 만족시킬 수 있을까요?문제 핵심 IDEA문제에 규칙이 뚜렷하게 보이는 문제입니다.멀티탭이 남는 지 안남는 지를 확인하는 게 아니라 모두가 멀티탭을 쓸 수 있는가?가 핵심입니다.그냥 연달아 끼우지만 않으면 되기에 1칸씩만 띄우고 멀티탭을 채우면 됩니다.이렇게 하면 조건을 만족하면서 가장 많이 채울 수 있습니다.생각 흐름우리가 해결해야하는 것은 2가지입니다. 주어지는 멀티탭 한개에 최대로 꽂을 수 있는 콘센트는 몇개인가? 콘센트는 어떻게 구현할 것인가?입력을 보면 멀티탭의 구 개수가 모두 같다는 보장을 할 수 없습니다.구 갯수가 입력들어올때마다 처리를 해줘야합니다.첫번째 접근전체 학생 수를 멀티탭에 꽂는 최대 수를 계산할 때마다 처리를 해준다고 생각했습니다.멀티탭은 최대로 꽂기 위해서는 1개의 코드가 2칸을 차지한다고 생각하면 됩니다. 즉 2로 나눠주면 됩니다.두번째 접근하지만 순수하게 2로만 나눠주면 문제가 발생합니다. 바로 홀수의 경우입니다.홀수의 구멍이 있는 멀티탭은 마지막에 1개가 더 꽂을 수 있습니다.이 경우를 위해 홀수는 +1을 해줘야 합니다.세번째 접근사실 저는 이 문제를 맨 처음에 스택을 이용해서 풀었습니다. 하지만 이 글을 쓰기 위해서 문제를 본 결과 스택이 필요가 없었네요….;;정답 코드#include &amp;lt;iostream&amp;gt;using namespace std;int check(int num) // 멀티탭에 규칙으로 꽂을 수 있는 최대 콘센트 수{ int result{0}; if (num % 2 == 0) { result = num / 2; } else if (num % 2 != 0) { result = num / 2 + 1; } return result;}int main(){ int multi, people, hole; cin &amp;gt;&amp;gt; people &amp;gt;&amp;gt; multi; int pass = people; for (int i = 0; i &amp;lt; multi; i++) // 멀티탭 구 갯수 입력 { cin &amp;gt;&amp;gt; hole; pass -= check(hole); } if (pass &amp;lt;= 0) { cout &amp;lt;&amp;lt; &quot;YES\\n&quot;; } else if (pass &amp;gt; 0) cout &amp;lt;&amp;lt; &quot;NO\\n&quot;; return 0;}깃허브 - https://github.com/cow-coding/algorithm/blob/master/BOJ/15780.cpp" }, { "title": "[BoostCamp AI Tech / Level 1 - AI Math] Day2 - Vector", "url": "/posts/day2_5_vector/", "categories": "NAVER BoostCamp AI Tech, Level 1 - AI Math", "tags": "NAVER, BoostCamp, AI Tech, AI Math, math, vector", "date": "2022-01-18 01:32:00 +0900", "snippet": "AI Math : VectorVector\\[\\begin{aligned} \\mathbf{X} = \\begin{bmatrix} 1 \\\\ 7 \\\\ 2 \\end{bmatrix} \\quad\\quad\\quad \\mathbf{X}&amp;amp;^\\intercal = \\begin{bmatrix} 1 &amp;amp; 7 &amp;amp; 2 \\end{bmatrix}\\end{aligned}\\] 각각을 열벡터, 행벡터라고 부른다 벡터 연산 벡터의 +, - : 동일한 크기면 element-wise +, - 연산 $\\odot$ : element-wise product로 matrix 성분곱을 의미 vector norm\\[\\begin{aligned} \\mathbf{X} = \\begin{bmatrix} x_{1} \\\\ x_{2} \\\\ x_{3} \\end{bmatrix} \\quad\\quad \\begin{aligned} L_{1} : ||\\mathbf{X}||_{1} = \\sum_{i=1}^{d}|x_{i}| \\\\ L_{2} : ||\\mathbf{X}||_{2} = \\sqrt{\\sum_{i=1}^{d}|x_{i}|^{2}} \\end{aligned} \\end{aligned}\\] 임의의 차원 d에 대해 모두 성립 $L_{1}$ norm : 각 성분의 변화량의 절대값 좌표축을 따라 이동한 거리 $L_{2}$ norm : 피타고라스 정리에 기반한 유클리드 거리를 언급 두 norm값은 기하학적 성질이 다름 두 벡터 사이의 거리$|| \\mathbf{x} - \\mathbf{y}||_2 = || \\mathbf{y} - \\mathbf{x}||_2 $ 두 벡터 사이의 각도\\[\\begin{aligned} ||\\mathbf{x} - \\mathbf{y}||_2 = ||\\mathbf{y} - \\mathbf{x}||_2 \\\\ \\cos\\theta = \\frac{&amp;lt; \\mathbf{x}, \\mathbf{y} &amp;gt;}{||\\mathbf{x}||_{2}||\\mathbf{y}||_{2}}\\end{aligned}\\] 내적의 해석 정사영(orthogonal projection)된 벡터의 길이 $\\text{Proj}(\\mathbf{x}) = ||\\mathbf{x}|| \\cos \\theta$ " }, { "title": "[BoostCamp AI Tech] Day2", "url": "/posts/day2/", "categories": "NAVER BoostCamp AI Tech, Review", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech", "date": "2022-01-18 00:00:00 +0900", "snippet": "Day2 Review당신은 오늘 하루 어떻게 살았나요? 첫주차 정식 모더레이터 Python 강의 Chapter 4-5 + AI Math Chapter 1 퀴즈 AI Math 1강 퀴즈 - 벡터 1 ~ 5 첫 정규 피어세션 모더레이터 ✳️ 역할분담을 어떤 방식으로 할 지 정하기 MLops 채널을 만들어 보자!오늘의 피어세션 역할분담 어떻게 할지 정했다. 첫 코드리뷰코드리뷰 .isdigit()을 쓰면 re로 숫자를 검출하지 않아도 숫자판단이 가능! r&#39;[^\\_]&#39;로 정규표현식을 작성하면 _만 제거할 수 있음오늘 하지 못한 것들 MLops 채널을 만들어 보자! 대신 블로그, 깃허브 채널 생성…. MLOps는 좀 나중에? 내일은 어떤 것을 할까? 오프라인 정리 노트에 기존 정리 내용 옮기기마무리 오늘도 열심히 잘 보냈다. 아직은 좀 가볍지만 이제 슬슬 긴장을 갖자!" }, { "title": "[BoostCamp AI Tech / Level 1 - Python/PyTorch] Day2 - Module &amp; Package", "url": "/posts/day2_2_module_package/", "categories": "NAVER BoostCamp AI Tech, Level 1 - Python/PyTorch", "tags": "Python, NAVER, BoostCamp, AI Tech, Python, Basic", "date": "2022-01-18 00:00:00 +0900", "snippet": "Python : Module and PackageIntroduction 다른 프로그램을 사용하는 방식은 클래스와 모듈방식이 있다. 보통 모듈을 사용하는 것이 더 좋다. Module : 대상의 부품 및 조각 Package : 모듈을 모아놓은 단위Module .py파일을 모듈이라고 함. import 명령어를 사용하여 해당 모듈을 메모리에 load함 namespace from [module_name] import [method] alias import pandas as pd 처럼 as명령어를 사용하여 단순하게 명칭을 붙일 수 있음 Package 대형 project를 만드는 코드의 묶음 module들을 모아놓은 module의 합이라고 할 수 있다. __init__.py, __main.py__와 같은 키워드 파일들을 사용한다. python 3.3전에는 __init__.py가 반드시 필요했지만 이후에는 없어도 상관이 없어졌다. __init__.py에는 __all__ keyword를 사용하여 module을 미리 선언한다. " }, { "title": "[BoostCamp AI Tech / Level 1 - Python/PyTorch] Day2 - Python OOP", "url": "/posts/day2_1_python_oop/", "categories": "NAVER BoostCamp AI Tech, Level 1 - Python/PyTorch", "tags": "Python, NAVER, BoostCamp, AI Tech, OOP, Python, Basic", "date": "2022-01-18 00:00:00 +0900", "snippet": "Python : Object-Oriented ProgrammingOOP 기본 개념 객체 : 속성(attribute) + 행동(action)으로 구성되어있음 속성(attribute)는 일반적으로 변수 행동(action)은 method 클래스 : 객체의 설계도 흔히들 많이 쓰는 비유는 붕어빵틀을 클래스, 붕어빵을 객체라 하는데 개인적으로 좋아하진 않는 비유이다. 그거 하나로는 뒤에 오는 내용들을 통틀어 설명하기 어려움. 그래서 나는 보통 제품 사용설명서 혹은 제품 조립설명서라고 비유한다. Rule파이썬의 표준 작성법인 PEP-8에 따르면 변수, 함수명은 Snake case로 작성하고 클래스명은 Camel case로 작성한다. 변수(variable) / 함수(function) person_name = &quot;park&quot; def add_number(a, b): return a+b 변수와 함수는 snake case로 작성함이 원칙이다. 클래스(class) class Person(object): def __init__(self): self.name = &quot;park&quot; def about_me(self): print(f&quot;My name is {self.name}.&quot;) def __str__(self): return f&quot;My name is {self.name}.&quot; 클래스는 camel case로 작성함을 원칙으로 한다. 변수 및 함수에 __(double underscore)가 붙는 경우를 mangling이라고 한다. 이는 특수 예약 함수에 자주 사용되며 private 변수의 역할로도 사용한다. 자세한 내용을 언급한 블로그 글을 참고하길 바란다. 파이썬 더블 언더스코어:Magic Method - corikachu blog __str__은 출력문의 값을 결정한다. 클래스에 (object)는 상속하는 객체를 넣는다. 따로 상속하지 않는 클래스면 적지않아도 무방하다. OOP의 특징객체지향프로그래밍의 핵심 요소는 상속(Inheritance), 다형성(Polymorphism), 가시성(Visibility)가 있다.상속(Inheritance) 부모 클래스로부터 attribute, method를 물려받는 것 super()명령어를 사용하여 부모 class의 값을 가져올 수 있다. 상속을 할 때는 클래스 선언부에 class 클래스명(부모 클래스)의 형태로 작성한다. 상속 예시 class Person: def __init__(self, name, age, gender): self.name = name self.age = age self.gender = gender def about_me(self): print(f&quot;저의 이름은 {self.name}입니다. 나이는 {self.age}입니다.&quot;) def __str__(self): return f&quot;저의 이름은 {self.name}입니다. 나이는 {self.age}입니다.&quot; class Korean(Person): pass class Employee(Person): def __init__(self, name, age, gender, salary, hire_date): super().__init__(name, age, gender) self.salary = salary self.hire_date = hire_date def do_work(self): print(&quot;열심히 일을 합니다.&quot;) def about_me(self): super().about_me() print(f&quot;제 급여는 {self.salary}원 이구요. 제 입사일은 {self.hire_date}입니다.&quot;) 다형성(Polymorphism) 같은 이름 method의 내부로직을 다르게 작성하는 것을 말한다. 다형성 예시 class Animal: def __init__(self, name): self.name = name def talk(self): raise NotImplementedError(&quot;Subclass must implement abstract method&quot;) class Cat(Animal): def talk(self): return &#39;Meow!&#39; class Dog(Animal): def talk(self): return &#39;Woof! Woof!&#39; 가시성(Visibility) 객체 내부 변수의 접근을 막는 것을 말함 encapsulation이라고도 한다. class간의 간섭/정보공유를 최소하하는 것이 목적이다. 내부의 attribute들을 직접 활용하는 것이 아닌 interface만 사용한다. 이를 위해 __를 붙인 manglin 변수를 사용한다. private 변수는 decorator인 @property를 사용하면 접근이 가능하다. 가시성 예시 class Inventory: def __init__(self): self.items = [] self.test = &quot;abc&quot; def add_new_item(self, product): if type(product) == Product: self.items.append(product) print(&#39;new item added&#39;) else: raise ValueError(&quot;Invalid Item&quot;) def get_number_of_items(self): return len(self.items) decorator 활용 # visibility class Inventory: def __init__(self): self.__items = [] def add_new_item(self, product): if type(product) == Product: self.__items.append(product) print(&#39;new item added&#39;) else: raise ValueError(&quot;Invalid Item&quot;) # use decorator @property def items(self): return self.__items def get_number_of_items(self): return len(self.__items) DecorateFirst-class objects 변수나 data 구조에 슬 수 있는 객체를 일급객체라고 한다. 일급객체는 parameter나 return value로 사용이 가능하다. python의 모든 함수는 일급객체이다. 일급객체 예시 def square(x): return x * x def cube(x): return x * x * x def formula(method, argument_list): return [method(value) for value in argument_list] f = square f(5) Inner-function 함수내에 함수가 존재하는 것 closure : inner function을 return value로 반환하는 것 다양한 변형을 생성할 수 있다. Inner-function 예시 def print_msg(msg): def printer(): print(msg) printer() print_msg(&quot;Hello, Python&quot;) # closure 1 def print_msg(msg): def printer(): print(msg) return printer another = print_msg(&quot;Hello, Python&quot;) another() # closuer 2 def tag_func(tag, text): text = text tag = tag def inner_func(): return f&#39;&amp;lt;{tag}&amp;gt;{text}&amp;lt;/{tag}&amp;gt;&#39; return inner_func h1_func = tag_func(&#39;title&#39;, &#39;This is Python Class&#39;) h1_func() Decorator function을 꾸며주는(decorate) 역할을 한다. closure를 좀 더 간단하게 구현한 것 Decorator 예시 def star(func): def inner(*args, **kwargs): print(&#39;*&#39; * 30) func(*args, **kwargs) print(&#39;*&#39; * 30) return inner @star def printer(msg): print(msg) printer(&quot;Hello&quot;) " }, { "title": "[BoostCamp AI Tech] Day1", "url": "/posts/day1/", "categories": "NAVER BoostCamp AI Tech, Review", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech", "date": "2022-01-17 00:00:00 +0900", "snippet": "Day1 Review당신은 오늘 하루 어떻게 살았나요? 팀원들과의 인사 첫주차 모더레이터 역할로 깃허브 구성 및 양식 마련 Python 강의 Chapter 3까지 과제 Basic Math Text Processing 1 Text Processing 2 오늘의 피어세션 기본적인 인사와 룰 정하기오늘 하지 못한 것들 1주차 1일차 블로그 포스팅 정리 아는 것이 있어도 내용정리 간략하게라도 하자! 기본 과제 제출내일은 어떤 것을 할까? 첫 정규 피어세션 모더레이터 ✳️ 역할분담을 어떤 방식으로 할 지 정하기 MLops 채널을 만들어 보자!마무리 오늘 하루는 너무 빠듯했지만 내일부터는 겸손한 자세로 공부하자!" }, { "title": "[BoostCamp AI Tech / Level 1 - Python/PyTorch] Day1 - Pythonic code", "url": "/posts/day1_pythonic_code/", "categories": "NAVER BoostCamp AI Tech, Level 1 - Python/PyTorch", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech, Basic, Level 1 - Python/PyTorch", "date": "2022-01-17 00:00:00 +0900", "snippet": "Python : Pythonic codesplit, join split string을 특정기준으로 분리하여 리스트로 반환 example s = &quot;Today is good day&quot; s.split() # [&quot;Today&quot;, &quot;is&quot;, &quot;good&quot;, &quot;day&quot;] join 분리된 리스트 data를 특정 기준으로 합침 example &#39; &#39;.join(s) # &quot;Today is good day&quot; list comprehension 파이썬에서 리스트를 가장 효율적으로 다룰 수 있는 방법 list를 활용해 list를 생성하는 방식 example [i for i in range(4)] # [0, 1, 2, 3, 4] 반복문 + 조건문을 활용한 응용이 많이 사용됨enumerate, zip enumerate 데이터에 개수에 맞춰 정수도 함께 생성 example for i, v in enumerate(&quot;ABC&quot;): print(i, v) &#39;&#39;&#39; &amp;lt; output &amp;gt; 0 A 1 B 2 C &#39;&#39;&#39; zip 여러 데이터를 병렬적으로 추출 example ls1 = [1, 2, 3, 4] ls2 = [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;] for a, b in zip(ls1, ls2): print(a, b) &#39;&#39;&#39; &amp;lt; output &amp;gt; 1 a 2 b 3 c 4 d &#39;&#39;&#39; 만약 두 데이터의 길이가 다르면 적은쪽 데이터까지만 출력 두 함수를 활용하여 dict type 생성에 활용됨lambda, map, reduce lambda : 익명함수 람다 함수는 함수를 매우 간단한 형식인 약식으로 작성한 것 약식으로 작성한만큼 간단한 처리에서 활용이 많이 됩니다. example def plus(a, b): return a + b plus2 = lambda x, y : x + y # 두 함수는 같은 역할을 한다. map : sequence data에 정해진 함수를 mapping example a = list(map(str, range(10))) a # [&#39;0&#39;, &#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;4&#39;, &#39;5&#39;, &#39;6&#39;, &#39;7&#39;, &#39;8&#39;, &#39;9&#39;] reduce : 데이터 대용량 처리에 사용되며 누적계산에 많이 사용함 functools 모듈에 존재 평소 사용할 일은 별로 없지만 하둡, 스파크처럼 빅데이터 처리에서 자주 사용됨 asterisk arguments나 keyword arguments의 길이가 가변적일 때 사용한다. *args : argument list **kwargs : keyword argument list **kwargs를 사용할 때는 unpacking형태를 사용하는 것이 좋다. unpacking하기 위해서는 인자로 넘겨줄 때 앞에 *이나 **을 붙이면 된다. example def plus(*args, **kwargs): return args plus(1,2,3,4, num1=1) kwar = {&quot;num2&quot;:100, &quot;num3&quot;:200} data = [1, 2, 3] plus(*data, **kwar) 함수에 들어가는 parameter를 작성하는 순서는 다음과 같다. function(arg, kwarg, *args, **kwargs) example def func(a, b=1, *args, **kwargs): pass " }, { "title": "[BoostCamp AI Tech / Level 1 - Python/PyTorch] Day1 - Python DataStructures", "url": "/posts/day1_python1/", "categories": "NAVER BoostCamp AI Tech, Level 1 - Python/PyTorch", "tags": "Deep Learning, NAVER, BoostCamp, AI Tech, Basic, Level 1 - Python/PyTorch", "date": "2022-01-17 00:00:00 +0900", "snippet": "Python : Python DataStructureStack Last In First Out(LIFO) 성질을 갖고있는 자료구조 리스트를 통해 간단히 구현 가능 push : append(data)를 활용 pop : pop()을 활용 pop()은 return 값을 갖고 있음 example ls = [1, 2, 3] ls.append(1) # [1 ,2, 3, 1] ls.pop() # 1 Queue First In First Out(FIFO)의 성질을 갖고있는 자료구조 리스트를 통해 구현 가능 push : append(data) 활용 pop : pop(0)를 활용하면 가장 앞의 값을 pop함 example ls = [2, 1, 3] ls.append(3) # [2, 1, 3, 3] ls.pop(0) # 2 Tuple 값의 변경이 불가능한 리스트 (a, b)로 선언할 수 있고 만약 값이 1개라면 (a,)와 같이 선언 함수의 return 값으로 사용Set 집합의 성질을 갖는 자료구조 add, remove, update, discard와 같은 함수로 데이터를 추가/제거할 수 있음 union은 합집합 연산이며 |으로도 할 수 있음 intersection은 교집합 연산이며 &amp;amp;로도 할 수 있음 difference는 차집합 연산이며 -로도 할 수 있음Module : collectionsimport collections dequeue : stack과 queue의 성질을 모두 갖는 자료구조 rotate, reverse등 linked list의 성질을 지원 기존의 list보다 효율적인 연산을 보여줌 defaultdict : dict의 기본값을 지정함 신규값을 생성할 때 활용 text mining 과정에서 자주 사용됨 Counter sequence type data의 요소 개수를 반환 example ls = [1, 1, 2, 3, 4, 4, 5, 5] collections.Counter(ls) # Counter({1:2, 2:1, 3:1, 4:2, 5:2}) namedtuple C나 C++의 구조체와 같은 역할 " }, { "title": "[Review] 네이버 부스트캠프 AI Tech 3기 합격 후기", "url": "/posts/BCTest/", "categories": "Life, Career", "tags": "네이버, NAVER, BoostCamp, 부스트캠프, AI Tech", "date": "2022-01-06 00:00:00 +0900", "snippet": "네이버 부스트캠프 AI Tech네이버의 AI 엔지니어 육성 프로그램인 부스트캠프 AI Tech가 벌써 3번째 기수를 선발했다. 여느때처럼 일반전형 50명, KDT 200명으로 선발하였는데 작년 하반기부터는 KDT전형을 위한 국민내일배움카드 발급 조건이 6차학기 재학학생으로 변경되어 운이 좋게 KDT전형에 지원할 수 있었다.1기때부터 꾸준히 관심은 갖고 있었는데 3번째 도전만에 참가하게되었다. 1기때와 2기때는 자소서라는 것을 본격적으로 작성해 본 경험이 없어서 자소서의 완성도가 많이 떨어진 것을 느낄 수 있었다. 3학년 들어와서 여러가지 자소서 작성하고 과거에 쓴 걸 읽어보니… 크흠이번엔 부캠 AI Tech 3기 지원을 하는 과정을 리뷰해보겠다. 모든 지원과정들이 그렇듯이 일부 공개하기 어려운 내용들은 자세히 다룰 수 없음을 이해해주시길….서류전형부캠 서류전형 이전에 카카오 추천팀 인턴 지원이 있었다. 가장 크게 기대를 했던 인턴십이라 자소서를 상당히 열심히 준비했었다. 이때 자소서와 이전에 작성했던 자소서를 기반으로 작성해서 크게 오랜 시간이 걸리지는 않았다. 사실 문항은 지원기간동안 공개되니까 대략 공개해도 되겠지…? (문제되면 삭제될 예정)1. 부캠 지원계기 및 부캠을 선택한 이유 + AI 엔지니어 희망 이유 이 문항에서 중점적으로 적은 내용은 내가 지원한 “추천시스템” 분야의 특성을 사용해서 작성했다. 아무래도 다른 분야에 비해 국내에 배우기위한 자료가 한정적이다보니 이런 점을 들었다. 추가적으로 다른 관점을 보면 요즘 AI 관련 캠프들이 상당히 많은데, 관심을 갖고 개인적으로 공부를 해 본 사람들이라면 캠프의 커리큘럼이 상당히 빈약하다는 것을 알 수 있다. 그런 점을 언급하는 것도 좋을 것 같다.2. 개발 경험 및 인공지능 공부 경험 깃허브에 들어가보면 개인적으로 공부한 것들이 짜잘짜잘하게 있는 것을 알 수 있다. 그 외에도 상당히 고심 끝에 아주 좋은 데이터 사이언스 강의가 있어서 구매해서 수강도 했다. 조금 사담이지만 대학 입학 후 지속적으로 데이터 분야 내용을 알아보기도 하고 공부도 해서 그런 내용을 고를 때 커리큘럼을 상당히 까다롭게 본다. 솔직히 2~3년 전만해도 흔히 SNS에 자주 광고로 나오는 강의 사이트에 있는 강의들 커리는 구렸다. 근데 1년 사이에 굉장히 양질의 강의와 공부하기 어려운 내용의 강의도 많아졌다. 여튼 다시 주제로 돌아오면 이런 경험들을 언급했다. 그리고 학교에서 다중전공 진행 중인 내용도 언급했다.3. 캠프 수료 5년 후 미래의 나는 어떨 것인가? 개인적으로 제일 어려워하는 질문이다. 이게 적을 때는 그냥 참고 적는데… 뭐 10, 20년도 아니고 5년 후에 내가 얼마나 발전해 있을 지는 감이 잘 안오는 기간이라고 생각했다. 물론 그렇게 말하기 어려울 정도로 5년 안에 성장을 못할 거냐?라고 하면 그건 아니라고 자부할 수 있는데, 시기가 6차학기고 곧 7차학기라 불확실한 미래때문인가… 좀 확신이 많이 떨어지긴했다. 그래도 작성했다. 솔직히 자신은 있었다. 내가 자리를 잡으면 누구보다 뛰어나게 5년 안에 성장할 것이라는 것을. 그리고 무엇보다 “추천시스템”이라는 분야에 너무나 확고하게 길을 잡고 있었어서 해당 분야에서 발전한 것을 생각하고 작성했다.4. 자기주도 학습 및 커뮤니티 경험 자기주도 경험은 일단 1학년때 결정했던 통계학과 연계전공 내용을 언급했다. 그리고 커뮤니티 경험은 데이터 관련 강의 조교 경험을 말할려했는데 칸이 부족해서…코딩 테스트 및 AI 지식 테스트1차 역량 테스트 (20문제 : 15 + 5)AI 지식을 확인하는 문제가 15문제 출제되었고 Problem Solving으로 5문제가 출제되었다. 부캠에 도전하려는 사람에게 말하는데 AI 지식문제는 기본 공부를 안하면 풀 수 없다. 대부분의 문제를 풀기는 했는데 2~3문제는 기억이 안나는 것도 있고 모르는 것도 있어서 찍은 것도 있었다. 공부할 때 가장 도움이 된 책은 유명한 책이지만 밑바닥부터 시작하는 딥러닝 1이다.PS문제는 총 5문제가 출제되었다. 문제 자체를 설명하는 건 문제가 되기 때문에 언급을 하진 않겠다. 근데 2기때 코테를 보기도 했는데 3기 코테라 비교해서 생각해보면 이게 KDT랑 일반전형을 따로 출제하는지가 의심이 될 정도로 조금 난이도 체감차는 컸다. 일단 5문제 중 5문제 All Solve였다. 2기 모집때는 2문제 틀렸는데 떨어졌는데 이번에는 다 풀기도 했고 난이도 체감상 4솔 이상 아니면 통과를 했을 수 있을까 생각이 들긴했다. 근데 KDT는 좀 더 컷이 낮다라는 얘기가 있긴 하다. 그래도 너무 쉬웠는데…백준 티어로 말하면 실버 정도 티어면 충분히 다 풀 수 있다. 요즘 코딩테스트 트렌드에 맞춰 구현이나 탐색 위주로 푸는 것이 좋다.2차 역량테스트 (7솔 / 8문제)2차 역테는 PS 8문제가 나왔다. 1차보다 난이도 자체는 올라갔다. 2차는 실1 ~ 골드정도의 난이도로 출제되었다. 무슨 문제가 나왔었는지 기억이 잘 나지는 않는데, 적당히 집중해서 풀면 다 풀 수 있었다. (이거 약간 기만인가…) 못 푼 문제 1개는 사실 무슨 알고리즘인지도 알고 비슷한 문제를 풀기도 했는데 해당 알고리즘 연습이 부족해서 실전에서 쓰기엔 실력이 부족했다. 알고리즘은 Backtracking이었다. 솔직히 위에서 말했듯이 탐색류의 성격을 갖고 있는 건데 DFS랑 구분이 잘 안가서 어떻게 공부해야할 지가 너무 어려워서 공부를 많이 못했다.난이도 체감사 3~4문제 커트라인이 형성될 것으로 보였다. 5문제면 안정권이었을 것이다.전반적으로 보면 부캠 코테는 대비한다면 구현, 탐색 잘하면 될 것 같다. 근데 사실 요즘 코테 트렌드가 다 저쪽이긴해서 나는 구현이랑 탐색을 정말 많이 풀었기 때문에 요즘 코테는 진짜 나한테 잘 맞는 것들이 많아서 개인적으로는 항상 재밌게 푸는 것 같다.결과이래저래해서 합격했다. 이렇게 된 거 열심히 달려봐야겠다. 추천시스템 내용을 아예 모르는 것도 아니고 ML/DL도 어느정도 알기는 하는데 솔직히 체계가 잡혀있다는 느낌은 안 든다. 그래서 주변 사람들한테 “나 머신러닝 좀 할 수 있어”라고 자신있게 말할 확신이 들지 않았다. 이번 기회로 그렇게 말할 수 있는 엔지니어가 됐으면 좋겠다.그리고 무엇보다 나는 스스로 하고싶은 것에 대한 열정이 많은데, 이런 절차를 뚫고 들어온 사람들과 함께 열정적으로 할 수 있기를 기대하고 있다.마치며2021년의 마무리가 부캠 합격이라 정말 좋았다.솔직히 말해서 3학년이라 학업적으로도 너무 힘들었고 주변 일들로 스트레스도 많았고 원래도 예민한 성격에 더 예민해져서 여러가지로 맘에 안드는 일이 너무 많았었다. 심지어 카카오 추천팀 코테 부캠 코테, 기말고사 기간이 헬게이트로 겹쳐서 너무 힘들었는데 그래도 결과가 1개는 좋아서 다행이다.빨간 일정은 코테랑 합격자 발표, 노란색은 시험일정인데 진짜 너무 힘들었었다.지금 이 글을 쓰고 있는 때는 스스로 휴가기간에 들어가서 거의 2~3주는 사람을 안 만나고 혼자 하고 싶은대로, 자고 싶은만큼 지내면서 나 자신한테 집중하다보니 너무 행복한 것 같다.제일 하고 싶었던 거는 카카오 추천팀 인턴이었는데 인턴 코테 2차 떨어지고 상심이 컸었다. 기대가 컸었어서 실망이 컸었다. 이제 4학년인데 방학때 뭘 하면서 지내야할 지가 너무 막막했었다. 이미 방학 학부인턴은 마감했고 인턴은 다 떨어졌고…. 마지막 희망이 부캠이었는데 이렇게 좋은 결과가 나와서 지금 굉장히 행복하다.하고 싶었던 걸 할 수 있게 됐고 이제 나를 최대한 발전시킬 것이다. 으아자으아자~" }, { "title": "[Review] 카카오 추천팀 겨울 인턴 2021 코테 후기", "url": "/posts/KakaoRecomTest/", "categories": "Life, Career", "tags": "Kakao, 카카오, 추천팀, Recommeder System", "date": "2022-01-05 00:00:00 +0900", "snippet": "추천팀 인턴 모집카카오 추천팀 (분석) 인턴카카오 추천팀 (개발) 인턴지난해 11월에 카카오 추천팀의 인턴십 모집을 진행했다. 평소 모집과 다르게 분석팀과 개발팀 두 분류로 나눠서 진행되었다. 지원 전에 개발팀이랑 분석팀 고민을 많이 했는데 공부한 것과 평소 관심있는 분야는 데이터 분석이라 분석팀으로 지원했다.이번에 합격한 네이버 부스트캠프 AI Tech에서 추천시스템을 수료하면 아마 개발팀으로 지원해볼 수 있을 것 같다. 아마도?분석팀의 업무는 A/B Test나 통계적 원리를 통한 feature 개발과 같은 것을 진행한다. 개발팀의 업무를 봤을 때 정확한 업무가 사실 뭔지 느낌은 잘 안오지만 대략적으로 봤을 때 분석팀의 모델을 개발 및 적용하는 것으로 보인다.간단하게 말하면 분석팀은 데이터 사이언스, 개발팀은 데이터 엔지니어링 느낌인 것 같다. 오피셜은 아니다. 그냥 내 생각이래저래 각설하고 간단하게 코테 본 후기를 말하겠다. 참고로 코딩 테스트 내용은 말할 수 없기 때문에 간단하게만 언급하고 복기할 것이다.그리고 일상 카테고리니까 그냥 이래저래 주저리 주저리 적을 거라 가볍게 읽으면 좋을 것이다.1차 코딩 테스트 (4솔 / 5문제)우선 코딩테스트는 일반적으로 잘 알려져 있듯이 프로그래머스로 모집한게 아니면 카카오는 Hackerrank를 통해서 진행되었다. 이 점에 대해서는 미리 어느정도 알고 있어서 Hackerrank 사이트 연습만 살짝했다.1차 코딩테스트는 총 4시간동안 진행되었고 5문제가 출제되었다. 이 점은 이전의 추천팀 인터 1차 코테랑 거의 똑같은 양식이었다.참고로 Tip을 말해주면 일반적인 코테랑은 조금 달랐다. SW maestro 코테를 생각하면 편할 것 같다. 물론 난 소마 코테 안 봤지만 찾아본 게 있어서 뭐가 나오는지는 대충 안다.내가 백준 380문제 정도 풀고 골드 2정도니까 대충 이 정도하면 4문제 풀 수 있다. 마지막 1문제는 솔직히 시간이 좀 더 있었으면 풀었을 거 같은데 어려웠다. 체감상으로는 골드1에서도 플레랑 간당간당할듯….그리고 무엇보다 백준에 있는 문제랑 똑같은게 나왔는데 문제를 완전 다르게 적어놔서 솔직히 같은 문제인 줄 몰랐다. 테스트 케이스를 손으로 직접 해보면서 같은 문제라는걸 알았다.어쨌든 결과적으로 5문제 중 4솔했다.솔직히 카카오 코테는 프로그래머스로만 4번인가 봤는데 한번도 1차에서 합격 메일을 받아본 적이 없었다. 처음으로 1차에서 합격메일을 받았는데 세상에서 제일 기쁜 날 중 하나였던걸로 기억한다. 물론 4솔하고 떨어질거라 생각은 안했지만 그래도 눈으로 합격이 오니 느낌이 달랐다.시험기간이라 엄청 바빴는데 버틸 수 있는 원동력이 되었다.2차 코딩 테스트 (1솔?2솔?/3문제)1차 발표 후 3일 정도 후에 2차가 진행되었다. 2차관련 내용은 워낙 찾기가 힘들어서 몇가지 알아낸 정보로는 일반적인 PS문제가 나오지 않는 경우가 많고 수식 구현이 나온다는 말이 많았다. 사실 수식 구현이 나온 적이 있다길래 주피터 노트북을 하나 했긴했다.사실 이것도 자세히 말할 수는 없는데 확실한건 2차는 기존의 코테와는 색다른 코테였다. 특히 마지막 문제가 어떻게 보면 진짜 이 직무를 위한 과제였다.개인적으로 참 후회가 있기는 하는데 좀 더 열심히 해야겠다는 생각도 들었다.데이터 분석쪽으로 가려면 확실히 통계적인 기법 분석을 잘 공부할 필요가 있다고 생각이 든다.당연하게도 결과는 불합…진지하게 숙고해주신 거… 맞죠? ㅠㅠ어쨌든 여러가지로 많은 걸 깨달은 코테였다. 코테관련 하고 싶은 말이 많지만 문제가 될 수 있기 때문에 요 정도밖에 말은 못한다는 점을 이해해주길 바란다.그래도 추천팀은 분기별? 방학별로 뽑으니까 AI Tech로 실력 닦고 이번 여름인턴 노려봐야겠다." }, { "title": "[알.쓸.선.잡] 공공데이터 활용 선거 및 국회의원 정보 제공 서비스", "url": "/posts/vote/", "categories": "Project, VOTE", "tags": "projet, java script, node.js, python, open data", "date": "2021-07-27 00:00:00 +0900", "snippet": "시작하며3학년 1학기에 수강하는 데이터베이스 과목의 최종 프로젝트는 공공데이터 포털의 공공 데이터를 활용하여 서비스를 제작하는 것이었습니다. 한학기 동안 진행되는 3개월 프로젝트였으며 저는 제 동기 한명과 팀을 이뤄서 서비스 제작을 했습니다. 프로젝트를 구상하던 당시에 서울특별시장 선거를 포함한 보궐선거가 진행되던 시기라 선거정보 및 국회의원 정보를 제공하는 서비스를 제작하기로 했습니다. 공공데이터 포털에 중앙선거관리위원회의 선거 정보들과 국회 사무처 제공의 국회의원 정보 데이터를 활용했습니다. 서비스는 100% 웹 어플리케이션으로 기획하였으며 기반 백엔드언어는 Node.js를 활용하였습니다. 실제 임시 배포를 하고자 GCP를 사용하였고 서버를 기반으로 하는 GCP OS는 우분투로 하였습니다.제가 맡은 부분은 데이터베이스 구축과 이를 위한 데이터 파싱작업과 백엔드 API 제작이었습니다.디렉토리 구조├── README.md├── node_modules│ └── ...│ ...│ └── ...├── imgs│ ├── title1.png│ └── title2.png├── models├── package-lock.json├── package.json├── public│ └── src│ ├── readmore.min.js│ ├── search.png│ ├── sidebar.css│ ├── sidebar_bgd.png│ ├── stamp.png│ ├── style.css│ └── w3_sidebar.css├── router│ └── main.js├── server.js└── views ├── CandInfo.html ├── CandProm.html ├── Member.html ├── PollPlace.html ├── PrevElec.html ├── about.html └── index.html데이터 수집 및 파싱데이터베이스는 MySQL을 사용하여 저장했습니다. 이에 따라 테이블 설계를 진행했고 총 6개의 테이블이 설계되었습니다. 각 테이블에 필요한 데이터를 공공데이터 포털의 API를 통해 불러오고 파싱하는 작업을 했습니다. 데이터 파싱과 데이터프레임 제작은 재사용성 향상과 수정의 편의를 위해 모두 모듈화를 통해 제작했습니다.1. 사용 라이브러리import jsonimport urllibimport pandas as pdfrom urllib.parse import urlencode, quote_plus, unquote, quotefrom urllib.request import urlopenfrom pandas.io.json import json_normalizefrom sqlalchemy import create_engineimport pymysqlimport datetime as dtimport sklearnfrom sklearn.preprocessing import LabelEncoderimport jsonimport xmltodictimport multiprocessing as mpimport cryptographypymysql.install_as_MySQLdb()import MySQLdb모든 API는 json형식으로 로드하였지만 일부 API는 xml만 한정적으로 로드하기 때문에 xmltodict라이브러리를 추가로 사용했습니다. mysql연결을 위해 sqlalchemy와 pymysql을 활용했습니다.2. 데이터 파싱 및 데이터프레임 제작def get_election_df(pg_num = 1, num_row = 100): election_base_url = &#39;http://apis.data.go.kr/9760000/CommonCodeService/getCommonSgCodeList&#39; page_no = str(pg_num) num_of_rows = str(num_row) # maximum 34 datas queryParams = &#39;?&#39; + urlencode({ quote_plus(&#39;pageNo&#39;) : page_no, quote_plus(&#39;numOfRows&#39;) : num_of_rows, quote_plus(&#39;resultType&#39;) : &#39;json&#39;, quote_plus(&#39;ServiceKey&#39;) : service_key }) API_election_code_url = election_base_url + unquote(queryParams) # Election code load &#39;&#39;&#39; &amp;lt; election code &amp;gt; (0)대표선거명 (1)대통령,(2)국회의원,(3)시도지사,(4)구시군장,(5)시도의원,(6)구시군의회의원 (7)국회의원비례대표,(8)광역의원비례대표,(9)기초의원비례대표,(10)교육의원,(11)교육감 &#39;&#39;&#39; response = urlopen(API_election_code_url) json_str = response.read() json_object = json.loads(json_str) body = [json_object[&#39;getCommonSgCodeList&#39;][&#39;item&#39;]] election_code_data = pd.json_normalize(json_object[&#39;getCommonSgCodeList&#39;][&#39;item&#39;]) return election_code_data대부분의 데이터 파싱 및 데이터프레임 제작 모듈은 위의 코드를 기반으로 제작되어 있습니다. 기본적인 API를 통해 데이터를 로드하기 전에 필요한 query를 설정하고 서비스키와 함께 요청을 보냅니다.수신한 정보의 딕셔너리의 키값 중 ‘getCommonSgCodeList’의 ‘item’에서 데이터를 가져옵니다. 그렇게 불러온 데이터를 모두 DataFrame형태로 변환하여 데이터 전처리를 쉽게 진행하였습니다.3. 데이터 전처리def candidate_code_preprocessing(): election_code = election_code_preprocessing() data = election_code.reset_index() election_data = data[[&#39;sgId&#39;, &#39;sgTypecode&#39;]] candidate_df = pd.DataFrame() # concat all candidate df for i in range(len(election_data)): curr_id = election_data[&#39;sgId&#39;][i] curr_code = election_data[&#39;sgTypecode&#39;][i] curr_df = pd.DataFrame() if curr_code != 0: curr_df = get_candidate_df(curr_id, curr_code) candidate_df = pd.concat([candidate_df, curr_df], ignore_index=True) # candidate column change candidate = candidate_df.drop([&#39;NUM&#39;, &#39;GIHO_SANGSE&#39;, &#39;HANJA_NAME&#39;, &#39;JOB_ID&#39;, &#39;EDU_ID&#39;, &#39;EDU&#39;, &#39;CAREER1&#39;, &#39;CAREER2&#39;, &#39;AGE&#39;], axis=1) candidate.columns = [&#39;sgId&#39;, &#39;sgTypecode&#39;, &#39;cnddtId&#39;, &#39;sggName&#39;, &#39;sdName&#39;, &#39;wiwName&#39;, &#39;giho&#39;, &#39;partyName&#39;, &#39;name&#39;, &#39;gender&#39;, &#39;birthday&#39;,&#39;address&#39;, &#39;job&#39;, &#39;status&#39;] # encoding data le = LabelEncoder() for category in [&#39;gender&#39;, &#39;status&#39;]: candidate[category] = le.fit_transform(candidate[category]) # giho null value candidate[candidate[&#39;giho&#39;] == &#39;&#39;] = 1 # type matching to sql column type candidate = candidate.astype({&#39;sgId&#39;:int, &#39;sgTypecode&#39;:int, &#39;cnddtId&#39;:int, &#39;gender&#39;:int, &#39;status&#39;:int, &#39;giho&#39;:int}) candidate[&#39;birthday&#39;] = pd.to_datetime(candidate[&#39;birthday&#39;], format=&#39;%Y-%m-%d&#39;) candidate.set_index(&#39;cnddtId&#39;, inplace=True) cndd_idx = candidate[candidate[&#39;sgId&#39;]==1].index candidate = candidate.drop(cndd_idx) return candidate데이터베이스를 설계할 때 필요한 컬럼들을 미리 설정했습니다. 따라서 데이터프레임의 구조를 파악하고 필요한 컬럼을 추출하거나 상황에 따라서는 scikit-learn의 LabelEncoder를 통해 데이터 식별을 했습니다. 사실상 설계와 제작에 가장 오랜시간이 걸린 부분이 데이터 전처리 파트였습니다.서버서버는 배포를 위해 GCP (Google Cloud Platform)를 활용했습니다. 모든 API는 Node.js와 express로 제작되었으며 GET방식으로 송수신을 하게 설계하였습니다.  app.get(&quot;/CandInfo_do&quot;, function (req, res) { var sgId = req.query.date; var sggName = req.query.elecplace var name = req.query.name; if (sgId === undefined) { sgId = &#39;&#39;; } var sql = &#39;SELECT DISTINCT sgId FROM election_code; &#39; var sql1 = &quot;SELECT * FROM candidate WHERE sgId LIKE &#39;%&quot;+sgId+&quot;%&#39; AND sggName LIKE &#39;%&quot;+sggName+&quot;%&#39; AND name LIKE &#39;%&quot;+name+&quot;%&#39; ORDER BY giho ASC; &quot;; conn.query(sql + sql1, function (err, result) { if (err) { console.log(err); }else { var resultArray1 = Object.values(JSON.parse(JSON.stringify(result[0]))); var resultArray2 = Object.values(JSON.parse(JSON.stringify(result[1]))); res.render(&quot;CandInfo.html&quot;, { codes : resultArray1, candList : resultArray2 , searchSgId:sgId, searchKey: sggName}) } }) })페이지를 이동하는 API가 아닌 유저가 특정 동작을 수행할 경우 반환하는 API입니다. github - https://github.com/cow-coding/V.O.T.E" }, { "title": "[Review] Naver AI Rush 2021 1 / 2라운드 후기", "url": "/posts/AIRushEnd/", "categories": "Life, Career", "tags": "AI Rush, Naver", "date": "2021-07-07 00:00:00 +0900", "snippet": "AI Rush 2021지난 5얼 18일부터 6월 4일까지 AI Rush 2021 라운드 1이 진행되었습니다. 시험기간이 겹치지 않아서 많은 시간을 투자할 수 있었습니다. 우선 기본적으로 기밀 유지 서약을 했기 때문에 자세한 내용을 구체적으로 설명드리기는 어려운 점을 미리 알려드립니다. 우선 대회의 진행은 정해진 경로를 통해 접근이 가능한 깃허브 리포지토리와 네이버 자체의 NSML 플랫폼을 통해 진행되었습니다. 대회 진행은 네이버 AI lab의 전폭적인 지원을 통해 진행되었습니다. Naver의 Tesla V100 그래픽 카드를 인당 4장 정도까지 지원해주었습니다. (더 지원해줬을 수도 있지만 제 기술부족으로 4장까지만 동시에 진행해본 것 같습니다.) 전체적인 방식은 코드를 작성하여 네이버 서버로 보내면 서버상에서 해당 코드를 구동하는 방식이었습니다. 무엇보다 이 방식이 저에게는 많이 낯선 방식이었습니다. 자세한 코드는 공개할 수 없기 때문에 유사한 코드인 카카오 아레나의 베이스라인 코드의 형식을 예시로 보여드리겠습니다.카카오 아레나 - 쇼핑 목록 분류위의 링크로 들어가시면 아래와 같은 코드 리포지토리가 나타납니다.실제로 model 폴더에 들어가면 아무것도 없습니다만 AI Rush에서는 각 담당 PM분들이 만들어둔 베이스라인 모델이 있었습니다. 그래서 깃 리포지토리에 있는 것처럼 실행을 하면 코드가 알아서 수행이됩니다. 물론 AI Rush에서는 더 정형화된 방식으로 진행을 했습니다. 다양한 하이퍼 파라미터의 조정은 shell script상에서 옵션을 통해 조절하는 방식도 가능했습니다. 물론 베이스라인 코드를 작성하신 PM분들의 스타일에 따라 전달방식은 차이가 있었습니다.제가 아는 학부연구생 친구의 연구실도 저런 방식으로 코드를 작성하는 것처럼 보였습니다. 그래서 저렇게 하는 것을 연습하고 싶은데 어떻게 하는 건지를 잘 모르겠네요….아무튼 각설하고 이제 각 라운드 후기로 넘어가겠습니다.Round 11라운드는 총 5개의 과제가 진행되었습니다. 그 중 저는 회원&amp;amp;인증플랫폼의 ‘회원 키보드, 마우스 기록을 통한 어뷰징 탐지’를 핵심 주제로 분석하였습니다. 모든 과제가 실제 데이터를 제공받지는 못하여서 NSML상에 코드를 보내서 데이터의 구조와 형태를 대략적으로 파악했습니다. 전체적으로 머신러닝 공부를 많이 하지 못해서 가능했던 것은 새로운 층 추가. 피쳐 생성 등이었습니다. 운이 좋게 특정 피처를 추출하여서 점수가 많이 올라 13등으로 라운드 1을 통과했습니다.개인적으로 서버상에 보내서 코드를 돌리는 형식의 머신러닝 방식을 처음 겪다보니 초반 2주는 코드의 구동방식을 이해하는 것에 투자했습니다. 다행히 코드 구동방식을 이해해서 피처 추가와 일부 변형을 가하는 데 충분히 남은 1주를 소모할 수 있었습니다. 이 이상의 설명은 할 수 없는 점이 안타깝습니다.개인적으로 라운드 1을 통과하면 다행일 것이라 생각했고 각 과제별로 14등까지 진출하는 상황에서 아슬아슬하게 13등으로 통과했습니다. 등수가 낮아서 높은 성능을 보여주지 못한 것이라 생각하실 수도 있지만 전체 과제 참가자의 평균정도의 성능 향상은 보여줬습니다.Round 22라운드는 총 8개 과제가 진행되었습니다. 저는 TUNE의 ‘재생 및 메타데이터를 활용한 음악 추천’을 진행했습니다. 문제는 2라운드 진행이 6월 8일부터 7월 1일까지 진행되었다는 겁니다. 제가 현재 3학년 1학기라 OS, DB등 중요한 전공과목들이 많았는데 정확히 2라운드 시작부터 2주동안 시험기간으로 건들지를 못했습니다. 심지어 이후 일주일도 기말 프로젝트 때문에 건드리지 못했죠. 그래서 겨우 마지막 1주만 진행을 했습니다. 2라운드는 일부 과제는 훈련 데이터를 제공해주셨습니다. 그래서 실제 시각화를 진행하여 피처의 연관관계 파악도 할 수 있었습니다.문제는 제가 코드 분석을 할 시간이 없어서 제대로 원하는 모델링을 하지 못한 것이었습니다 ㅠㅠ 그리고 추천 시스템은 처음 접한 것이다보니 너무 낯설었다는 것이 문제였습니다. 기본적인 아이디어는 CF(Content-based Filtering)이었으나 이해도가 부족해서 아쉽게 살려내진 못했네요.마무리길고긴 2~3달간의 대장정이 끝났습니다. 이 대회를 참여하면서 많은 것을 배우게 될 것이라 생각했습니다. 물론 많은 걸 배웠습니다. ~가장 큰 거는 나는 굉장히 부족하구나였다는 것을….~ 우선 실제 대규모 회사에서는 어떤 방식으로 데이터 모델링과 학습을 진행하는 지 알 수 있었습니다. 또한 데이터의 구조도 파악할 수 있는 좋은 기회였습니다. 그리고 무엇보다 공부 방향성을 잡을 수도 있었습니다. 개인적으로 데이터 및 AI쪽에서도 어떤 분야로 공부를 할지도 생각을 해보게 된 것 같습니다.오늘 AI Rush Conference가 있었습니다. 그래서 1등 분들의 모델과 생각하는 것들을 봤습니다. ~일단 저보다 지식적으로 뛰어나신 분들이 넘나 많았다는 것….~ 그래서 일단 방학동안 어떤 분야의 머신러닝과 데이터 분석을 직업으로 삼을지도 고민을 많이 해봐야할 것 같습니다.공부를 확실히 해서 내년 AI Rush에서는 꼭!!! 수상권으로 들 수 있기를 빌며" }, { "title": "[Review] Naver AI Rush 2021 코딩테스트 후기 및 합격", "url": "/posts/AIRushPass/", "categories": "Life, Career", "tags": "AI Rush, Naver", "date": "2021-05-10 00:00:00 +0900", "snippet": "AI Rush 2021지난 4월 30일에 Naver AI Rush 2021 코딩테스트를 진행했다. 시험 일정이 진짜 헬게이트였어서 4월 30일에 시험이 끝났었다. 그래서 당일 3시였나? 4시쯤에 코딩테스트 입장해서 문제를 풀었다.문제 내용을 말하는 건 문제가 될 수 있기 때문에 대략적인 풀이 후기만 작성하겠다.코딩테스트 후기우선 코딩테스트는 2시간동안 진행되었다. 4월 30일 당일 아무때나 2시간동안 진행하면 되었다. 총 4문제가 출제되었고 프로그래머스를 통해 진행되었다. 작년에도 신청을 해서 코딩테스트를 봤는데 1번 문제만 붙잡고 있다가 한문제도 못 풀고 광탈한 기억이 있다…;; ㅠㅠ그때는 2학년이었고 이제는 3학년이다. 물론 3학년될 때까지 알고리즘 실력이 엄청 늘었다고 생각이 들진 않지만… 생각보다 문제풀이가 수월했다. 개인적으로 인터넷에서 요즘 코테의 방향성이 기술적 알고리즘보다 구현능력을 주요하게 본다는 것을 기억해서 구현 문제들을 많이 풀었다. 추가적으로 백준은 재미로, 프로그래머스를 좀 진지하게 했다.결과적으로 전체 예제 통과를 기준으로 3솔을 했다. 실제 채점 데이터는 제출해도 채점이 되지 않아서 문의한 결과 네이버측에서 공개를 원하지 않아서 채점은 비공개로 이루어졌다고 했다.문제 후기는 푼 순서대로 말하고 각 문제별 난이도는 체감상 백준 티어로 말하겠다.문제 1 (Bronze 2 ~ Silver 5) - Solve코딩테스트 문제 중 가장 쉬운 난이도 문제 였다. 단순 구현 문제였고 시간복잡도를 걱정할 부분도 딱히 없었다. 중간에 문제 조건 하나 빼먹어서 20분? 25분정도 걸렸다. 굉장히 여유롭게 풀어서 20 ~ 25분정도..? 따로 기술적인 아이디어가 필요한 문제는 아니었다.문제 3 (Silver 2 ~ Silver 1) - Solve2번 문제를 봤을때 생각보다 시간이 오래 걸릴 것 같아서 3번을 먼저 확인했다. 3번 문제와 1번 문제의 느낌이 비슷했다. 문제는 3번 문제가 좀 길었던 게 문제였다. 그래서 좀 자세히 읽었다. 조건을 순서대로 처리하는 문제였으며 요즘 카카오 코테에서 1문제는 꼭 나오는 형식의 순차적 조건처리 문제였다. 구현 문제였으며 효율성 테스트가 진행되었다. 이 문제는 구현으로는 큰 문제는 없었으나 중간 조건이 아이디어를 활용하지 않으면 시간이 꽤 걸릴 수 있는 문제였다. 하지만 해당 조건의 예시를 보고 특이한 점을 발견하면 시간을 단축시킬 충분히 간단한 해법을 알 수 있었다.풀이 시간은 30 ~ 40분 정도 걸렸다.문제 2 (Silver 1 ~ Gold 5) - Solve문자열 문제였다. 이 문제도 효율성 테스트가 진행됐으며 문제가 꽤 길었다. 문자열 문제는 개인적으로 항상 느끼지만 파이썬이 최고라고 생각한다. 하지만 이 문제는 내가 메인으로 사용하는 C++로 풀기에 어려움이 별로 없던 문제였다. 하지만 신경써야할 부분이 많았고 반복문을 최소화 해야하는 아이디어를 써야했다. 개인적으로 사용한 아이디어는 자료구조 시간에 배운 환형 큐 아이디어를 사용했다.풀이 시간은 50 ~ 1시간 정도였다.문제 4 (Gold 4 ~ Gold 2) - UnsolveBFS, DFS류의 문제인 것으로 파악했다. 일단 시간이 부족했던 것도 있었고 시간이 2시간 full로 사용해야 풀 수 있었을 것 같았다. 파악해야할 것이 많아서 생각보다 오래 걸렸을 것 같다.최종 후기전체적으로 두 문제는 쉬웠다. 그래서 반드시 2번 문제를 해결해야 한다고 생각했다. 그래서 2번을 어떻게든 꾸역 꾸역 풀어 내긴했다. 그래서 좀 불안하긴 했지만 3솔이면 충분히 통과할 것 같다고 생각했다.합격다른 네이버 채용 직군보다 좀 늦은 시간에 합격발표가 났다. 기분이 정말 좋았다. 처음으로 메이저 회사에서 진행하는 프로젝트급의 대회에 붙어 본 것이라….이제 빡세게 머신러닝, 딥러닝 공부를 할 예정이다. 시계열 부분을 모델링 하고 싶어서 시계열 파트도 공부할 예정이다." }, { "title": "[Cabinet Project] 사물함 배정 프로그램 개발일지 2 - 사물함 정보 및 신청 API 개발", "url": "/posts/cabinet2/", "categories": "Project, Inha Cabinet", "tags": "project, Node.js, MongoDB, BackEnd, FullStack", "date": "2021-02-07 00:00:00 +0900", "snippet": "Front End 설계 재구성보안을 위해 대부분의 API는 POST방식을 선택했다.하지만 그래도 페이지 링크를 통한 페이지 이동이 진행되면 보안 상에 문제가 있을 수 있다고 생각한다.그래서 동일 페이지에서 body값만 변경되게 설정했다.&amp;lt;html&amp;gt; &amp;lt;head&amp;gt; &amp;lt;%-include(&#39;header&#39;) %&amp;gt; &amp;lt;/head&amp;gt; &amp;lt;body&amp;gt; &amp;lt;% if(!name &amp;amp;&amp;amp; !floor) { %&amp;gt; &amp;lt;%-include(&#39;main&#39;) %&amp;gt; &amp;lt;% }else if(name &amp;amp;&amp;amp; !floor) { %&amp;gt; &amp;lt;%-include(&#39;stairs&#39;) %&amp;gt; &amp;lt;% }else if(cabinets != undefined) { if(floor == 1) { %&amp;gt; &amp;lt;%-include(&#39;first&#39;) %&amp;gt; &amp;lt;% }else if(floor == 2) { %&amp;gt; &amp;lt;%-include(&#39;second&#39;) %&amp;gt; &amp;lt;% } } %&amp;gt; &amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt;솔직히 말하면 이렇게 하면 코드가 꼬이는 것 같다는 느낌이 슬슬 들기 시작했다.일단은 별로 큰 규모의 웹사이트가 아니기 때문에 별 문제가 없으면 진행을 할 예정이다.이전 글에 설계한 로그인 페이지에서 이름과 학번 8자리를 입력하면 다음과 같이 사물함 층을 선택하는 화면으로 넘어간다. session에 name이 넘겨졌다는 것은 로그인이 되었다는 의미이다.층을 선택하면 각 층의 사물함 배치에 맞춰서 사물함 레이아웃이 나타난다.1층 레이아웃2층 레이아웃각 층의 레이아웃에 존재하는 사물함은 모두 버튼으로 구현했다. 해당 사물함 버튼을 클릭하면 신청 API로 넘어가게 된다. 이미 배정된 사물함은 빨간색으로 변경되고 버튼자체가 비활성화된다.Models 설계 재구성user DTO앞서 설계한 DTO model중에 user모델을 API 개발에 맞춰서 재구성을 했다.이전에 있던 DTO의 필드에서 새로운 필드를 추가했다.const mongoose = require(&quot;mongoose&quot;);var Schema = mongoose.Schema;var userSchema = new Schema({ studentID: { type: Number, unique: true, required: true }, name: { type: String, required: true }, isAdmin: { type: Boolean, default: false }, isPay: { type: Boolean, required: true }, useCabinet: { type: Number, default: 0 }, useFloor: { type: Number, default: 0 },});module.exports = mongoose.model(&quot;user&quot;, userSchema);사용하는 사물함의 번호와 층수를 기록하게 되어있다. 층수를 기록한 이유는 서로 다른 층에 있는 사물함 번호가 겹치는 경우가 있어서 데이터 관리의 편의를 위해서 기록했다.cabinet_info DTO사물함 정보를 저장하는 DTO model이다. 사물함 정보를 넘겨주는 역할을 하게된다.const mongoose = require(&quot;mongoose&quot;);var Schema = mongoose.Schema;var cabinetSchema = new Schema({ cabinetNumber: { type: Number, unique: true, required: true }, studentID: { type: Number, default: 00000000 }, floor: { type: String, required: true }, isUsed: { type: Boolean, default: false },});module.exports = mongoose.model(&quot;cabinet&quot;, cabinetSchema);기본적인 정보인 해당 사물함의 번호와 층수를 저장한다. 그리고 해당 사물함을 사용하는 사람이 있으면 사용자의 학번을 등록한다. 그리고 사용여부를 기록하고 기본값은 false로 모두 초기화 시켜둔다. 그 후 API를 통해서 값을 변경해주면된다.APIMain page APIapp.get(&quot;/&quot;, function (req, res) { var sess = req.session; res.render(&quot;index&quot;, { name: sess.name, studentID: sess.studentID, floor: sess.floor, cabinets: undefined, }); });메인페이지를 로드하는 API이다. 이전에 있던 메인페이지에 세션값들을 변수로 넘겨줬다.사물함을 관리하는 cabinets는 특정한 경우 아니면 사용되지 않아서 초기는 undefined로 넘겨준다.Login API우선 로그인 API를 일부 수정했다. 세션상에 현재 로그인한 학생정보를 다루면서 메인 페이지를 변경시키는 원리로 설계했기 때문에 세션에 값을 넘겨주는 부분을 추가했다.app.post(&quot;/login&quot;, function (req, res) { const notUser = &#39;&amp;lt;script type=&quot;text/javascript&quot;&amp;gt;alert(&quot;존재하지 않는 학생입니다.&quot;); window.location=&quot;/&quot;;&amp;lt;/script&amp;gt;&#39;; var sess = req.session; var currID = req.body.studentID; User.findOne({ studentID: currID }, function (err, user) { if (err) { console.error(err); return; } if (!user) { // console.log(&quot;Not found&quot;); res.send(notUser); } else { sess.studentID = user.studentID; sess.name = user.name; res.redirect(&quot;/&quot;); } }); });우선 유저를 탐색 후 값이 존재하는 경우에 세션에 값을 추가해준다. 세션 값이 변동되면서 메인 페이지로 넘어가는 API에서 넘어가면서 조건에 맞춰서 불러오는 페이지를 변경한다.First Floor API사물함 층별로 사물함 데이터를 받아와서 메인페이지에 사물함 정보를 넘겨주는 API이다.app.post(&quot;/first&quot;, function (req, res) { var sess = req.session; var currFloor = &quot;first&quot;; sess.floor = 1; Cabinet.find({ floor: currFloor }, function (err, cabinets) { if (err) { return res.status(500).send({ error: &quot;database failure&quot; }); } res.render(&quot;index&quot;, { cabinets: cabinets, name: sess.name, studentID: sess.studentID, floor: sess.floor, }); }); });주어진 데이터베이스를 탐색할 때 현재 층수로 검색을 진행한다. 그래서 해당 층수의 모든 사물함 정보를 가져온다.검색을 통해 찾아낸 모든 사물함 정보를 변수로 넘겨주어서 프론트에서 자바스크립트를 통해서 해당 사물함의 사용 여부를 확인한다. 동일한 형태로 2층 API도 처리해준다.Apply Cabinet API사물함을 신청하는 API다. 사물함 버튼을 누르면 신청 API로 넘어오게 된다.이 API에서 처리하는 것은 현재 사물함 데이터를 가져와서 사용상태를 업데이트하고 신청 유저의 상태도 업데이트하는 API다.app.post(&quot;/apply&quot;, function (req, res) { var currCabinet = Number(req.body.cabinetNum); var sess = req.session; var f; if (sess.floor == 1) { f = &quot;first&quot;; } else { f = &quot;second&quot;; } Cabinet.findOne( { floor: f, cabinetNumber: currCabinet }, function (err, cabinets) { cabinets.studentID = sess.studentID; cabinets.isUsed = true; cabinets.save(function (err) { if (err) { res.status(500).json({ result: &quot;failed&quot; }); } console.log(&quot;success&quot;); }); } ); User.findOne({ studentID: sess.studentID }, function (err, user) { user.useCabinet = currCabinet; user.useFloor = sess.floor; user.save(function (err) { if (err) { res.status(500).json({ result: &quot;failed&quot; }); } console.log(&quot;success&quot;); }); }); });사물함의 번호는 Number자료형으로 입력받는데 실제로 받아오는 값이 String 값으로 받아졌었다. 그 부분때문에 계속 데이터 탐색이 이뤄지지 않았다.혹시나해서 currCabinet의 자료형 확인을 했더니 String이어서 데이터베이스에서 탐색을 하지 못했다. 그래서 강제로 자료형을 타입캐스팅을 진행했더니 데이터베이스에서 탐색을 할 수 있었다. 그래서 해당 사물함의 이용자 학번을 현재 세션에서 갖고 있는 학번으로 설정해주고 사물함 사용 여부를 true로 변경해준다.그 후 해당 사물함 데이터 정보를 업데이트하고 동일하게 사용자 정보도 업데이트한다. &amp;lt; Cabinet Project &amp;gt; [Cabinet Project] 사물함 배정 프로그램 개발일지 1 - 로그인 화면, API 개발" }, { "title": "[Cabinet Project] 사물함 배정 프로그램 개발일지 1 - 로그인 화면, API 개발", "url": "/posts/cabinet1/", "categories": "Project, Inha Cabinet", "tags": "project, Node.js, MongoDB, BackEnd, FullStack", "date": "2021-02-04 00:00:00 +0900", "snippet": "시작하며이번에 학과에서 회장을 맡게되면서 해결해야할 여러 문제들이 많이 생겼다. 사실 이 문제들은 모든게 코로나때문에 생겨난 문제들이다.작년부터 갑자기 모든 것들이 비대면으로 진행되면서 학생들을 대상으로하는 여러 업무처리가 대부분 대면으로만 있던 것들이 결국 발목을 잡았다.그 중 가장 큰 문제 중 하나가 바로 학생들의 사물함을 배정하는 일이었다. 사실 랜덤으로 지정해도 된다고 생각할 지 모르지만 엄연히 본인들이 자치비를 지불하고 사용하는 것이라 본인들이 선택권을 갖는 것이 맞다고 생각한다.그래서 이번 기회에 사물함을 본인이 골라서 선택하는 웹 어플리케이션을 개발하자고 생각했다. 전체적인 풀스택 개발경험을 가질 수 있는 토이 프로젝트라고 생각되기도 한다.고려할 점우선 학과 학생들을 대상으로 하는 어플리케이션이니 고려해야할 것들이 많다. 우선 가장 걱정되는건 상대는 컴퓨터공학과 학생들이라는 점이다.허술한 웹 어플리케이션을 만들고 배포해서 사용한다면 이런 점을 뚫어내려하는 사람들이 존재할 것이 분명하다. 보안적인 측면도 어느정도 고려를 해야할 것 같다. 어떤 서버언어를 사용할 것인가?spring boot, Node.js, Golang이 내가 머릿속에 리스트에 올려둔 서버개발 언어들이다. 하지만 spring boot는 책을 보면서 공부하는 것도 굉장히 힘들어 하는 상황이다. 일단 진입장벽이 높다는 점은 누구나 인정하는 것이니 말이다.그래서 요즘 핫하다는 Golang을 해보려 했으나…. Golang으로 web서버를 개발하는 것은 자료를 찾기가 생각보다 어려웠다. 그리고 무엇보다 서버언어를 선택하는 과정에서 나는 서버개발자는 뭘 개발하는거지…?라는 생각이 강했다.결국 웹 어플리케이션에서 많이 사용하는 Node.js로 선택했다. 데이터베이스는 어떤 것을 선택할 것인가?Node.js를 공부하는 도중에 mongoose를 활용해서 MongoDB와 연결하는 것을 자주 접했다. 이전에 프로젝트로 MongoDB를 구축해 본 경험도 있어서 MongoDB로 선택했다. 과 자치비 지불여부를 확인해서 약 1000여명의 데이터베이스를 구축해야한다.작년 1학기 기준으로 우리 과는 전체 인원이 약 960여명이 재학중이었다. 평균적으로 1학기에 1000여명이 재학 등록을 한다는 것이다. 물론 나는 작년과 올해 학생회 간부(前부비상대책위원장, 올해 現비상대책위원장)라 과자치비 납부 여부 자료에 접근이 가능하다. 해당 자료가 엑셀 자료인데, 그 데이터를 파이썬으로 읽어와서 파싱 후 MongoDB에 저장할 예정이다. (이 부분은 좀 테스트를 해봐야 할 듯)어플리케이션 설계우선 개발은 Node.js로 진행하고 Node.js 개발 프레임워크인 Express.js를 사용할 것이다. 전체적인 프론트엔트는 ejs를 사용할 예정이다.프로젝트 구조서버 개발과정을 알아보던 도중 폴더와 파일 구조 설계가 중요하다는 것을 알았다. 하지만 이 부분을 자세히 알기엔 우선은 개발을 목적으로 한 것이라 대략적인 폴더 구성을 했다. 완성된 후 설계 구조를 공부하고 리팩토링을 해야할 듯 하다.Cabinet-Project├─ server.js├─ config├─ models│ ├─ cabinet_info.js│ └─ user.js├─ package.json├─ node_modules├─ public│ ├─ images│ └─ css│ └─ style.css├─ routes│ └─ index.js└─ views기본적이 프로젝트 구조는 이렇게 설정했다. 초기 구조는 이렇게 했고 이후에 변하게 될 경우 변한 지점 포스팅에 남겨둘 예정이다.대부분의 API는 routs/index.js에 구현을 할 예정이다.Front End우선 프론트 엔드 개발 옵션은 자세한 html과 CSS 구현은 너무 시간소모가 커서 bootstrap을 사용했다. 또한 모든 파일이 ejs이므로 중간중간에 코드를 삽입해서 진행할 예정이다. 부트스트랩으로 레이아웃을 가져오고 일부 레이아웃을 살짝 수정했다.따로 계정 로그인이 필요하진 않으므로 이름과 학번정보만 가져오면된다.Back End이번 토이프로젝트의 핵심이다. 우선 개발 1일차에 활용한 백엔드 옵션은 아래와 같다.Modules개발 1일차에 사용한 모듈은 아래와 같다. 원래 hash key를 활용한 비밀번호 암호화를 위해 crypto를 설치했지만 실제로 사용할 지는 아직 고려중인 상황이다. 굳이 사용한다면 학번정보를 암호화하지 않을까 생각중이다. express body-parser express-session mongoose해당 옵션들을 npm으로 설치해서 진행했다.$ npm install --save express body-parser express-session mongooseModels이번 프로젝트에 사용되는 DTO들을 선언한 폴더이다. 기본적으로 학생정보를 갖는 user.js와 사물함 소유 정보를 저장하는 cabinet_info.js로 구성되어있다.&amp;lt; user.js &amp;gt;const mongoose = require(&quot;mongoose&quot;);var Schema = mongoose.Schema;var userSchema = new Schema({ studentID: { type: Number, unique: true, required: true }, name: { type: String, required: true }, isPay: { type: Boolean, required: true }, isAdmin: { type: Boolean, default: false },});module.exports = mongoose.model(&quot;User&quot;, userSchema);학생 정보를 저장하는 DTO는 간단한 구성이다. 학번, 이름은 실제로 학생들이 입력한 데이터와 비교하기 위해 존재하고 isPay는 과자치비를 지불했는지 여부이다. 이후에 사물함을 선택하는 경우에 과자치비 지불을 하지 않은 사람은 선택이 불가능하게 할 것이다. 이 부분 데이터는 사용자에게 제공되거나 노출되는 것은 아니다.user.js의 정보는 user collection에 저장된다.index.js (API)1일차에 개발한 API는 로그인 페이지 로드 API와 로그인을 진행할 때 데이터베이스에서 해당 입력 데이터를 잘 가져오는지 확인하는 작업을 진행했다.&amp;lt; Login Page API &amp;gt;app.get(&quot;/&quot;, function (req, res) { var sess = req.session; res.render(&quot;index&quot;);});로그인 페이지를 기본적으로 로드하는 API이다. 요청의 session값이 가져와지면 해당 값을 저장한다. 향후 로그인 이후 페이지에서 사용될 경우를 대비해서 선언했다.views 폴더에 있는 index.ejs파일을 render로 응답해주는 간단한 API이다 페이지 로드이므로 GET방식을 사용했다.&amp;lt; Create User API &amp;gt;app.post(&quot;/create/user&quot;, function (req, res) { var user = new User(); user.studentID = req.body.studentID; user.name = req.body.name; user.isPay = req.body.isPay; user.save(funciotn (err) { if (err) { console.error(err); res.json({result : 0}); return; } res.json({result:1}); });});POST 방식으로 전달하지만 실제 배포에는 구현하지 않을 API이다. 단순 테스트와 개발을 위해서 만든 API이다.&amp;lt; Login API &amp;gt;app.post(&quot;/login&quot;, function (req, res) { const notUser = &#39;&amp;lt;script type=&quot;text/javascript&quot;&amp;gt;alert(&quot;존재하지 않는 학생입니다.&quot;); window.location=&quot;/&quot;;&amp;lt;/script&amp;gt;&#39;; var currID = req.body.studentID; var currName = req.body.name; User.findOne({studentID: currID}, function (err, user) { if (err) { console.error(err); return; } if (!user) { res.send(notUser); } else { console.log( `student id: ${user.studentID}, name: ${user.name}, isPay:${user.isPay}` ); res.redirect(&quot;/&quot;); } })})우선 로그인 테스트 개발이라 콘솔에 데이터가 잘 불러와 지는지 테스트를 진행했다.1일차 소감우선 백엔드 개발자는 뭘 개발하는지 하나도 몰랐다. 막연히 백엔드 개발자, 데이터 사이언티스트가 꿈이라고 했지만 백엔드 개발자는 그냥 데이터베이스 다루는 거 말곤 아는게 없었다.그런데 이번에 토이 프로젝트를 진행하면서 가장 중요한 백엔드 개발자의 업무를 알았는데, 바로 API 개발이었다.이번 토이 프로젝트가 좀 더 백엔드 개발자로서 기본 소양을 키울 수 있는 기회가 되면 좋겠다." }, { "title": "[BOJ] 7576 토마토", "url": "/posts/7576/", "categories": "Algorithm, BOJ", "tags": "Algorithm, BOJ, BFS", "date": "2021-01-25 00:00:00 +0900", "snippet": "https://www.acmicpc.net/problem/7576문제를 요약하자면 익은 토마토가 토마토 상자에 있는 모든 토마토를 익히는 데 까지 얼마나 걸리는지 확인하는 것입니다.바이러스가 퍼지듯이 동시다발적으로 퍼지는 것이라 주요 논점은 DFS가 아닌 BFS를 활용해야한다는 점입니다.문제 핵심 IDEA토마토 문제의 핵심아이디어라면 당연히 위에 말했듯이 BFS입니다.하지만 제일 큰 고민점은 BFS를 진행할 때마다 날짜 카운트를 해주는 건 아니라는 것입니다.BFS에서 다루는 것중 하나인 Level을 체크해주는 것이 곧 날짜 카운트와 동일합니다.제가 문제 풀때 발생했던 실수와 해결책은 생각 흐름 파트에서 설명해드리겠습니다.생각흐름기초 흐름토마토는 사실 BFS문제로 유명한 문제입니다. 그런 이유로 보자마자 BFS설계를 한 것도 있습니다.하지만 그걸 몰라도 BFS임을 알 수 있는 이유는 다음과 같습니다. [ BFS인 이유 ] 1. 전체 토마토상자(map)를 모두 탐색을 해야 한다.2. 단순하게 상자를 탐색하는 것이 목적이 아닌 &quot;최단 시일&quot;을 찾아야 하는 게 핵심이다.3. 토마토가 퍼지는 것은 인접한 위치가 &quot;동시 다발&quot;적으로 진행된다.흔히 BFS에서 Level을 체크하는 것은 최단 경로를 찾는 느낌과 비슷하기에 level을 계산해주는 코드로 작성을 해줍시다.while문 안에 for문을 큐의 크기만큼 반복해주고 for문의 종료 이후 날짜 카운트를 올려주면 됩니다.토마토 상자는 map으로 하겠습니다.첫번째 접근 맨 처음에는 2차원 배열을 2개 만들어서 원본 map은 1이 있는 위치만 저장해서 확인하는 역할을 하고 사본 map에서 탐색을 진행하려고 했습니다.근데 생각해보니까 굳이 그렇게 할 이유가 없더군요. 사실 메모리 관리도 중요하니까요.그래서 저는 start라는 큐에 시작 위치 i, j를 저장하고 그걸 BFS에서 사용하는 Q에 넣었습니다.사실 이 부분도 메모리 낭비죠.두번째 접근 일반적인 BFS를 진행하는 데 문제가 발생했습니다.백준 예제 중 하나가 익는 데 시간이 없으므로 0일이 나와야 하는 데 무조건 레벨 체크를 하다 보니 1이 나왔습니다.이를 해결하기 위해서 BFS가 한 번이라도 돌면 카운터를 올려줘서 카운터가 0이 아니면 날짜 카운트를 올려주게 했습니다.문제가 발생한 예제는 BFS를 한 번도 돌리지 않기 때문에 날짜 카운트가 skip 됩니다.세번째 접근 이제 날짜를 어떻게 기록할지가 문제가 발생했었습니다.가장 처음 작성한 코드는 단순히 날짜를 출력하게 했습니다. 그랬더니 1일이 더 많게 나왔습니다.이유를 확인해보니까 마지막에도 BFS가 돌게 되지만 맵상에 모든 토마토는 꽉 차거나 접근 불가능한 영역에 있게 됩니다.이런 경우 결국 레벨을 체크한 것이므로 마지막 레벨이 1번 더 돌게 됩니다. 그래서 날짜 카운트가 1이 추가가 된 것입니다.그걸 해결하기 위해서 그냥 BFS가 돌 때마다 해당 레벨의 날짜 카운터를 맵에 기록해서 채웠습니다.사실 0만 아니면 익은 거로 취급이 가능하니까요.최종적으로는 map을 이중 for문으로 쭉 훑으면서 최고 값을 답으로 기록하면 됩니다.정답 코드#include &amp;lt;iostream&amp;gt;#include &amp;lt;queue&amp;gt;#include &amp;lt;queue&amp;gt;#include &amp;lt;algorithm&amp;gt;#include &amp;lt;utility&amp;gt;#include &amp;lt;vector&amp;gt;using namespace std;typedef pair&amp;lt;int, int&amp;gt; P; // P(i, j) = (y, x)const int dirx[4] = {0,0,1,-1};const int diry[4] = {-1,1,0,0};int map[1001][1001];int N, M;bool visit[1001][1001];int day = 0;int cnt = 0;queue&amp;lt;P&amp;gt; start;queue&amp;lt;P&amp;gt; Q;void BFS() { visit[Q.front().first][Q.front().second] = true; while (!Q.empty()) { int qsize = Q.size(); for (int i = 0; i &amp;lt; qsize; i++) { int x = Q.front().second; int y = Q.front().first; Q.pop(); if (map[y][x] == 0) // In BFS, It means adjenct tomato map[y][x] = day; for (int i = 0; i &amp;lt; 4; i++) { int nx = x + dirx[i]; int ny = y + diry[i]; if (ny &amp;gt;= 0 &amp;amp;&amp;amp; ny &amp;lt; M &amp;amp;&amp;amp; nx &amp;gt;= 0 &amp;amp;&amp;amp; nx &amp;lt; N) { if (map[ny][nx] == 0 &amp;amp;&amp;amp; !visit[ny][nx]) { cnt++; visit[ny][nx] = true; Q.push(P(ny, nx)); } } } } if (cnt != 0) { day++; } }}int main() { cin &amp;gt;&amp;gt; N &amp;gt;&amp;gt; M; // map making for (int i = 0; i &amp;lt; M; i++) for (int j = 0; j &amp;lt; N; j++) { cin &amp;gt;&amp;gt; map[i][j]; if (map[i][j] == 1) start.push(P(i,j)); // start point save queue } int iter = start.size(); Q = start; BFS(); int ans = 0; bool answerflag = true; // if ans = true for (int i = 0; i &amp;lt; M; i++) { for (int j = 0; j &amp;lt; N; j++) { if (map[i][j] == 0) { answerflag = false; break; } ans = max(ans,map[i][j]); } if (!answerflag) break; } if (cnt == 0) { ans = 0; } if (!answerflag) { cout &amp;lt;&amp;lt; -1 &amp;lt;&amp;lt; &quot;\\n&quot;; } else { cout &amp;lt;&amp;lt; ans &amp;lt;&amp;lt; &quot;\\n&quot;; }}일부 좀 불필요한 코드들도 있다고 생각을 하지만 제가 생각한 방법은 이게 최선인 것 같네요.나중에 좀 더 실력이 늘면 달라질 수도…?GitHub - https://github.com/cow-coding/algorithm/blob/master/BOJ/C%2B%2B/7576.cpp" }, { "title": "[BOJ] 1987 알파벳", "url": "/posts/1987/", "categories": "Algorithm, BOJ", "tags": "Algorithm, BOJ, BFS", "date": "2021-01-25 00:00:00 +0900", "snippet": "https://www.acmicpc.net/problem/1987문제에서 요구하는 것은 (0,0) 인덱스에서 시작해서 한 칸씩 상하좌우로 이동할 때,중복된 알파벳을 피해서 최대한 이동 가능한 횟수를 찾는 것입니다.이전에 했던 토마토처럼 탐색형 문제이면서 DFS를 이용하는 문제입니다.( BFS도 가능할 겁니다.)문제 핵심 IDEA알파벳 문제는 경로 탐색형 문제입니다. 해결 방법은 크게 DFS, BFS 두 가지가 있습니다.확인해줘야 하는 것은 알파벳의 중복성, 이미 확인한 경로인가?입니다.그리고 이전 값으로 돌아올 경우 이전에 지난 데이터는 원상 복구시켜줘야 합니다.생각흐름기초 흐름문제를 보자마자 DFS로 풀이를 잡았습니다.신경 써줘야 하는 조건은 다음과 같습니다. [ 조건 ] 1. 이미 지난 알파벳인가? -&amp;gt; bool array로 26개 만들고 처리해줍시다. 2. 방문한 곳인가? -&amp;gt; B, DFS에서 기본인 bool visit array로 처리합니다.첫번째 접근 처음에는 간단하게 생각해서 단순 DFS로 풀려고 했습니다.하지만 문제가 발생한 것은 이미 지나온 경로와 데이터를 처리할 수 없었습니다.예를 들어, 하나의 경로에서 뻗어나가는 4가지 경우의 수(상, 하, 좌, 우) 중에 2번째(하 이동)에 들어간 DFS가 끝났다고 해서 그 값이 최대라는 보장이 없습니다.이후 3번째(좌 이동), 4번째(우 이동) DFS에서 더 많은 이동이 나올 수도 있기 때문입니다.두번째 접근 그래서 중복을 없애면서 모든 경우를 다 확인하는 형식의 DFS를 진행해야겠다고 생각을 했습니다.이와 같은 연습하기 가장 좋은 게 N과 M 시리즈입니다.DFS를 빠져나온 뒤 진행되는 코드에 해당 인덱스의 알파벳과 방문을 false로 바꿔주고, 경로를 -1 해주는 것입니다.즉, 내가 처리한 경로를 없던 셈 치고 진행을 하겠다는 겁니다.그 후 그냥 if문 조건이 넘어가면 max값 비교를 하게 짰더니 문제가 발생하더군요.그냥 조건이 안 맞으면 무조건 max값을 처리해 버리지 뭡니까…세번째 접근 해결 방법은 단순했습니다. 상하좌우 어디로도 이동이 불가능하면 그때 max를 처리해주면 됩니다.즉 이동 반복문의 iterator가 최댓값 3에 도달한 경우, max 비교를 해주면 됩니다.정답 코드#include &amp;lt;iostream&amp;gt;#include &amp;lt;cstdlib&amp;gt;#include &amp;lt;cmath&amp;gt;#include &amp;lt;string&amp;gt;using namespace std;int dirx[4] = {-1, 0, 0, 1};int diry[4] = {0, -1, 1, 0};int R, C;int cnt;int result = -1;char map[20][20];bool visit[20][20];bool alphabet[26]; // A = 0 ~ Z = 25void dfs(int x, int y) { visit[x][y] = true; cnt++; for (int i = 0; i &amp;lt; 4; i++) { int nx = x + dirx[i]; int ny = y + diry[i]; if (nx &amp;gt;= 0 &amp;amp;&amp;amp; nx &amp;lt; R &amp;amp;&amp;amp; ny &amp;gt;= 0 &amp;amp;&amp;amp; ny &amp;lt; C) { if (!visit[nx][ny]) { char next_alphabet = map[nx][ny]; if (!alphabet[next_alphabet - &#39;A&#39;]) { alphabet[next_alphabet - &#39;A&#39;] = true; dfs(nx, ny); alphabet[next_alphabet - &#39;A&#39;] = false; visit[nx][ny] = false; cnt--; } } } if (i == 3) result = max(cnt, result); }}int main() { string input; cin &amp;gt;&amp;gt; R &amp;gt;&amp;gt; C; for (int i = 0; i &amp;lt; R; i++) { cin &amp;gt;&amp;gt; input; for (int j = 0; j &amp;lt; C; j++) { map[i][j] = input[j]; } } alphabet[map[0][0] - &#39;A&#39;] = true; dfs(0, 0); cout &amp;lt;&amp;lt; result &amp;lt;&amp;lt; &quot;\\n&quot;;}좀 깔끔하게 풀은 거 같네요.삼성 A형을 따기 위해서 요즘 B, DFS 위주의 공부를 하는 중인데 익숙지가 않습니다…;;GitHub - https://github.com/cow-coding/algorithm/blob/master/BOJ/C%2B%2B/1987.cpp" }, { "title": "[Go] Go로 만들어보는 자료구조 - 배열(Array)", "url": "/posts/DSArray/", "categories": "DataStructure, Go", "tags": "DataStructure, Array, Go", "date": "2021-01-10 00:00:00 +0900", "snippet": "시작하며2학년 2학기에 들은 수업에서 우연찮게 Go를 접하게 되었다. 생각보다 다른 언어들에 비해 배우기도 어렵지 않았고 사용하는 것도 어렵지 않았다. 큰 틀이 C와 유사해서 그런것 같기도 하고…Go언어를 사용하는 개발자가 가장 연봉이 높다는 말도 있고(Go를 쓰는 개발자가 구글 개발자라 그렇다는 말도…), 꿈은 데이터 사이언티스트지만 많은 언어를 배우고 다루는 것에 관심이 많다보니 Go를 익숙하게 만들기 위해 Go로 자료구조를 만들어 보자.배열 (Array)배열의 기본 구조는 크기를 선언하고 인덱스 접근, 데이터 삽입, 데이터 제거, 특정 인덱스 값 변경 등등을 기능으로 가져야 한다.자료구조의 정확한 내용은 C++로 작성한 자료구조 포스트를 참고하길 바란다.구현우선 고민이 되었던 것은 Go에는 클래스가 없다. Go로도 객체지향 프로그래밍을 할 수 있다. 클래스는 없지만 다행히 구조체는 존재한다. 활용방법과 메소드 선언도 거의 유사하다. 일단 직접 부딪혀보자.구조체 선언type Array struct { n int // array save count arraySize int // full size arr []int}본격적으로 배열의 역할을 할 구조체이다. 배열의 메인 역할은 arr이 맡게된다. 생성자를 만들어보자. Go에서 생성자는 조금 다른 방식으로 구현한다.생성자func NewArray(arraySize int) *Array { a := Array{} a.n = 0 a.arraySize = arraySize a.arr = make([]int, arraySize) for i := 0; i &amp;lt; arraySize; i++ { a.arr[i] = 0 } return &amp;amp;a}Go는 c++에서 new나 delete같은 함수가 없다. (사실 있다. 나중에 알아보니까 new(struct)로 작성하면 모든 필드가 0으로 초기화되어 생성된다.) 근데 여기서는 크기를 지정하는 생성자가 필요하므로 생성자를 만들어준다. new()는 기본생성자라고 보면 된다.그래서 생성자를 만들 때 포인터를 활용해서 변환된 객체를 반환해주는 코드를 작성한다. 그래서 return값이 객체 포인터 값으로 지정된다. 이 자체로 new와 같은 역할을 한다. delete는 해당 객체를 nil로 만들면된다.인덱스 참조 함수func (a *Array) at(idx int) int { return a.arr[idx]}맨 처음에는 리시버의 대상(a *Array)을 포인터 리시버가 아닌 value 리시버(a Array)의 형태로 모든 메소드 함수를 작성했다. 그러다보니 값을 설정하는 함수에서 해당 구조체의 값을 바꾸지 못했다.특정 값을 변경하고 싶다면 포인터 접근을 잊지말아야한다.add, set함수func (a *Array) add(idx int, data int) { if a.arr[idx] == 0 { a.arr[idx] = data a.n++ }else { for i := a.n; i &amp;gt; idx; i-- { a.arr[i] = a.arr[i-1] } a.n++ a.arr[idx] = data }}func (a *Array) set(index int, data int) { if a.arr[index] != 0 { a.arr[index] = data }else { fmt.Println(0) }}기본적인 추가 함수는 한칸씩 값을 밀어내는 것이다. 그래서 맨 끝에 저장된 곳에서부터 뒤로 값을 댕겨온다고 생각하면 편하다.set함수는 간단하다 그냥 해당 인덱스 직접 접근하면 끝이다.remove 함수func (a *Array) remove(idx int) { if a.arr[index] == 0 { fmt.Println(0) }else { ret := a.arr[index] for i := index; i &amp;lt; a.n; i++ { a.arr[i] = a.arr[i + 1] } a.arr[a.n] = 0 a.n-- fmt.Println(ret) }}지우는 것은 add의 역방향 연산으로 진행하면된다. 뒤에서 앞으로 댕겨오면 되는 것이다.자세한 전체 코드는 깃허브를 참고하길 바란다." }, { "title": "[Medipush Project] Database 구축", "url": "/posts/database1/", "categories": "Project, Medipush", "tags": "Medipush, project, NoSQL, MongoDB, database, AWS EC2, colaboratory, crawling, beautifulsoup", "date": "2020-08-26 00:00:00 +0900", "snippet": "시작하며데이터베이스를 저장할 AWS 연결도 끝났고 이제 데이터베이스를 구축할 차례다.데이터베이스 구축은 파이썬 크롤링을 통해서 만들 예정이다. 파이썬의 dictionary와 list를 적절히 잘 활용하면 단순히 코드로만 JSON형식의 파일을 만들 수 있다.이번 프로젝트의 데이터베이스 MongoDB기반의 NoSQL이므로 총 2가지 document을 갖게된다.환자의 정보를 저장하는 document인 patient와 의약품 정보를 저장하는 collection인 medicine document이다.Database modeling데이터베이스 구축 코딩 이전에 데이터베이스 모델링을 우선 작성해봤다.앞에서 말했듯이 2가지 document를 모델링 해야한다.아직 데이터베이스 모델링과 데이터베이스의 지식이 많이 부족해서 이해가 안되는 모델링 방식도 있을 수 있으니 양해바란다.patient document modeling초기 모델링우선 환자정보를 저장할 patient document를 모델링해보자.초기에 디자인한 patient document는 다음과 같았다.{ _id: &amp;lt;ObjectId1&amp;gt; patient_info: { name: &quot;patient_name&quot;, SSN: &quot;200825-1234567&quot;, pregnant: false }, take_med:[ { prod_name: &quot;medicine_name&quot;, take_session: [true, true, false] } ]}간단하게 데이터의 구조를 설명해보면 환자의 인적사항을 저장하는 patient_info document를 Embedded 형식으로 저장하려고 했다.그리고 take_med는 현재 데이터를 갖는 환자가 복용하고 있는 의약품의 정보가 배열로 저장되는 document이다. prod_name은 의약품의 이름이다.take_session은 크기가 3으로 고정인 배열로 유지가 되는데, 이유는 일반적으로 약국에서 약을 제공할 때 아침, 점심, 저녁으로 제공하는 경우가 많다. 각각의 배열 인덱스는 순서대로 아침, 점심, 저녁인지 아닌지를 저장하는 bool type 변수이다.이렇게 초기 모델링을 진행했는데, 약사가 사용하는 환자정보 입력은 웹을 통해서 진행을 하게된다. 이때, 웹에서 입력받은 데이터를 묶어서 입력하는 과정에 문제가 발생해서 patient_info를 따로 Embedded document형식이 아닌 밖으로 분리하는 방식을 택했다.나중에 생각해보면 굳이 해당 환자 인적정보를 다른 document에 입력할 이유가 없으므로 Embedded형식으로 만들 이유가 없었다.최종 모델링{ _id: &amp;lt;ObjectId1&amp;gt; name: &quot;patient_name&quot;, SSN: &quot;200825-1234567&quot;, pregnant: false, take_med:[ { prod_name: &quot;medicine_name&quot;, take_session: [true, true, false] }, ... ]}medicine document modeling이번 프로젝트에서 가장 큰 크기를 차지하게 될 document다.사실 실전으로 들어가면 대한민국 국민이 5천만이라서 patient가 가장 큰 document겠지만, 임시구현에서는 의약품 정보를 담고있는 medicine document가 가장 크다.최종 모델링{ _id: &amp;lt;ObjectId2&amp;gt; prodName: &quot;medicine_name&quot;, ingredient: [&quot;ingr1&quot;, &quot;ingr2&quot;, ...], cautionInfo: [ { ingr: &quot;caution_ingr_name&quot; dur: &quot;dur_type&quot; }, ... ]}크롤링 사이트 탐색medicine 도큐먼트는 웹 크롤링을 통해서 데이터를 구축하려고했다. 그래서 의약품 데이터를 찾던 도중 약학정보원을 크롤링 사이트로 선정을 했었다. 문제는 약학정보원의 의약품 검색 페이지 양식이 asp파일로 구성되었고 페이지 분석이 너무 어렵고 대부분이 jQuery의 형태를 띄고 있어서 beautifulsoup를 활용해서 크롤링이 너무 어려웠다.그래서 약학정보원에 문의해서 데이터베이스를 제공받을 수 있는지 알아봤는데, 1년에 50만원으로 계약을 진행하는 것이라 이번 프로젝트에서 사용하기에 무리로 판단해서 다른 사이트로 선회했다.약학정보원에 문의를 하던 도중에 식약처에서 제공하는 의약품정보 데이터가 있을 수 있다고 하여서 식약처에서 의약품을 관할하는 사이트인 의약품안전나라 웹 페이지를 분석했다.다행히도 의약품안전나라에서 제공하는 의약품 상세정보는 페이지가 GET방식을 활용했다.페이지의 링크를 자세히보면 itemSeq라는 변수로 페이지가 이동함을 알 수 있다. 여기서 itemSeq는 의약품안전나라 의약품검색에서 ‘품목기준코드’정보임을 쉽게 알 수 있었다.그래서 모든 데이터를 수집하기는 무리이므로 의약품 중에서 완제품만 품목코드를 크롤링했다. 품목코드 크롤링은 R을 활용해서 csv파일로 저장했다.(그 당시 코딩할땐 코딩한 기억이 있는데 좀 멀쩡한 상태로 코딩하려니 갑자기 안된다…. ㅠㅠ)사이트 분석우선 의약품안전나라의 의약품 상세정보 사이트를 분석해보면의약품 제품명과 유효성분을 가져올 수 있다. 이 정보는 의약품 정보에 저장하기엔 충분히 유효한 정보다.이 부분은 DUR이라고해서 의약품을 복용할 때 주의할 점을 말해주는 사용정보이다. 의약품 정보에 충분히 기록할만하다. 하지만 이 부분은 모든 의약품이 갖는 정보가 아니다. 그래서 조건문으로 이 부분이 존재하는 경우에만 데이터를 가져오기로 한다.Crawling codelibrary and module import and connect AWSimport pymongoimport requestsimport reimport csvimport urllib.parsefrom bs4 import beautifulsoupfrom urllib.request import urlopenfrom multiprocessing import Pool, Managerconn = pymongo.MongoClient(&#39;AWS IP address&#39;, uesrname=&#39;username&#39;, password=&#39;password&#39;, authMechanism=&#39;SCRAM-SHA-1&#39;)medipush = conn.medipushMongoDB를 연결하고 데이터를 넣을 라이브러리인 pymongo를 필요로한다. 그리고 크롤링을 위해서 beautifulsoup를 같이 연결해줘야한다. re와 parse는 페이지 분석 후 데이터를 넣기위해서 문자열 데이터를 정제할 때 사용한다.MongoClient함수를 활용해서 연결해주면된다. 보안이 없으면 단순히 IP만 적으면 되지만 보안 연결이 된 경우에는 Robo 3T에 연결할 때 처럼 알맞은 정보를 입력해주면된다.csv readf = open(&#39;med_code.csv&#39;, &#39;r&#39;, encoding=&#39;utf-8&#39;)rdr = csv.reader(f)lists = []for line in rdr: lists.append(line)f,close()이 부분은 ‘품목기준코드’를 리스트로 저장해서 웹페이지 변동에 사용한다. lists의 내부 데이터를 보면 2차원 배열 형태라서 후에 조금 조절을 해줘야한다.Crawling총 품목코드 개수가 44469개인데 크롤링과정에서 너무 오랜시간이 걸려서 1만개 데이터베이스만 구축했다.manager= Manager()med_data_list = manager.list()cnt = manager.list()index = listsmedicine = medipush.medicineerror_index = manager.list()def crawling(i): cnt.append(i) print(&#39;count : &#39;, len(cnt)) print(&#39;Now data number : &#39;, index[i][0]) url = &quot;https://nedrug.mfds.go.kr/pbp/CCBBB01/getItemDetail?itemSeq=&quot; + index[i][0] res = requests.get(url) soup = BeautifulSoup(res.content, &#39;html.parser&#39;) med_basic = soup.select_one(&#39;div.r_sec tr&#39;) # HTTPError인 경우 데이터 처리 if med_basic == None: error_index.append(i) return # product name save med_dict = dict() med_name = med_basic.select_one(&#39;td&#39;).text input_med = med_name.replace(&#39;\\n&#39;, &#39; &#39;) med_dict[&#39;prodName&#39;] = input_med.strip() # 병용금기 전용 유효성분 저장 리스트 ingr_list = list() # product ingredient save if soup.select_one(&#39;#scroll_02 &amp;gt; h3&#39;): med_ingredient = soup.select_one(&#39;#scroll_02 &amp;gt; h3&#39;).text med_ingredient = med_ingredient.replace(&#39;유효성분 : &#39;, &#39;&#39;) ingredient = med_ingredient.split(&#39;,&#39;) med_dict[&#39;ingredient&#39;] = list(set(ingredient)) ingr_list = list(set(ingredient)) # product caution save ## dur info exist if soup.select_one(&#39;#scroll_06 &amp;gt; table&#39;): temp = soup.select(&#39;#scroll_06 &amp;gt; table &amp;gt; tbody &amp;gt; tr &amp;gt; td&#39;) dur_info = list() tmp = list() for item in temp: tmp.append(item.text.split(&#39;\\n&#39;)) data_insert = False name = list() dur_list = list() for item in tmp: dur_data_input = False input = dict() if &quot;DUR성분(성분1/성분2..[병용성분])&quot; in item: dur_ingredient = item[item.index(&#39;DUR성분(성분1/성분2..[병용성분])&#39;) + 1] if &quot;DUR유형&quot; in item: dur_type = item[item.index(&#39;DUR유형&#39;) + 1] if item[item.index(&#39;DUR유형&#39;) + 1] == &#39;병용금기&#39;: dur_data_input = True data_insert = True # dur_mix를 저장하는 딕셔너리 생성 ## 문자열 파싱 파트 dur_main = re.sub(&#39;\\[\\w*\\]&#39;, &#39;&#39;, dur_ingredient) dur_mix = re.search(&#39;\\[\\w*\\]&#39;, dur_ingredient) dur_mix = dur_mix.group().replace(&#39;[&#39;,&#39;&#39;).replace(&#39;]&#39;,&#39;&#39;) for t in ingr_list: if dur_mix in t: # 부분일치가 dur_mix에 있는 경우 dur_main을 넣어줘야함 mix = True break elif dur_main in t: # 부분일치가 dur_main에 있는 경우 dur_mix에 넣어줘야함 mix = False break else: mix = False if mix: if dur_main not in name: name.append(dur_main) input[&#39;ingr&#39;] = dur_main input[&#39;dur&#39;] = &#39;병용금기&#39; else: continue else: if dur_mix not in name: name.append(dur_mix) input[&#39;ingr&#39;] = dur_mix input[&#39;dur&#39;] = &#39;병용금기&#39; else: continue elif item[item.index(&#39;DUR유형&#39;) + 1] != &#39;분할주의&#39;: data_insert = True dur_data_input = True input[&#39;ingr&#39;] = dur_ingredient input[&#39;dur&#39;] = dur_type if dur_data_input: dur_list.append(input) if data_insert: med_dict[&#39;cautionInfo&#39;] = dur_list medicine.insert_one(med_dict)if __name__ == &#39;__main__&#39;: med_data_list = manager.list() start_time = time.time() pool = Pool(processes = 10) pool.map(crawling, range(0, 10000, 1)) pool.close() conn.close() print(&#39;----------------------------------------\\n&#39;) print(&#39; \\n&#39;) print(&#39;All data search end and Make a database!\\n&#39;) print(&#39;total time : &#39;,time.time() - start_time) print(&#39;error product code list : &#39;, error_index) print(&#39; \\n&#39;) print(&#39;----------------------------------------\\n&#39;)프로젝트 진행하면서 가장 많이 수정된 코드다. 우선 최종적인 코드를 간단하게 리뷰해보면 다음과 같이 구성되어 있다. 제품 이름 설정 제품 성분 설정 제품 주의 내용 설정 병렬 연산으로 크롤링 속도 증가 1번 코드 1번과 2번은 모든 의약품 대상으로 처리를 해준다. 이때 일부 페이지가 서버 로드가 느려서 HTTPError가 뜨는 경우가 있어서 1번 데이터에서 None 값이 잡히게 된다.이 부분이 None값이 되면 insert_one에서 오류가 나서 프로세스가 종료가 된다. 그래서 해당 부분이 None일 때는 따로 개별 처리 단계로 넘기기 위해서 데이터 인덱스를 저장시키고 스킵시켰다. 3번 코드3번 코드는 굉장히 길다. 그래서 자세히 리뷰를 해보겠다. 우선 크롤링 페이지에는 우리가 크롤링 하려는 위험 정보 즉, DUR정보가 있는 의약품이 있고 없는 의약품이 있다. 그래서 무조건적으로 해당 부분을 select_one을 하면 에러가 나서 프로세스 종료가 난다. 그래서 조건문 처리를 진행했다. 크롤링으로 가져온 데이터를 보면 ‘\\n’으로 데이터가 구분되어 있음을 알 수 있다. 그래서 수집 데이터 중에서 필요한 부분만 인덱스를 가져오기 위해서 문자열 파싱을 진행했다. 변수의 역할을 설명해보면 data_insert는 이후에 자세히 설명을 하겠지만 DUR중에 유일하게 데이터 양식이 다른 데이터가 있는데, 그 데이터를 구분하기 위한 bool타입 변수다.name은 DUR성분을 보면 중복된 값들이 존재하는데, 중복데이터 처리를 위한 리스트다. 서버관리 파트 팀원의 요구로 자신이 가진 성분을 제외하고 병용금기 데이터를 가져와달라고 요청했다. 우선 병용금기 데이터를 보면 ‘성분1[성분2]’와 같은 방식으로 구성되어있는데, 성분1과 성분2의 문자열을 분리하는 작업을 진행해야했다. 그 후 성분을 저장한 리스트의 데이터를 탐색하면서 if ~ in문으로 성분1이 일치하는지, 성분2가 일치하는지를 확인해본다. 그 후에는 각주 설명처럼 코드를 연결해주면된다. 4번 코드 multiprocessing을 활용해서 병렬 연산처리를 진행했다. 기존의 크롤링으로 진행하면 100개 데이터 처리시 210초 정도 걸린 반면 8개 pool을 사용해서 크롤링할 경우 30초까지 줄일 수 있다. 그래서 10개 정도로 데이터 연산을 진행했다.이 방식을 상대방 입장에선 서버 공격으로 느낄 수 있으므로 주의해야한다. " }, { "title": "[Medipush Project] AWS connect", "url": "/posts/medipush/", "categories": "Project, Medipush", "tags": "Medipush, project, NoSQL, MongoDB, database, AWS EC2", "date": "2020-08-21 00:00:00 +0900", "snippet": "시작하며기초적인 MongoDB 사용도 알았으니 이제 AWS 서버랑 연결해서 데이터베이스 구축을 해보자.전체적인 구축 방식은 Google colab을 활용해서 의약품안전나라에 있는 의약품 정보를 크롤링해서 데이터베이스를 구축할 예정이다.데이터베이스 저장위치는 AWS EC2를 활용할 예정이고 팀원 중 서버담당을 맡는 친구가 이미 AWS EC2 세팅을 끝내놨다. 나는 그 서버에 권한받고 이제 서버에 몽고DB 설치를 진행할 것이다. 그리고 마무리로 Robo 3T에 연결까지 하는 내용을 다룰 것이다.AWS-MongoDb install아마존 AWS 사이트에 들어가서 AWS EC2인스턴스를 만들었다. AWS EC2 인스턴스로 서버 만드는 방법은 인터넷 검색을 추천한다. 또는 추후에 포스팅을 추가로 진행할 예정이다.서버 생성할 때 발급받은 private key를 잘 보관하고 서버 형식에 맞춰서 터미널에서 연결을 한다. private key가 pem형식이라 윈도우보다 리눅스기반에서 연결이 더 원활하다. 나는 맥북으로 연결하기 때문에 바로 연결을 해보겠다.AWS connect in Terminal (Linux / Unix / Mac OS X)연결할 때, 중요한 정보들이 노출될 위험이 있으므로 코드만 작성하겠다.우선 private key가 있는 폴더로 이동한다.# Amazon Linx AMI기준ssh -i key_name.pem ec2-user@public_DNS_address# Ubuntu AMI기준ssh -i key_name.pem ubuntu@public_DNS_addressAmazon Linux AMI와 Ubuntu AMI로 나눠지는데 각각의 경우 연결방식과 내부에서 몽고DB설치 방식이 다르다.그리고 몽고DB연결과 설치를 위해서는 AWS EC2내부의 보안그룹에서 사용하고 있는 AWS의 보안그룹에 다음과 같이 추가를 해줘야한다.포트범위 27017은 MongoDB가 기본적으로 설치되는 포트번호이다. 해당 부분에 일종의 permission을 내려줘야 연결이 가능하다.MongoDB install위에서 연결을 했으면 아래와 같이 나오게 된다. Amazon Linux AMI Ubuntu AMI Amazon Linux AMI우선 Amazon Linx AMI 기준 MongoDB설치 방법을 설명하겠다.$ sudo su$ vi /etc/yum.repos.d/mongodb-org.4.4.repo설치할 때 원활하게 하기위해 sudo su를 활용해서 root권한을 얻어온다. 그 후 vim을 활용해서 위의 명령어를 이용해서 파일을 만들어준다. 이때, 가장 중요한 것은 파일이름에 설치하고자하는 MongoDB 버전에 맞춰서 org.[version].repo로 이름을 지어줘야한다.해당 파일이 열리면 아무것도 없는데, 그 안에 다음과 같이 입력해준다.[mongodb-org-4.4]name=MongoDB Repositorybaseurl=https://repo.mongodb.org/yum/amazon/2013.03/mongodb-org/4.4/x86_64/gpgcheck=1enabled=1gpgkey=https://www.mongodb.org/static/pgp/server-4.4.asc여기서 가장 중요한 것은 내부에 연결된 링크의 버전과 파일에 있는 버전이 동일해야 문제가 없이 잘 돌아간다는 점이다. 다른 형식은 동일하므로 MongoDB가 향후에 버전이 올라가면 4.4적힌 부분만 버전에 맞춰서 바꾸면 된다.그 후 esc -&amp;gt; :wq를 눌러서 나온다.$ yum install mongodb-org명령어를 입력하면 뭐가 막~~~뜨는데 Complete! 나올 때까지 가만히 냅두면된다.그 후 service mongod start로 시작된 것이 확인되면 설치가 된 것이다.Ubuntu AMIUbuntu AMI는 조금 더 간단하다면 간단하고 어렵다면 더 어렵다.아래 코드를 순서대로 입력해주면된다.$ sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 0C49F3730359A14518585931BC711F9BA15703C6$ echo &quot;deb [ arch=amd64,arm64 ] http://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/3.4 multiverse&quot; | sudo tee /etc/apt/sources.list.d/mongodb-org-3.4.list$ sudo apt-get update$ sudo apt-get install -y mongodb-org이렇게 하면 설치가 완료가 된다. 실행은 sudo service mongod start로 해주면된다.AWS-Robo 3T connect이제 AWS와 Robo 3T를 연결해보겠다.이전에 한 것처럼 server연결화면에서 create를 누르고 address에 AWS IP v4를 입력해주면된다.이때, AWS 서버에서 mongodb를 실행시킨 상태여야한다. 혹시 제대로 입력했는데, 오류가 발생하는 경우가 있다.사실 위 사진은 local컴퓨터 연결 오류라서 127.0.0.1로 나오는데, AWS 연결오류면 IP위치에 AWS IP가 나온다.이때는 터미널의 AWS console에서 다음과 같이 입력해서 일부 정보를 수정해줘야한다.sudo vi /etc/mongod.conf위 코드는 Linux나 Ubuntu 공통이다. 해당 파일에 들어가서 아래쪽으로 이동해보면 bindIP부분이 존재할텐데 해당 부분의 IP가 127.0.0.1로 되어있을 확률이 있다. 해당 부분의 IP를 0.0.0.0으로 변경하고 다시 연결해보면 연결이 된다.AWS-Robo 3T 보안연결AWS를 활용하다보면 보안연결을 위해서 서버 담당자가 보안 설정 연결을 하는 경우가 존재한다.보통 AWS console에서 mongodb console 실행시키는 경우에 사용하는 root 비밀번호를 활용하면된다위의 목록에서 Address부분에 AWS의 IP v4 address를 입력해주고 Authentification으로 이동한다.database부분은 admin으로 해두고 user named은 일반적으로 root를 적는다.그리고 password에는 몽고DB 콘솔 연결 비밀번호를 적어주면된다." }, { "title": "[Medipush Project] MongoDB", "url": "/posts/MDB1/", "categories": "Project, Medipush", "tags": "Medipush, project, NoSQL, MongoDB, CRUD, database", "date": "2020-08-19 00:00:00 +0900", "snippet": "시작하며몽고DB를 설치하고 활용하는 과정에서 GUI프로그램으로 Robo 3T를 사용하게된다. 기본적으로 파이썬 프로그래밍이나 DB를 다룰 때 사용되는 콘솔 명령어를 Robo 3T에서도 사용이 가능하지만, GUI 프로그램이므로 클릭으로도 형성이 가능한 경우가 많다.MongoDB 데이터베이스 생성방식비콘솔 방식위의 사진처럼 PC서버를 연결하고 우클릭을 하면 Create Database가 나온다. 이를 클릭해서 이름을 짓고 만들 수 있다. 데이터베이스 만들 때는 이 방식을 추천한다.무슨 에러인지는 모르겠는데 콘솔방식으로 만들면 Refresh해도 생성된 DB가 안 보인다…콘솔 방식설정한 서버를 우클릭하면 Create Database말고 Open Shell이 나온다. 여기서 다음과 같은 명령어를 실행하면된다.use database_name사실 위의 명령어는 특정 데이터베이스로 이동할 때 사용하는 명령어기도 하다. 없는 경우에는 생성후 이동하게된다.MongoDB의 CRUD방식일반적으로 데이터베이스의 기본은 CRUD라고한다. C : Create R : Read U : Update D : Delete이 4가지를 자유자재로 다뤄야하고 효율적으로 운영하게 만드는 것이 핵심이다.RDBMS들과 마찬가지로 NoSQL에도 CRUD를 위한 명령어가 존재한다. GUI를 통해 만드는 방법도 있지만, 그건 뭐 굳이 설명할 필요가 없으니 명령어로 설명하겠다.Create (document, collection 생성)MongoDB가 사용하는 데이터 저장방식은 JSON방식을 사용한다. 그래서 저장하는 내용물을 document 즉, 문서라고 부르게된다.일반적으로 RDBMS에서는 table을 만든다고한다. NoSQL에서 테이블과 같은 역할을 하는 것이 바로 collection이다.기존의 RDBMS에서는 테이블의 생성과 변경이 이뤄지는데, 생성에는 CREATE, 테이블 수정에는 ALTER를 사용했다. 하지만 NoSQL에서는 ALTER의 역할이 필요가 없어지는데, 이유는 NoSQL은 RDBMS와 다르게 컬럼의 규격 제한이 없기 때문이다. 저장하는 document별로 컬럼 값의 변경이 가능하기 때문이다.이제 NoSQL과 RDBMS의 문법을 비교하면서 콘솔 명령어를 설명하겠다.RDBMSCREATE TABLE emp ( id INT NOT NULL AUTO INCREMENT, user_id VARCHAR(30), age INT, status CHAR(1), PRIMARY KEY (id));NoSQL (use database로 특정 db로 이동한 후로 가정)db.createCollection(&quot;emp&quot;)db.createCollection(&quot;emp1&quot;, { capped: true, size: 10000})// collection을 만들 때, 저장사이즈 크기를 고정시킬 수 있다. 옵션부여db.emp.insertOne( { user_id: &quot;abcd1&quot;, age: 24, status: &quot;A&quot; } )db.emp.insertMany( [ { user_id: &quot;abcd2&quot;, age: 26, status: &quot;A&quot; }, { user_id: &quot;abcd3&quot;, age: 18, status: &quot;B&quot; }, { user_id: &quot;bcde4&quot;, age: 46, status: &quot;C&quot; }, { user_id: &quot;abcd5&quot;, age: 33, status: &quot;B&quot; } ])위의 명령어는 모두 같은 역할을 한다. 재밌는 점은 기존의 RDBMS는 테이블을 만들 때, 규격을 정하고 그에 맞는 데이터들만 집어넣는다. 하지만 NoSQL에서는 우선 collection을 이름만 지어서 만든다. (필요에 따라서 옵션을 부여할 수 있다.) 그리고 그 후에 넣는 컬럼들의 데이터 타입은 들어가는 데이터의 형식에 따라서 결정된다.여기서 좀 특이한 점이라면, RDBMS에서는 명시해주는 PRIMARY KEY의 역할을 자동적으로 _id라는 변수가 맡게된다.Read (데이터 탐색)문법 작성역시나 RDBMS에서 사용하는 방식과는 꽤 많이 다르다. 사실 document를 읽는 문법은 RDBMS보다 직관력이 많이 떨어지고 손이 많이 간다고 생각된다. 이유는 비교 문법때문인데, 같은 탐색문을 이번에도 적어보겠다.RDBMSSELECT user_id, age FROM emp WHERE age &amp;gt; 30;NoSQLdb.emp.find( { age: { $gt: 30 } }, { user_id: 1, age: 1 })아마 문법으로 보고 물음표 한 3개 정도 뜨면 정상인거다. 직관력이 많이 떨어진다는 이유는 바로 비교기호를 사용하지 않는다는 점이다. 마치 HTML문서에 작성하듯이 코드를 활용해서 비교문을 작성한다.그리고 역시나 문법이므로 작성순서가 존재하는데, 앞에 나오는 중괄호는 조건문의 역할이고 뒤에 나오는 중괄호는 옵션이라고 부르고 탐색하고자 하는 컬럼을 1로 명시해준다. 만약 모든 데이터를 탐색한다면, 조건만 적거나, 내용이 빈 중괄호만 작성하면된다.이때, _id변수는 0으로 지정하지 않는 이상 무조건 데이터를 표시한다. 위에서 말했듯이 _id의 역할은 데이터를 구분해주는 역할을 하게 된다.문법 종류데이터 탐색을 할 때 사용하는 문법이 까다롭다는 것은 예시를 통해서 알았으니 이제 사용 문법을 좀 정리할 필요가 있다. $eq : = $in : 해당 데이터를 갖는 자료 $gt : &amp;gt; $gte : &amp;gt;= $lt : &amp;lt; $lte : &amp;lt;= $ne : != $nin : 해당 데이터가 없는 자료 db.collection_name.distinct(“column_name”) : 특정 컬럼의 종류를 반환AND문법은 단순히 콤마(,)로 이어서 작성하면 되지만 OR은 형식이 존재한다.db.emp.find( { $or: [ { status: &quot;A&quot; }, { age: 20 } ]})Update (데이터 수정)데이터 수정도 역시나 데이터베이스에서는 중요하다. 기존에 RDBMS를 좀 공부해 본 사람이라면 대충 느낌이 오겠지만, 데이터 수정에서는 조건이 활용되므로 탐색과정에서 활용한 조건 문법들을 활용해야한다.RDBMSUPDATE emp SET status = &quot;B&quot; WHERE age &amp;gt; 40;NoSQL// Many는 One으로 바꾸면 데이터 1개만 찾는다.db.emp.updateMany( { age: { $gt: 40 } }, { $set: { status: &quot;B&quot; } })이거 보면 진짜 NoSQL은 $를 좋아하는 것 같다. 그리고 마치 굉장히 불편해 보인다고 생각이 들지만 자세히보면 꼭 그렇지도 않다. 필요한 묶음은 모두 같은 중괄호에 묶어버리면 되기 때문이다. 그래서 익숙해지면 생각보다 눈에 잘 들어오게 카테고리화가 된다.Delete삭제 구문은 크게 어렵지않다. 역시나 RDBMS에서도 특정 조건으로 삭제가 가능한 것처럼 여기서도 조건문으로 활용하거나, { }로 작성해서 모든 데이터 삭제가 가능하다.RDBMSDELETE FROM emp WHERE status = &quot;A&quot;;NoSQL// One이면 데이터 1개만 삭제db.emp.deleteMany( { status: &quot;A&quot; })" }, { "title": "[Data Structure] DFS (깊이 우선 탐색)", "url": "/posts/DFS/", "categories": "DataStructure, C++", "tags": "DataStructure, Graph, Graph Search, DFS, Depth First Search", "date": "2020-08-01 00:00:00 +0900", "snippet": "그래프 탐색 (Graph Search)앞서서 그래프를 배웠다. 트리에 트리를 탐색하는 트리 순회가 있었다면, 그래프는 그래프 탐색이 존재한다. 그래프 탐색 방법에는 DFS와 BFS가 존재한다.이번 글에서는 DFS에 대해서 이야기할 예정이다.그래프 탐색을 배우는 이유는 그래프의 이동 경로나 생김새를 파악하기 위해서 사용한다. 탐색을 통해서 그래프에 사이클이 몇개고, 전체적인 묶음이 몇개인지 알 수 있기때문이다.DFS VS BFSDFS는 깊이 우선 탐색(Depth-First-Search)의 약자이고 BFS는 너비 우선 탐색(Bredth- First-Search)의 약자이다. 이름에 탐색 방식이 들어가 있다. DFS는 깊게 들어가는 것을 우선으로 하는 탐색방식이고 BFS는 넓은 분포, 즉 한 level씩 탐색을 하는 방식이다.단순히 탐색만을 한다면 어떤 방식을 써도 상관없지만 특정 경우를 필요로하면 상황에 따라서 DFS와 BFS를 선택해야한다.우선 BFS는 최단 경로를 찾을 때 사용한다. 일반적인 그래프는 가중치가 없으므로 BFS로 최단경로를 찾을 수 있고 가중치가 존재하는 가중치 그래프에서는 BFS를 변형하는 다익스트라(Dijkstra) 알고리즘을 사용하면 최단 경로를 찾을 수 있다.이렇게 BFS는 사용처가 명확한데, DFS는 사용처가 약간 불분명하다고 느껴질 수 있다. 사실 DFS는 BFS에 비해 코딩이 직관적이라 그래프 뿐만 아니라 경로 찾기 문제를 풀 때 자주 사용한다. (최단경로가 아닌 단순 경로) 하지만 명확하게 DFS가 사용되는 곳을 말하면 백트래킹(Backtracking) 에서 자주 사용된다. 백트래킹은 특정 경우의 수를 찾기 위해 불가능한 경우의 수를 가지치기 해야하므로 하나의 방식을 끝까지 확인하면서 가능성을 파악해야한다. 그렇기 때문에 DFS 방식을 사용한다.이 글은 알고리즘 글이 아니라 자세히 설명하지는 않겠다.Depth-First-Search(깊이 우선 탐색)란?DFS의 원리를 간단하게 설명하면 한 우물 파기 이다. 우물의 끝을 볼 때까지 그 우물만 파는 방식이다. 그렇기 때문에 그 우물이 끝이 있어야만 탐색이 끝나고 끝이 없을 수도 있다면 DFS를 사용하면 안된다. 그래프를 가지고 DFS방식을 설명해보겠다.전반적으로 트리처럼 생겼지만 트리는 그래프의 특수한 경우를 트리라고한다. 그래서 위의 트리같이 생긴 그래프를 탐색한다고 보면 보통 시작 정점을 주는데 일반적으로 1번 정점이나 0번 정점을 시작 정점으로 설정한다. 전체적인 탐색 방식은 트리의 전위순회 방식과 같다고 생각하면 편하다.1번 정점을 탐색을 완료 했다면 현재 정점의 인접하는 정점들을 확인한다. 탐색 순서를 작은 숫자를 우선으로하면 다음 탐색 위치는 2번 정점이 된다. 이제 2번 정점을 탐색하게 된다.2번 정점에 들어오게 되면서 2번 정점을 탐색한다. 현재 정점 탐색이 완료되었으므로 현재 정점의 인접 정점들을 확인한다. 인접 정점에는 1번과 4번, 5번 정점이 있다. 여기서 1번 정점은 이미 탐색을 했으므로 1번 정점으로 탐색방향을 잡으면 안된다. 그래서 4번과 5번 중 하나로 탐색 방향을 잡게된다. 작은 정점으로 탐색하므로 4번 정점으로 탐색한다.4번 정점으로 들어왔다. 이제 인접 정점으로 탐색을 진행하는데, 2번과 7번 중 탐색을 하지 않은 7번 정점으로 이동한다.7번 정점 탐색을 마쳤다. 7번 정점의 인접정점 중에서 탐색을 하지 않은 정점은 5번 정점이 되게 된다. 5번으로 탐색을 하게된다.여기서부터가 가장 중요하다. 5번 정점까지 탐색이 끝났다. 5번 정점의 인접 정점에서는 더 이상 방문할 수 있는 정점이 없다. 이 경우에는 바로 이전에 탐색한 정점으로 이동 한다. 만약 이전에 탐색한 정점에도 방문 가능 정점이 없으면 방문이 가능한 경우가 나올 때까지 반복해서 돌아간다. 5-&amp;gt;7-&amp;gt;4-&amp;gt;2-&amp;gt;1로 가게 되면 1번 정점에서는 인접 정점으로 이동이 가능하게 된다. 1번 정점의 인접 정점 중 탐색하지 않은 3번 정점으로 이동하자.3번 정점 탐색이 끝났다. 이전 방식과 동일하게 인접 정점 중 탐색하지 않은 정점으로 이동하자.6번 정점까지 탐색이 되었다. 인접 정점에서 탐색이 가능한 정점이 없으므로 이전처럼 바로 전에 탐색한 정점으로 돌아간다. 끝까지 탐색할 수 있는 정점이 없다면 탐색을 종료하게 된다.전체적으로 DFS를 모아서 보면 아래와 같이 탐색을 한다.DFS의 예시DFS는 미로탐색에서 길 찾기 방식에서 사용된다.시작에서 미로찾기를 수행하다가 막다른 길을 만날 때까지 해당 경로를 탐색하는 것이다.사진에서는 직관적으로 잘 보이기 위해서 방문 경로를 지웠는데, 실제로는 visit이 false가 되는 것이 아니라 갈림길이 나올 때까지 재귀를 탈출하게된다.DFS implementation (DFS 구현)DFS 구현을 위한 class를 만들면 아래와 같이 만드는데, 사실 실전에서 Problem solving(알고리즘 문제풀이)을 할 때는 아래처럼 풀지 않는다. 우선 이론적 코딩으로 설명을 하고 2가지 방식의 실전적 코딩을 알려주겠다.Class codingGraph making &amp;amp; prototype#include &amp;lt;vector&amp;gt;#include &amp;lt;iostream&amp;gt;#include &amp;lt;algorithm&amp;gt;using namespace std;class Graph() {private: int N; vector&amp;lt;vector&amp;lt;int&amp;gt;&amp;gt; adj_list; vector&amp;lt;bool&amp;gt; visit;public: Graph() { N = 0; } Graph(int n) { N = n + 1; adj_list.resize(N); visit.resize(N); } void sortGraph() { for(int i = 0; i &amp;lt; N; i++) sort(adj_list[i].begin(), adj[i].end()); } void addEdge(int u, int v) { adj_list[u].push_back(v); adj_list[v].push_back(u); } void dfs(); void dfs(int curr);};그래프 만드는 함수는 이전 글에서 그대로 가져왔다. dfs함수가 2가지가 있는데, 파라미터가 없는 함수는 dfs 시작 함수이고 파라미터가 있는 함수는 실제로 dfs를 수행하는 함수이다. 전위순회와 동일한 방식으로 돌아가므로 재귀함수로 구현을 할 수 있다.private에 visit이라는 bool타입 변수를 만들어준다. 이 배열의 역할은 해당 정점의 방문여부를 결정해준다. 그래서 그래프 생성과 동시에 N의 크기로 만들어준다.DFS functionvoid dfs() { fill(visit.begin(), visit.end(), false); dfs(1)}void dfs(int curr) { visit[curr] = true; cout &amp;lt;&amp;lt; &quot;Vertex &quot; curr &amp;lt;&amp;lt; &quot; visit\\n&quot;; for(int i = 0; i &amp;lt; adj_list[curr].size(); i++) { int next = adj_list[curr][i]; if (!visit[next]) dfs(next); }}파라미터가 없는 dfs시작 함수의 역할은 방문여부를 결정하는 visit벡터를 미방문(false)으로 초기화해준다.그리고 여기서는 1번 정점을 시작 정점으로 해서 1을 넣었지만 실제로는 저 부분은 시작 정점에 맞춰서 진행한다.실제로 dfs를 수행하는 함수의 시작은 현재 정점을 방문으로 바꿔주는 것이다. 그 후 해당 정점에 연결된 정점들을 돌면서 탐색을 진행해준다. 여기서 조건문으로 방문이 이뤄지지 않은 정점만 dfs 재귀를 수행해준다.여기까지가 이론적 부분이고 이제부터는 실전적 부분 설명이다. 관심없으면 이 밑은 읽지 않아도 괜찮다.Problem Solving Coding위의 class 방식은 앞서서 말했지만 실전 알고리즘 문제에서는 잘 사용하지 않는다. 실전 알고리즘 문제는 방식에 따라서 크게 2가지 방식으로 코딩을 하는데, 이건 문제에서 그래프를 어떤 방식으로 주어지냐에 따라 코딩 방식이 바뀐다.위 문제는 인접리스트 방식으로 그래프를 구현하는 경우에 해당하는 문제이다. 인접행렬로 구현해도 괜찮지만 인접리스트로 하는 것이 좀 더 편할 수 있다.위 문제처럼 특정한 지도(map)를 주는 방식은 인접행렬로 구현하는 것이 유리하게 된다. 위에서 언급한 미로찾기 방식처럼 탐색을 하기에 굉장히 편한 그래프 형태이다.인접리스트 풀이 코드#include &amp;lt;vector&amp;gt;#include &amp;lt;iostream&amp;gt;using namespace std;vector&amp;lt;vector&amp;lt;int&amp;gt;&amp;gt; map;vector&amp;lt;bool&amp;gt; visit;int N, M; // 전체 정점 개수, 전체 간선 수void dfs(int curr) { visit[curr] = true; cout &amp;lt;&amp;lt; &quot;Visit &quot; &amp;lt;&amp;lt; curr &amp;lt;&amp;lt; &quot; vertx\\n&quot;; for(int i = 0; i &amp;lt; map[curr].size(); i++) { int next = map[curr][i]; if (!visit[next]) dfs(next); }}int main() { cin &amp;gt;&amp;gt; N; // 시작 정점이 1인 경우로 설정 map.resize(N + 1); visit.resize(N + 1); for (int i = 0; i &amp;lt; M; i++) { int u, v; cin &amp;gt;&amp;gt; u &amp;gt;&amp;gt; v; map[u].push_back(v); map[v].push_back(u); } dfs(1);}이미 앞서서 이론적으로 설명한 기반이 인접리스트라 코드는 동일하다. 단지 문제풀이때는 전역변수로 설정해서 모든 값이 정수는 0으로, bool타입은 false로 자동 초기화가 된다.인접행렬 풀이 코드#include &amp;lt;iostream&amp;gt;#include &amp;lt;utility&amp;gt;#include &amp;lt;vector&amp;gt;using namespace std;typedef pair&amp;lt;int&amp;gt; P; // x, y 좌표 저장vector&amp;lt;P&amp;gt; start;int dirx[4] = {-1, 1, 0, 0};int diry[4] = {0, 0, -1, 1};bool map[1001][1001];bool visit[1001][1001];int N; // 전체 정점 개수, 전체 간선 수void dfs(int x, int y) { visit[x][y] = true; cout &amp;lt;&amp;lt; &quot;x : &quot; x &amp;lt;&amp;lt; &quot; , y : &quot; &amp;lt;&amp;lt; y &amp;lt;&amp;lt; &quot; visit \\n&quot;; for (int i = 0; i &amp;lt; 4; i++) { int nx = x + dirx[i]; int ny = y + diry[i]; if (nx &amp;gt;= 0 &amp;amp;&amp;amp; nx &amp;lt;= N &amp;amp;&amp;amp; ny &amp;gt;= 0 &amp;amp;&amp;amp; ny &amp;lt;= N) { if (!visit[nx][ny]) dfs(nx, ny); } }}int main() { int u, v; // 시작 정점 역할 cin &amp;gt;&amp;gt; N; // N x N 행렬로 만든다고 가정한다. // map이 입력으로 주어질 때로 가정 for (int i = 0; i &amp;lt; N; i++) { for (int i = 0; i &amp;lt; N; i++) { int curr; cin &amp;gt;&amp;gt; curr; // 보통 입력은 1, 0으로 주어진다. map[i][j] = curr; } } cin &amp;gt;&amp;gt; u &amp;gt;&amp;gt; v; dfs(v, u);}인접행렬 방식의 그래프는 필요한 변수가 여러 개가 존재한다. 좌표의 형식으로 경로를 저장하기 때문에 좌표를 이동해주는 변수가 필요하다. 그 변수가 바로 dirx와 diry의 역할이다. 위의 코드처럼 단일 배열 2개로 만들어도 괜찮고 2차원 배열로 만들어도 괜찮다. 이동 방향이 4방향이냐 8방향이냐에 따라 배열의 이동 좌표를 확장해주면 된다.보통 visit을 건들 때 일반적으로 말하는 x,y의 위치가 배열에서 나타내는 순서와 반대로 말하므로 헷갈리지 않게 조심해야한다. 특히 dfs 내부에서 첫번째 if문의 역할인, 다음에 이동하는 좌표가 탐색을 요구하는 범위 내에 있는 지를 확인하는 상항에서 out of range 에러를 일으킬 수 있다. 그래서 헷갈리지 않게 주의해줘야한다.그 외의 방식은 좌표를 이동하면서 일반적인 dfs와 유사하게 방식을 진행하면된다." }, { "title": "[Data Structure] Graph (그래프)", "url": "/posts/Graph/", "categories": "DataStructure, C++", "tags": "DataStructure, Graph", "date": "2020-07-24 00:00:00 +0900", "snippet": "Graph (그래프)그래프란 정점과 간선으로 구성된 자료구조이다. 일반적으로 G = (V,E)로 나타내는데, V는 vertices로 정점을 말하고, E는 edges인 간선을 의미한다. |V|는 정점의 개수를 의미하고 |E|는 간선의 개수를 의미한다.그래프는 간선의 종류에 따라 2가지로 구분할 수 있다. 그리고 그 종류에서도 그래프의 특징으로 종류를 더 세분화할 수 있다.그래프의 종류그래프는 간선의 종류로 크게 2가지로 나눠지게 된다. 무향 그래프 (undirected graph) 유향 그래프 (directed graph)명칭에서 보이듯이 그래프의 간선에 방향성이 있고 없고의 차이를 보여준다.무향 그래프 (undirected graph)그래프의 이름 그대로 간선에 방향성이 존재하지 않는다. 그래서 1에서 4, 4에서 1의 이동이 모두 가능한 그래프이다. 그래프의 간선을 표시할 때 보통 (1, 4)와 같이 표시하는데, 이 과정에서 순서가 정해지지 않은 pair가 된다. 즉 무향 그래프에서는 (1,4)나 (4,1)이나 같은 간선을 말한다.유향 그래프 (directed graph)그림이 커져보이는 거는 기분 탓이다간선에 방향성이 생긴 그래프다.이 그래프는 무향 그래프와 다르게 (1,4)와 (4,1)은 서로 다른 간선을 나타내게 된다. 간선이 방향성을 갖기 때문에 순서가 곧 방향성을 나타내기 때문이다. 그래서 유향 그래프는 순서가 정해진 pair가 된다.일반적으로 무향 그래프보다는 유향 그래프에서 자주 쓰이는 추가적인 그래프의 옵션이 있는데, 그래프의 간선에 가중치가 존재하는 경우를 가중치 그래프라고 한다. 가중치 그래프에서 가중치는 보통 이동할 때 걸리는 시간, 비용등으로 대표가 되어서 최단거리, 최소비용으로 이동하는 경로를 찾을 때 사용된다.그래프 용어그래프에서 사용되는 용어가 몇가지가 존재한다. 인접 정점 (adjacent vertices) 차수 (degree) 사이클 (cycle)인접 정점 (Adjacent Vertices)그래프의 인접한 정점이란 말 그대로 특정 정점에서 직접적으로 갈 수 있는 정점들을 말한다.보통 인접정점들을 묶어 놓은 것을 level이라고 한다.1번 정점의 인접 정점은 2, 4가 있고 2의 인접 정점은 3, 4와 같이 있다. 이렇게 인접 정점을 아는 이유는 그래프를 만드는 방법을 결정하는 것에도 관련이 깊다. 이런 인접 정점들의 개수를 차수라고 한다. 차수를 계산하는 방법은 무향 그래프와 유향 그래프에 따라 다르다.차수 (degree)우선적으로 차수를 계산하기 쉬운 그래프인 무향 그래프를 설명해보면, 무향 그래프는 말 그대로 그냥 인접한 정점 개수를 세면된다. 위에 있는 그래프를 기준으로 설명하면 1번 정점의 차수는 2가 되게 된다.이게 유향 그래프로 가면 고려해야할 것이 생긴다. in-degree와 out-degree를 따로 구해서 그 두 degree를 합친 것이 총 차수가 되게 된다.4번 정점을 기준으로 설명해보면 4번 정점의 in-degree는 3이다. 1, 2, 6번 정점으로부터 들어오는 간선이 있기 때문이다. out-degreesms 1이 된다. 6번 정점으로 나가는 간선이 있기 때문이다.그래서 4번 정점의 총 차수는 3 + 1인 4가 된다. 단순하게 차수만 계산한다면 무향 그래프와 원리는 같지만 그 차수를 구성하는 종류가 2가지로 나뉘는 것을 생각해볼 필요가 있다.사이클 (cycle)사이클이란 그래프의 부분을 보는 게 아니라 전체적인 그래프의 이동 경로를 봤을 때 나타난다.전체적인 경로를 봤을 때, 경로가 순환이 발생하면 그 경우를 사이클이라고 한다. 이런 사이클이 발생을 하는 게 자연스럽고 큰 문제는 아니지만 알고리즘 중 벨만-포드 알고리즘과 같은 최적 경로 판단 문제에서는 사이클이 문제 요소가 될 수 있다. 그래서 이런 사이클을 확인하는 게 중요하다.나중에 DFS나 BFS문제를 풀 경우 사이클을 하나의 묶음으로 처리하여 사이클의 수를 확인해야하는 문제들도 있으므로 사이클 확인은 중요한 정보가 된다.구현 방법그래프의 구현 방법은 총 3가지가 있다. 간선 리스트 인접 리스트 인접 행렬이렇게 3가지 종류로 구현이 가능하다. 보통 알고리즘 문제를 풀 때는 편의를 위해서 인접 리스트와 인접 행렬 방식을 많이 사용한다. 실제로 간선 리스트 방식은 좀 오래된 그래프 표현 방식이다.그래프를 구현할 때 막상 구현하려고 하면 어떻게 해야 구현이 가능한지가 감이 안 잡힐 수 있다. 그래프의 구현은 생각보다 간단하다. 그래프의 구성은 정점과 간선이지만 우리가 실제로 그래프로 인식하는 이유는 간선이 생성되기 때문이다. 즉 그래프를 구성할 때는 간선을 저장하면 그것이 결국 그래프가 되는 것이다.아래의 그래프를 기준으로 각 구현방법을 설명해보겠다.간선 리스트 (Edge List)간선 리스트방식은 말 그대로 모든 간선의 종류를 저장하는 방식이다. 각 정점의 차수를 저장하는 배열이 존재하고 각 간선을 저장하는 배열도 존재하게 된다. 이 방식을 잘 활용하지 않는 이유는 우선적으로 다른 두 방식에 비해서 메리트가 떨어지기 때문이다.인접 리스트 방식은 간선 리스트보다 시간 복잡도 측면에서 유리한 메리트를 갖고 인접행렬은 공간 상의 낭비가 존재하지만 직접적으로 간선들을 컨트롤 할 수 있다는 장점이 있다.인접 리스트 (Adjacency List)알고리즘 문제를 풀 때 인접 행렬 방식과 유사한 빈도로 많이 쓰는 방식이다. 메모리 낭비를 줄이기 위해서 사용하는 방식이다. 일반적으로 간선이 별로 없는 희소 그래프(Sparse graph)에서 자주 사용된다. 직접적으로 특정 정점간의 연결관계를 파악하는 것보다 전체적인 연결 흐름을 파악하는 것을 주요 요구로 할 때 많이 사용된다.구현 방식은 2차원 배열로 만든다. 각 첫번째 배열의 인덱스가 정점들을 나타내고 각 인덱스 별로 2번째 배열에는 그 정점과 연결된 다른 정점의 이름을 저장한다.자세한 구현은 그래프 직접구현에서 설명하겠다.인접 행렬 (Adjacency Matrix)인접 행렬은 모든 정점을 2차원 배열로 만들고 저장한다. 그리고 간선의 연결 정보를 좌표로 표시해서 저장한다. 모든 정점의 순서쌍을 만들어서 저장하므로 $ O(V^2)$ 의 공간복잡도를 갖는다. 따라서 상당히 많은 메모리를 차지하므로 너무 많은 정점을 갖는 그래프에서는 잘 사용하지 않는다. 하지만 특정 간선을 확인해야하는 경우에서 굉장히 빠른 속도를 보여준다.인접 리스트는 최악 수행시간이 $ O(n)$ 의 시간을 갖지만 인접 행렬은 특정 연결관계 확인에 무조건 $ O(1)$ 시간이 발생한다.공간적인 소모가 크므로 보통 정점의 개수와 간선의 개수가 유사한 밀집 그래프(Dense graph)에서 자주 사용된다.Graph implementation (그래프 구현)그래프 구현은 간선 리스트는 자주 활용하지 않으므로 생략하고 인접 리스트 방식만 설명하겠다.인접 행렬은 그냥 단순히 정적 2차원 배열로 만들면 된다.Adjacency List GraphGraph prototype#include &amp;lt;vector&amp;gt;#include &amp;lt;iostream&amp;gt;using namespace std;class Graph() {private: int N; vector&amp;lt;vector&amp;lt;int&amp;gt;&amp;gt; adj_list;public: Graph() { N = 0; } Graph(int n) { N = n + 1; adj_list.resize(N); } void addEdge(int u, int v); int degree(int u);};그래프에 필요한 함수는 그렇게 많지 않다. 물론 전체를 다 리스트로 구현하면 좀 더 복잡하지만 벡터는 일종의 리스트 역할을 할 수 있으므로 벡터로 대신할 수 있다.벡터는 정점과 그 정점이랑 연결된 정점들을 저장하는 역할을 할 것이다. 중요한 점은 기본적으로 벡터에 정점의 개수만큼 정확하게 벡터 크기를 resize해준다.참고로 이 그래프는 무향 그래프를 기준으로 설명한다.add edge functionvoid addEdge(int u, int v) { adj_list[u].push_back(v); adj_list[v].push_back(u);}각 벡터의 정점 위치에 연결 정점을 저장해준다. 이는 무향 그래프 기준으로 만들경우 값을 바꿔서 한번 더 저장해주는 것이고 유향 그래프인 경우 한 번만 저장해주면 된다.degree functionint degree(int u) { return adj_list[u].size();}특정 정점의 차수는 해당 정점을 저장한 인접 리스트의 크기만큼을 반환해주면 된다. 인접 리스트의 원리를 알면 간단한 것이다." }, { "title": "[Data Structure] Binary Search Tree (이진 탐색 트리)", "url": "/posts/BST/", "categories": "DataStructure, C++", "tags": "DataStructure, Dictionary, Tree, Binary Search Tree, BST, AVL Tree", "date": "2020-07-21 00:00:00 +0900", "snippet": "Binary Search Tree (이진 탐색 트리)이진 탐색 트리는 앞서서 배운 트리의 확장적인 형태라고 보면 편하다. 일반적인 이진 트리는 탐색의 기준이 없어서 특정 데이터를 찾으려면 모든 노드를 탐색해야한다.하지만 이진 탐색 트리에서는 평균적으로 훨씬 빠른 시간에 탐색이 가능해진다.Binary Search (이진 탐색)우선 이진 탐색과 이진 탐색 트리를 구분해야 한다. 전자는 알고리즘의 일종이고 후자는 자료구조의 일종이다. 이진 탐색 트리의 원리를 알기 이전에 이진탐색의 원리를 알아야 한다.이진 탐색은 간단하게 말하면 탐색 범위를 절반으로 줄여나가는 탐색이다. 바로 이전 포스트에서 말한 search table과 같다. 굳이 자세한 설명을 더 하진 않겠다.이런 이진 탐색을 트리 형태로 구조화시키면 그게 이진 탐색 트리이다.시간 복잡도BST의 시간복잡도는 트리의 모양과 관련이 깊다. 기준값보다 큰 값을 우측으로, 작은 값을 좌측으로 보내기 때문에 특정 데이터 셋에서는 트리가 한쪽으로 일렬 형태의 트리가 나올 수도 있다. 시간복잡도는 일반적으로 $ O(h)$ 로 나타내는데, 이렇게 극단적인 경우에서의 시간복잡도는 $ O(n)$ 의 시간복잡도를 갖는다. 모든 데이터를 통과해야지 삽입, 삭제가 가능하기 때문이다.반면에 좌우 균형이 알맞은 이진 트리는 h가 $ \\log{n}$ 을 띄므로 $ O(\\log{n})$ 의 시간복잡도를 갖는다.Deletion case (삭제 케이스)이진 탐색 트리의 삭제 과정은 삭제하는 노드의 조건에 따라 삭제 방식이 바뀌게 된다. 크게 2가지 경우의 케이스로 분류된다. 삭제할 노드의 자식 중 하나라도 리프 노드일 때 삭제할 노드의 자식이 모두 존재할 때1번 경우는 처리해주는 것이 어렵지 않다. 삭제노드를 지우고 그 아래 있는 자식을 바로 연결해주면 된다. 문제는 2번 케이스에서 발생된다. 생각보다 해줘야 하는 작업이 많기 때문이다.2번 케이스가 1번 케이스와 다르게 문제가 발생하는 이유는 간단한데, 1번 케이스는 자신 밑에 자식이 있거나 없기 때문에 자신의 데이터가 후속 데이터에 크게 영향이 미치지 않는다. 하지만 자식이 둘이라는 의미는 후속 데이터가 영향을 받을 수 있다.자식이 둘인 경우 후속 데이터가 영향을 받는다는 게 무슨 의미냐면 아래와 같다고 생각하면 된다.위의 상황처럼 3번 노드를 제거하는 상황이면 단순하게 3번을 제거하고 2번이나 8번을 해당 위치에 대체하면 문제가 발생한다. 8번이 해당 위치를 대체한다면 6번 아래에 있는 모든 노드들은 2번 자식들로 모두 옮겨져야 한다는 문제가 발생한다. 2번을 대체한다고 해도 지금은 문제가 없어 보이지만 2번 아래에 자식이 있다면 똑같은 문제가 발생한다.그래서 일반적으로 삭제하는 노드들 대체하는 노드는 삭제 노드의 오른쪽 트리 중 가장 작은 값으로 대체한다. 오른쪽 부분 트리의 가장 왼쪽 리프 노드는 이진 탐색 트리의 원리상 삭제 노드와 가장 가까우면서 큰 노드가 되게 된다.AVL-Tree (균형 잡힌 이진 탐색 트리)앞서서 말한 시간복잡도 부분에서 완벽하게 $ O(\\log{n})$ 을 보장하지 못함을 알 수 있다. 이러한 것을 보장하기 위해서 나온 것이 AVL Tree이다. AVL 트리는 항상 양쪽 균형이 최대한 맞춰진 이진 탐색 트리이다. (참고로 AVL은 사람이름이다.)이전에 트리에서 노드의 높이를 말한 적이 있는데, 노드의 높이면 해당 노드를 루트 노드로 하는 부분 트리라고 했다. AVL Tree는 자식 노드의 높이차이가 최대 1까지만 나는 경우를 말한다.부분 트리의 높이를 계산할 때 아래서부터 카운트를 하는데, 리프노드가 1인 이유는 일반적으로 리프노드에 2개의 더미노드가 존재하기 때문이다. 더미노드의 높이를 0이라고 간주한다. 부모노드의 높이는 자식 노드 중 높은 것의 높이 +1을 한 값이 된다. AVL 트리의 이런 성질 height-balance property라고 한다. AVL Tree는 새로운 값이 삽입되거나 삭제되었을 때, height-balance property를 유지를 해야 한다. 그래서 그런 높이를 조정하는 과정이 Trinode Restructuring 또는 Rotation이라고 한다.Rotation (회전)회전은 AVL Tree의 각 부분트리들의 높이를 조정해주는 과정에서 사용된다. 회전은 보통 1번이나 2번으로 이루어지고 왼쪽 회전, 오른쪽 회전이라고 부른다. 이런 회전의 조합이 1번이냐 2번이냐에 따라 1번 회전, 2번 회전으로 구분된다.위의 트리에서 삽입이 이루어진 경우를 한 번 보자.54를 가진 노드가 추가되면서 78 값을 가진 노드에서 높이 밸런스 특성이 깨졌다. 설명의 편의를 위해서 노드의 값을 노드의 이름으로 부르겠다. 78노드의 높이가 4인 이유는 자식의 높이 중 가장 큰 높이+1이 부모의 높이가 되기 때문이다.AVL Tree는 height-balance property를 유지해야 하므로 회전을 진행한다. 회전을 진행하는 과정에서는 총 3개의 노드가 위치를 바꾸게 되는데, 3개의 노드는 x,y,z라고 지칭한다. 그리고 해당 노드를 정하는 조건은 다음과 같다. z노드는 최초로 height-balance가 깨진 노드로 한다. (78노드) y노드는 z노드의 자식 중 가장 높이가 높은 노드이다. (50노드) x노드는 y노드 중 가장 높이가 높은 노드이다. (62노드) 2,3번 과정에서 자식 노드들의 높이가 같으면 보통 왼쪽 노드를 선택한다. 회전의 편의성을 위해서이다.이렇게 정해진 x, y, z노드에 다시 a, b, c라는 이름을 붙여준다. a, b, c를 붙이는 조건은 중위순회를 기준으로 먼저 중위순회가 이뤄지는 순서로 이름을 붙여준다. 여기서는 y,x,z순서로 a,b,c가 붙여진다.이렇게 설정된 노드에서 x를 가운데로, y를 왼쪽, z를 오른쪽 자식으로 놓게 설정한다. 그 후에 자식을 크기 순서로 놓아주면된다. 이렇게 위치를 조정하면 아래와 같이 높이 밸런스가 알맞게 조정이 된다.실제로 이 과정은 이렇게 보면 단순하게 회전이 진행된 것 같지만 2번 회전한 경우에 해당된다. 일반적으로 1번 회전하는 경우는 a, b, c가 일렬로 연결이 된 경우에 해당된다. 위는 a, b, c가 지그재그 형태로 이뤄진 경우로 2번 회전을 진행하게된다. 간단한 회전의 경우를 보면 아래처럼 구분된다고 보면 된다.위와 같은 경우는 1번 회전을 진행한 single rotation이라고 한다. 중심이 되는 노드가 b가 되게 만드려면 왼쪽으로 돌려야하므로 왼쪽 회전이라고 한다. 반대의 경우는 오른쪽 회전이라고 한다.이 경우가 예시로 들었던 2중 회전의 경우이다. 2중 회전은 사실 별 거라고 느낄 필요는 없다. 우선 b와 c를 교체해준다. 그러면 single rotation의 형태를 띄게되는데, 여기서는 일반적인 single rotation처럼 진행해주면 된다.삭제가 발생했을 경우에도 삽입과 같이 높이들을 조정해주면 된다.여기서 주의해야할 점이 있는데, 삽입은 height balance가 깨진 노드를 조정해도 그 부모에 영향이 없다. 하지만 제거를 진행하면 height balance를 조정하더라도 부모 노드의 높이에 영향을 끼칠 수 있어서 루트노드까지 모든 hegith balance를 체크해주는 과정을 진행해야한다.사실 나도 AVL Tree는 너무 어렵다… 이론적으로 대충 이해만 하고 넘어가는 정도…BST implementation (이진 탐색 트리 구현)Node prototypeclass Node {private: Node* parent; Node* right; Node* left; int data;public: Node() { data = NULL; parent = NULL; right = NULL; left = NULL; } Node(int data) { this-&amp;gt;data = data; parent = NULL; right = NULL; left = NULL; } void insert(Node* child); int degree(); int depth(); ~Node() {} friend class BST;};이진 트리에서 한 것처럼 이진 트리의 노드 클래스의 함수들이다. 생성자는 일반적인 이진 트리의 생성자와 동일하다. 여기서 노드의 부모자식 관계를 처리하는 함수인 insert와 트리의 깊이와 자식 수를 반환해주는 함수를 만들었다.Node insert functionvoid insert(Node* child) { if (child-&amp;gt;data &amp;gt; data) { right = child; child-&amp;gt;parent = this; }else if (child-&amp;gt;data &amp;lt; data) { left = child; child-&amp;gt;parent = this; }}노드의 자식을 결정해주는 함수이다. 자식으로 들어오는 노드의 값이 현재 노드보다 크면 오른쪽 자식으로 지정해주고 반대의 경우는 왼쪽 자식으로 지정해준다.Node degree and depthint degree() { int deg = 0; if (left != NULL) deg++; if (right != NULL) deg++; return deg;}int depth() { if (parent == NULL) return; return parent-&amp;gt;depth() + 1;}degree는 간단하게 자식이 있는 경우를 보고 카운트를 올려주면 된다. depth가 좀 어려운데, 가장 좋은 방법은 재귀함수로 구현해주면 된다. 루트가 0이고 0에서부터 1씩 올려주면서 값을 반환해주면 된다.BST prototypeclass Node {...};class BST {private: Node* root;public: BST() { root = NULL; } ~BST() {this-&amp;gt;treeDestructor(this-&amp;gt;root);} void treeDestructor(Node* temp); Node* findNode(int data); Node* findMin(Node* node); void insert(int data); void delNode(int data); void transplant(Node* u, Node* v);}트리이므로 소멸자를 확실히 진행해줘야한다. findMin과 findNode는 노드를 제거하는 경우에서 활용이 되므로 반드시 만들어줘야한다.Tree destructor functionvoid treeDestructor(Node* node) { if (node == NULL) return; if (node-&amp;gt;right != NULL) treeDestructor(node-&amp;gt;right); if (node-&amp;gt;left != NULL) treeDestructor(node-&amp;gt;left); delete node;}트리를 제거하는 함수이다. 기본적인 원리는 dfs의 형식으로 가장 안쪽의 리프노드부터 제거해준다. 그렇게 제거해주면서 모든 노드를 제거한다.Find node functionNode* findNode(int data) { Node* node = root; while (node != NULL) { if (node-&amp;gt;data == data) return node; else if (node-&amp;gt;data &amp;lt; data) node = node-&amp;gt;right; else node = node-&amp;gt;left; } return NULL;}나름 가독성을 신경썼는데 별로 좋은거 같진 않다….반복문의 이동은 현재 노드가 더미에 도달한 상황에서 종료시킨다. 리프가 더미에 도달했다는 의미는 찾고자하는 노드가 없다는 의미이므로 해당 노드가 없다는 의미로 NULL을 반환해준다. 찾는 노드를 찾았다면 해당 노드를 반환해주면된다.Find min node functionNode* findMin(Node* node) { Node* tmp = node; while (tmp-&amp;gt;left != NULL) { tmp = tmp-&amp;gt;left; } return tmp;}주어진 노드를 루트로 하는 부분 트리의 최소 노드를 찾아낸다. 방법은 이진 탐색 트리 원리를 활용하면된다. 현재 루트부터 가장 왼쪽으로 계속 이동하면된다.Insert functionvoid insert(int data) { Node* newNode = new Node(data); if (root == NULL) { root = newNode; }else{ Node* tmp = root; while (1) { if (tmp-&amp;gt;data &amp;lt; data) { if (tmp-&amp;gt;right == NULL) break; tmp = tmp-&amp;gt;right; }else if (tmp-&amp;gt;data &amp;gt; data) { if (tmp-&amp;gt;left == NULL) break; tmp = tmp-&amp;gt;left; } } tmp-&amp;gt;insert(newNode); }}새로운 값을 삽입하는 함수이다. 들어갈 위치가 반드시 있으므로 들어갈 위치를 찾으면 반복을 멈춰준다. 반복이 종료된 이후에는 노드에 있는 삽입함수로 노드를 연결해주면된다.Delete and transplant functionvoid transplant(Node* u, Node* v) { if (u-&amp;gt;parent == NULL) { this-&amp;gt;root = v; }else if (u == u-&amp;gt;parent-&amp;gt;left) { u-&amp;gt;parent-&amp;gt;left = v; }else { u-&amp;gt;parent-&amp;gt;right = v; } if (v != NULL) v-&amp;gt;parent = u-&amp;gt;parent;}void delNode(int data) { Node* tmp = findNode(data); if (tmp-&amp;gt;left == NULL) { transplant(tmp, tmp-&amp;gt;right); }else if (tmp-&amp;gt;right == NULL) { transplant(tmp, tmp-&amp;gt;left); }else { Node* min = findMin(tmp-&amp;gt;right); if (min-&amp;gt;parent != tmp) { transplant(min, min-&amp;gt;right); min-&amp;gt;right = tmp-&amp;gt;right; min-&amp;gt;right-&amp;gt;parent = min; } transplant(tmp, min); min-&amp;gt;left = tmp-&amp;gt;left; min-&amp;gt;left-&amp;gt;parent = min; }}transplant함수는 이름처럼 자식을 삭제노드의 부모에 이식하는 함수이다. if문과 else if문까지는 deletion case1에 해당하는 자식이 1개이거나 없는 경우에 시행된다.가장 까다로운 부분이 else인 deletion case2에 해당된다. 우선 가장 작은 노드를 저장해주고 기존에 말했던 방식처럼 삭제를 진행해주면 된다." }, { "title": "[Data Structure] Hash (해시)", "url": "/posts/Hash/", "categories": "DataStructure, C++", "tags": "DataStructure, Dictionary, Hash, Search table, Binary Search, Hash table", "date": "2020-07-15 00:00:00 +0900", "snippet": "Dictionary (딕셔너리)딕셔너리는 사실 C++보다는 파이썬에서 더 익숙할 것이라 생각된다. 실제로 딕셔너리라고 아예 명확하게 말을 하기도 하니까…자료구조에서 딕셔너리는 key와 value를 함께 저장하는 entry 저장 자료구조이다. 대신 key의 중복을 허용한다.시간 복잡도딕셔너리의 시간복잡도는 딕셔너리를 리스트 구현을 기준으로 얘기를 해보겠다. 딕셔너리의 ADT는 우선적으로 put, find, erase정도각 대표적이다.put 함수는 리스트에서 바로 맨 뒤나 맨 앞에 저장하는 방식을 활용한다. 당연히 이 함수는 $ O(1)$ 의 시간복잡도를 갖는다. 보통 doubly-linked list나 배열로 구현을 한다.find와 erase는 doubly linked list를 기준으로 우선 탐색을 진행해야한다. 결국 $ O(n)$ 의 시간을 갖는다.이런 점에서 보통 딕셔너리로 저장을 하는 경우에는 삽입은 자주 이루어지지만 검색과 삭제는 적은 경우의 데이터 구조에서 활용한다. 그래서 보통 로그 파일에서 자주 사용되는 방식이다.Search Table (탐색 테이블)탐색 테이블은 정렬된 배열로 만든 딕셔너리 자료구조로 구현한다. 정렬된 배열로 구현하는 이유가 있다. 그 이유는 조금 뒤에 얘기를 하겠다.우선 탐색 테이블의 ADT는 일반적인 딕셔너리의 ADT와 동일한 ADT를 가진다. 대신 시간복잡도가 다르게 나타난다.put과 erase는 $ O(n)$ 의 시간이 소요된다. 배열로 구현했는데 상수시간이 아닌 이유는 맨 처음에 배열 구현에서 한 것처럼 남은 데이터들을 옆으로 옮겨주는 shift연산이 필요하기 때문이다.find는 $ O(\\log{n})$ 시간이 소요된다. 이는 정렬된 배열로 구현한 이유와 동일한 맥락이다.Binary Search (이진 탐색)이진 탐색은 탐색 테이블에서 정렬된 배열이 활용돼야하고 find연산 시간 $ O(\\log{n})$ 의 시간이 소요되는 이유이다.이진 탐색은 계속해서 중앙 값을 판단 기준으로 삼는 탐색방법이다. 만약에 현재 데이터 범위의 중앙 값이 자신이 찾고자 하는 값보다 크면 기준 값의 왼쪽 범위는 탐색 범위에서 제외한다. 찾고자하는 값이 중앙 값보다 작으면 기준 값의 오른쪽 범위는 탐색 범위에서 제외한다. 위의 방법을 계속해서 반복하고 결국 1개의 데이터가 남았을 때, 해당값이 탐색 값과 같으면 특정 결과를 반환해준다. 만약 값이 없다면 조건에 맞춰 출력해준다.이 방식은 계속해서 중앙을 기준으로 범위를 줄여나가기 때문에 전체 크기의 절반씩 줄여나간다. 결국 전체 탐색 횟수는 $ \\log_{2}{n}$ 만큼을 진행한다. 그래서 탐색 시간은 $ O(\\log{n})$ 시간이 걸리게 되는 것이다.Hash Table (해시 테이블)해시 테이블은 딕셔너리의 형태로 구현을 진행한다. 딕셔너리는 중복을 처리하는 것이 가능하다고 했는데, 중복이 발생하는 것을 충돌(collisions)이라고 한다. 이렇게 충돌이 발생했을 때, 처리 방법은 separate chaining방법을 활용한다. 그리고 딕셔너리 형태는 리스트 기반의 딕셔너리로 진핸한다.시간복잡도해시 테이블의 시간복잡도는 worst case로 나타내면 list based dictionary이므로 $ O(n)$ 시간이다. 하지만 해시 테이블의 특성때문에 구현자가 어떤 방식으로 해시 테이블을 구현하느냐에따라 모든 함수의 기대 시간복잡도 (expected time)은 $ O(1)$ 시간이 소요된다. 왜 시간복잡도를 다른 것들과 다르게 기대할 수 있는 시간복잡도를 말하는 지는 앞으로 나오는 해시 함수와 관련이 깊다.Hash function (해시 함수)해시에는 값이 저장되는 위치를 기록해줄때 해시 함수를 활용한다. 정확히 말하면 해시 함수의 역할은 key 정보를 활용해서 [0, N-1]의 범위 안에 해당 value를 매핑시켜주는 역할을 한다.그래서 해시함수는 $ h(x) = x\\mod N$ 로 활용한다. 이때 N은 꽤 큰 소수로 설정을 해야 데이터의 충돌을 줄일 수 있다. 해시 함수에 값을 넣고 나온 결과를 hash value라고 한다.근데 왜 해시 함수와 기대하는 시간복잡도가 $ O(1)$ 인 것과 관련이 깊은 것일까?해시 함수를 설정할 때 조건 중 가장 중요한 관점은 entry들을 가능한 골고루 흩어지게 하는 것이 좋다는 것이다. 그래서 해시 함수를 2개로 활용하는 경우도 있다.일단 1개의 해시 함수로 설명을 해보면 충분히 큰 소수로 나누게 될 경우 소수의 특성상 1과 자신을 제외하고는 들어오는 모든 숫자는 나머지로 그대로 빠져나가게 된다.\\(\\text{if n &amp;lt; N and N is enoughly big prime number }\\\\n \\mod N = n\\) 즉 N이 충분하게 큰 소수인 경우에는 자신의 key값이 결국 해시 값이 되게 되고 숫자가 클수록 N-1의 범위가 넓어지게 되어 데이터가 1개가 들어가는 범위가 넓어진다. 정말 최악의 경우가 진짜 우연하게 들어온 모든 데이터의 나머지가 다 동일한 경우이다.이는 확률적으로 보면 굉장히 작은데, 나머지의 범위는 0~N-1까지 N개이다. 즉 하나의 값에 적용될 확률은 $ 1/N$ 이다. 사실상 N이 충분히 크다면 2개가 연속할 확률도 굉장히 작아지게 된다. 결국 데이터 중복이 적어지게 되므로 모든 데이터가 분산되어진다. 그래서 기대하는 탐색시간은 $ O(1)$ 이다.충돌 처리 (Collision handling)Separate Chaining앞에서 충돌 처리 방식을 Separate Chaining이라고 했다. 정확하게 설명하면 배열로 만들어진 해시 테이블의 특정 인덱스에 중복이 발생하면 링크드 리스트로 연결해 나가는 방식이다. 간단한 방식이지만 메모리가 추가적으로 요구된다는 점이 단점이다.Open addressingOpen addressing 방식은 간단하게 말하면 충돌이 발생하면 링크드 리스트로 추가적인 연결을 진행하는 것이 아닌 같은 해시 테이블에서 다른 인덱스로 이동해서 값이 없다면 해당 위치에 값을 넣어주는 방식이다. 이러한 방식은 2가지 방식이 있다. Linear probing Double hashingLinear ProbingLinear probing 방식은 해시 함수를 1개 활용하는 방식이다. 최초의 해시 값이 할당되는 곳에서부터 1씩 증가시켜서 해시 값을 이동해서 빈 공간을 찾아내는 방식이다. 그래서 빈 공간이 발생하면 그 공간에 데이터를 삽입한다. 그래서 Linear Probing의 가장 큰 단점이라면 특정 지점은 연속적인 데이터가 저장된 덩어리 형태를 띄게되어 데이터의 수가 늘어날수록 데이터의 충돌 횟수가 늘어나게 된다.open addressing의 모든 방식에서 중요한 점은 데이터가 있는 공간 뿐만 아니라 있었던 공간은 비어 있으면 안된다.이게 정확히 무슨 말이냐면, 해시에서도 당연히 제거 함수가 존재한다. 제거를 하게 되는 경우 해당 테이블의 값이 제거되거나 없는 것과 같이 표시를 해주는데, 해시의 해시 테이블은 총 3가지 상태가 존재한다. NO ITEM IS ITEM AVAILABLE모든 해시 테이블은 NO ITEM에서 시작한다. 하지만 데이터가 삽입된 공간은 상태가 IS ITEM으로 변경된다. 즉 삽입을 진행할 때, IS ITEM인 공간은 넘어가게 되는 것이다. 하지만 데이터가 삭제되었다면 어떻게 할까?위에서 데이터가 연속적으로 덩어리를 이루게 된다고 했는데, 만약 중간에 특정 값을 지워버리고 NO ITEM으로 표시해버린다면 탐색을 멈춰버리기 때문에 중간에 끊긴 부분 이후의 값을 탐색할 방법이 없다. 그래서 탐색이 중간에 멈춰버리는 일을 막기 위해 데이터를 제거한 후에는 상태를 AVAILABLE로 바꿔준다.앞서서 간단히 말했지만 데이터가 많아질수록 덩어리가 만들어지므로 충돌 가능성이 늘어나게 된다. 이렇게 되면 $ O(1)$ 을 기대할 수 있지만 가능성이 낮아질 수 있다.Double HashingDouble Hashing은 해시 함수를 2개 활용한다. 즉 인덱스의 이동범위를 좀 더 넓혀준다. 해시 함수를 2개 활용하면 다음과 같이 식의 구조를 형성한다.\\((h(x) + j \\times d(k) ) \\mod N\\) 여기서 $ h(x)$ 는 기존에 쓰던 1차 해시함수, $ d(x)$ 는 2차 해시함수이다. 2차 해시 함수는 $ (q-(k\\mod q))$ 로 정의된다. 이때, $ q$ 는 N보다 작은 소수로 설정해준다. 전체 값에서 나머지를 빼주기 때문에 2차 해시값은 1~$ q$ 값을 갖게된다. 여기서도 $ q$ 가 소수가 아니라면 특정 숫자에 한해서는 인덱스를 특정 구간만 반복할 위험이 있다.이렇게 데이터의 덩어리 형성을 막아준다면 우리가 기대했던 것처럼 $ O(1)$ 의 시간복잡도를 가질 수 있다.Hash implementation (해시 구현)#include &amp;lt;iostream&amp;gt;using namespace std;#define MAX 353333#define NOITEM 0#define ISITEM 1#define AVAILABLE 2class cell {private: int key; int value; int flag;public: cell() { key = -1; value = -1; flag = NOITEM; } ~cell() {}}cell Hashtable[MAX];int sz = 0;int hashfunc1(int key) { return key % MAX:}int hashfunc2(int key) { return (17 - (key % 17));}void LinearInsert(int key);void LinearSearch(int key);void LinearDelete(int key);void DoubleInsert(int key);void DoubleSearch(int key);void tableClear() { for (int i = 0; i &amp;lt; MAX; i++) { if (HashArr[i].flag) { HashArr[i].key = -1; HashArr[i].flag = NOITEM; HashArr[i].value = -1; sz--; } if (sz == 0) break; }}따로 클래스로 해시를 구현할 필요는 없다. 전체적으로 데이터를 저장할 클래스정도만 만들고 나머지는 일반적인 함수로 구현해준다.1차 해시함수와 2차 해시함수를 위와 같이 설정해줬다.Linear Insert functionvoid LinearInsert(int key) { int probing = 1; while (Hashtable[hashfunc1(key + probing - 1)].flag) { probing++; } Hashtable[hashfunc1(key + probing - 1)].key = key; Hashtable[hashfunc1(key + probing - 1)].flag = ISITEM; sz++;}Linear Probing방식에서 설명한대로 구현을 진행했다. 현재 해시테이블의 상태가 NO ITEM인 경우만 확인하면 되므로 0으로 설정된 NOITEM은 조건에서 false로 인식해서 반복문을 돌려준다. 만약 값이 있다면 probing이라는 탐색횟수를 올려준다.값을 찾으면 찾았다는 표시를 해주고 데이터를 넣어준다. 그리고 전체 해시의 개수도 늘려준다.Linear Search functionvoid LinearSearch(int key) { int probing = 1; while (Hashtable[hashfunc1(key + probing - 1)].flag) { if (Hashtable[hashfunc1(key + probing -1)].key == key) { cout &amp;lt;&amp;lt; 1 &amp;lt;&amp;lt; &quot; &quot; &amp;lt;&amp;lt; probing &amp;lt;&amp;lt; &quot;\\n&quot;; return; } probing++; } cout &amp;lt;&amp;lt; 0 &amp;lt;&amp;lt; &quot; &quot; &amp;lt;&amp;lt; probing &amp;lt;&amp;lt; &quot;\\n&quot;;}탐색도 전체적인 흐름은 동일하다 플래그를 확인하고 값이 있는 경우에는 해당 값이 찾는 값과 동일한 지 확인하면된다.Linear Delete functionvoid LinearDelete(int key) { int probing = 1; while (Hashtable[hashfunc1(key + probing - 1)].flag) { if (Hashtable[hashfunc1(key + probing -1)].key == key) { Hashtable[hashfunc1(key + probing -1)].flag = AVAILABLE; Hashtable[hashfunc1(key + probing -1)].key = -1; cout &amp;lt;&amp;lt; 1 &amp;lt;&amp;lt; &quot; &quot; &amp;lt;&amp;lt; probing &amp;lt;&amp;lt; &quot;\\n&quot;; sz--; return; } probing++; } sz--; cout &amp;lt;&amp;lt; 0 &amp;lt;&amp;lt; &quot; &quot; &amp;lt;&amp;lt; probing &amp;lt;&amp;lt; &quot;\\n&quot;;}삭제는 탐색에서 플래그 상태와 벨류만 상황에 맞게 설정해주면 된다.Double Insert functionvoid DoubleInsert(int key) { int probing = 1; while (Hashtable[hashfunc1(key) + (probing - 1) * hashfunc2(key)].flag) { probing++; } Hashtable[hashfunc1(key) + (probing - 1) * hashfunc2(key)].flag = ISITEM; Hashtable[hashfunc1(key) + (probing - 1) * hashfunc2(key)].key = key; sz++;}사실 이전에 있는 linear에서 해시 함수만 건드려준 것이다. Double search도 동일하게 진행해주면 된다." }, { "title": "[Data Structure] Priority Queue (우선순위 큐)", "url": "/posts/PQ/", "categories": "DataStructure, C++", "tags": "DataStructure, Priority Queue, heap", "date": "2020-07-11 00:00:00 +0900", "snippet": "Priority Queue (우선순위 큐)이전에 큐에 대해서 배운 적이 있다. 이렇게 일반적인 큐를 FIFO Queue라고 한다. FIFO원리를 갖고 있는 큐이기 때문이다.이번 시간에는 다른 성질을 갖는 우선순위 큐에 대해서 이야기해보겠다.우선순위 큐는 이름에 특징이 잘 나와있는데, 큐의 pop 연산수행을 진행할 때, 프로그래머가 정해놓은 우선순위에 맞춰서 제거 연산을 진행한다.일반적으로는 우선순위 기준은 2가지인데, 크기가 큰 값을 우선순위로 두는 Max-Priority-Queue 크키가 작은 값을 우선순위로 두는 Min-Priority-Queue가 있다.우선순위 큐와 큐는 잘 알아둘 필요가 있는 이유가 있다. 이전 글에서 트리의 탐색 방식은 향후 그래프 탐색방법인 DFS와 일맥상통한다고 했다. 큐는 그래프 탐색방법 중 BFS에서 사용이 된다. 그리고 그런 BFS의 응용으로 사용하는 최단거리 계산 알고리즘인 다익스트라(Dijkstra) 알고리즘에서는 Min-Priority queue가 사용된다. 이 글에서는 Min-priority queue를 기준으로 구현하겠다.Priority Queue ADT우선순위 큐는 entry들의 모임이다. 여기서 entry란 새롭게 나오는 개념인데, (key, value)조합으로 이루어진 데이터 쌍이다. value는 없는 경우도 있는데, key는 반드시 존재하게된다. key값으로 우선순위를 결정하기 때문이다. insert(e) : e라는 값을 가진 entry 삽입 (여기서는 e는 key가 된다.) removeMin() : 가장 작은 값을 삭제해준다. min() : 가장 작은 값을 반환해준다. size() empty()우선순위 큐 구현방법우선순위 큐 구현방법에는 3가지 방식이 존재한다. 이유는 정렬 방식때문인데, 가장 효율적인 방법은 힙(heap)방식으로 구현하는 것이다. unsorted sequence sorted sequence heapSequence based Priority Queue시퀀스 기반의 우선순위 큐에는 2가지가 있는데, unsorted방식과 sorted방식이다. 이 두 가지 방식의 구현 원리를 설명해보겠다.Unsorted Sequence &amp;amp; Selection sort 기본 함수의 소요시간비정렬 방식의 삽입연산에는 $ O(1)$ 의 시간이 소요된다. 이유는 가장 마지막이나 가장 앞에 데이터를 넣어주기만 하면되기 때문이다.removeMin 연산이나 최소를 반환하는 연산같이 탐색을 동반하는 연산은 모두 $ O(n)$ 시간을 요구한다. 왜냐하면 전체 데이터에서 가장 작은 값을 찾기 위해서는 모든 값을 확인해야하기 때문이다. 선택정렬 (Selection sort)비정렬 방식으로 만든 우선순위 큐를 사용하는 정렬방식이 선택정렬이다.정렬하고자하는 sequence의 내부 데이터를 모두 우선순위 큐에 넣었다가 다시 앞에서부터 제거하면서 빼내는 절차를 거쳐야한다. 선택정렬의 삽입 과정에서는 $ O(n)$ 시간이 소요된다. 이유는 간단하다. 우선 비정렬 시퀀스에 넣은 값을 다시 앞에서부터 우선순위 큐에 삽입해주기 때문이다. 그래서 총 n번의 $ O(1)$ 연산이 수행되므로 총 $ O(n)$ 시간이 소요된다. 선택정렬에서 removeMin은 $ O(n^2)$ 시간이 소요된다. 앞서 말했듯이 비정렬방식에서는 removeMin을 위해서는 저장된 데이터만큼의 탐색을 진행해야한다. 그래서 총 n개라 했을 때 n번 탐색, 이후 n-1번 탐색을 1번 탐색까지 반복하므로 등차수열 합 공식을 적용 시킬 수 있다.\\(\\sum_{k=1}^{n}k = \\frac{n(n+1)}{2}\\) Sorted Sequence &amp;amp; Insertion sort 기본 함수의 소요시간정렬 방식에서 삽입연산은 $ O(n)$ 시간이 소요된다. 해당 데이터가 들어가기 가장 좋은 위치를 탐색해야하기 때문이다. 정확히 말하면 데이터를 삽입하는 동시에 삽입정렬이 이루어지기 때문이다.removeMin은 이미 삽입당시 정렬이 이루어졌으므로 $ O(1)$ 시간으로 가장 앞에 데이터를 제거해주면 된다. 삽입정렬 (Insertion sort)정렬 방식으로 구현한 우선순위 큐를 사용하는 정렬방식이 삽입정렬이다. 정렬하고자하는 sequence의 내부 데이터를 모두 우선순위 큐에 넣었다가 다시 앞에서부터 제거하면서 빼내는 절차를 거쳐야한다. 삽입정렬의 삽입과정은 $ O(n^2)$ 의 시간이 사용된다. 우리가 정렬하고자 하는 시퀀스의 데이터를 우선순위 큐에 넣을 때 우선순위 큐에 들어온 데이터의 크기만큼 탐색을하며 자신이 들어갈 위치를 찾는다. 결국 전체 데이터만큼 탐색을 해야하는데, 이때 $ O(n)$ 시간이 걸리게된다. 선택정렬에서 제거하는 원리처럼 등차수열 합공식에 의해서 $ O(n^2)$ 시간이 걸린다. 다행히 removeMin은 $ O(n)$ 시간이 걸린다. 가장 앞의 데이터를 빼주면 되므로 $ O(1)$ 만큼의 연산은 전체 데이터 횟수만큼 해주기 때문이다. Heap (힙)힙은 우선순위 큐의 구현방법 중 하나이다. 완전이진트리 구조로 데이터를 저장하고 있다. 우선순위 큐의 구현을 위한 것이므로 entry를 저장하고 있다. 힙에 반드시 필요한 2가지가 있다. heap-order : 힙 순서는 힙에서 데이터 삽입과 제거 과정에서 필요한 것이다. 부모와 자신의 데이터를 비교해서 두 노드의 정보를 바꿔줄 필요가 있다. 완전 이진트리 구조 : 노드의 제거와 삽입에서 사용하는 upHeap과 downHeap과정에서 좌우 노드의 탐색이 필요하기 때문에 반드시 왼쪽부터 꽉 채워지는 완전 이진트리 구조를 가져야한다. last node : 삽입하는 과정에서 보다는 제거하는 과정에서 필요한 마지막 노드를 가져오는 last node의 정보를 가져와야한다. last node는 말 그대로 가장 깊이 있는 노드 중 하나이다.힙의 깊이와 노드 개수의 연관성힙의 깊이와 노드 개수는 연관성이 깊다. 왜 갑자기 힙 트리의 높이를 계산하냐면 이후에 필요한 upHeap이나 downHeap 과정은 힙 트리의 높이가 결국 수행시간을 결정하게 되기 때문이다.완전 이진트리를 기준으로 할 때, 현재 이진트리가 h-1까지 구성되어있고 새로 데이터를 넣으면 깊이 h에 새로운 노드가 삽입되는 상황이라고 하자.현재까지 노드개수를 수식으로 표현하면 다음과 같다.\\(1 + 2+ 2^{2}+...+2^{h-1}=\\frac{1(2^h -1)}{2-1}=2^h -1\\) 깊이가 h-1인 상황에서 완전 이진트리가 되어 있는 경우는 $ 2^h -1$ 개의 노드가 있다. 여기서 만약 노드가 1개 추가된다면 h높이에 노드가 1개 추가된다.이전까지 $ 2^h -1$ 개의 노드가 1개가 추가되면서 $ 2^h$ 개가 된다. 즉 h높이 트리 노드 개수는 다음과 같은 수식을 만족한다.\\(n \\geq 2^h \\rightarrow \\; h \\leq \\log_{2}{n}\\) 결과적으로 높이와 관련된 연산은 $ O(logn)$ 연산 수행시간이 걸린다.insertion of heap (힙의 삽입과정)힙은 새로운 노드를 삽입하거나 제거를 했을 때 기본적으로 데이터가 정리가 되어 있는 트리의 형태를 띄게된다.우선순위 큐의 구현을 위해 만들어지므로 어떻게 보면 데이터 정렬이 당연해질 수 있다. 최소 힙을 기준으로 할 때, 루트 노드는 자연스럽게 가장 작은 값을 갖게되고 리프노드일수록 큰 값을 갖게된다. 새로 들어 온 노드와 자신의 부모노드와 크기 비교를 한다. 자신의 부모노드가 자신의 값보다 크기가 크다면 둘의 데이터를 바꿔준다. 아니라면 그대로 유지하고 함수를 종료 1번과 2번을 바꿀 수 없을 때까지 반복해준다.두 노드의 값을 변경하는 과정은 $ O(1)$ 시간이 소요된다. 이 과정을 최악의 경우 (리프 -&amp;gt; 루트)에 트리의 높이만큼 swap을 해줘야한다. 위에서 h는 아무리 최악이라도 $ \\log_{2}{n}$ 을 넘을 수 없다. 결국 힙의 삽입과정에서 소요되는 총 시간은 $ O(\\log{n})$ 시간이다.이렇게 힙의 삽입과정에서 정렬을 해주는 과정을 upheap이라고 한다. 위로 올라가는 형태이기 때문에 붙여진 이름이라고 생각하면된다.removal of heap (힙의 삭제과정)힙에서 삭제를 하게 될 경우 루트노드를 제거하게된다. 왜 루트노드를 제거하냐면 힙은 우선순위 큐의 구현을 위해 만들어진 방식인데, 우선순위 큐는 특정한 기준치를 따라서 Out과정이 이루어진다. 그 특정한 기준치로 데이터를 높이별로 정렬을 시킨 것이 바로 heap이기 때문이다.그렇다면 루트 노드가 제거되었다면 그 자리는 누가 차지하게 되는 걸까?그 자리는 가장 마지막 노드가 우선적으로 차지하게 된다. 우선 루트노드를 제거한다. 제거하기 이전에 가장 마지막 노드를 저장해준다. (가장 마지막 노드를 지시하는 방법은 힙의 크기를 저장하는 변수로 지시해주면 된다.) 그 후 루트노드 위치 (벡터 인덱스 1번)에 저장해 둔 마지막 노드 데이터를 넣어준다. 그와 동시에 마지막 노드 데이터를 벡터에서 제거해준다. 그 후 자식 노드에서 더 작을 것과 위치를 바꿔준다. 3번 과정을 더 이상 바꿀 수 없을 때까지 반복한다.이 과정도 삽입과 동일하게 $ \\log_{2}{n}\\시간이 소요된다. 자식과 데이터를 바꾸는 과정은 downheap이라고 하는데, 사실 upheap은 루트노드라는 쉬운 종결조건이 있다. 하지만 downheap은 조건이 명확하지 않으면 벡터의 out of index에러를 마주친다.종결조건은 자신의 key값이 자신의 자식들의 key값 중 어떤 것보다도 크지 않거나 본인의 위치가 리프노드면 종결한다.Heap implementation (힙 구현)Heap prototype#include &amp;lt;iostream&amp;gt;#include &amp;lt;vector&amp;gt;using namespace std;enum direction { MIN = 1, MAX = -1 };class Heap {private: vector&amp;lt;int&amp;gt; Node; int heap_size; int root_idx; int direction;public: Heap(int direction) { Node.push_back(-1); this-&amp;gt;heap_size = 0; this-&amp;gt;root_idx = 1; this-&amp;gt;direction = direction; } void Swap(int idx1, int idx2); void upHeap(int idx); void downHeap(int idx); void insert(int data); int pop(); int top(); int size(); bool empty(); void print(); int at();};특별히 설명하고 넘어갈 부분은 enum class파트의 역할과 벡터에 -1을 넣고 시작하는 이유정도이다.우선 enum class인 direction의 역할은 Max heap과 Min heap을 결정해준다. 여기서 왜 MIN이 1이고 MAX가 -1이냐면, 기본적으로 이후에 진행할 upHeap과 downHeap과정에서 부등호로 대소비교를 해준다. 우리가 구현할 힙은 최소 힙을 기준으로 하므로 우선 MIN이 1이 된다. 여기서 부등호에 -1을 곱해주면 부등호 방향이 바뀜을 활용해주면 된다. 그래서 MAX가 -1이 되는 것이다.벡터에 -1이 먼저 들어가는 이유는 간단하다. 우리는 앞서서 본 트리에서 벡터나 배열로 이진트리를 구현할 경우를 생각해봤다. 이진트리의 특성상 left child의 인덱스는 parent index * 2와 같고 right child는 left child + 1의 인덱스를 갖는다. 이런 성질은 문제없이 활용하기 위해서는 어떤 수에 0을 곱해도 0인 0번 인덱스를 피하고 데이터를 다뤄줘야한다. 그래서 우선적으로 0번 인덱스에 의미없는 수를 넣어준다.empty, size, top, at functionbool empty() { return heap_size == 0;}int size() { return heap_size;}int top() { return Node[root_idx];}int at(int idx) { return Node[idx];}간단한 반환 함수들이다. 설명은 생략…Swap functionvoid Swap(int idx1, int idx2) { int tmp = Node[idx1]; Node[idx1] = Node[idx2]; Node[idx2] = tmp;}일반적인 swap과 유사하다. 여기서 조금 주의해야할 점은 이름의 s는 반드시 대문자로 설정해주다. Visual Studio에는 기본적으로 swap이 있는데, 이게 헤더를 추가하지 않아도 사용이 가능한 구현함수라 사용할 때 문제가 발생할 수 있다.upheap functionvoid upHeap(int idx) { if (direction * Node[idx/2] &amp;gt; direction * Node[idx]) { Swap(idx/2, idx); upHeap(idx/2); }}heap에서 가장 중요한 함수 중 하나인 upheap 함수다. upheap을 진행할 때 기억할 중요한 점은 부모의 값과 자신의 값 비교 이다. C++성질상 나눗셈의 최솟값은 0이므로 무조건 0번 인덱스까지만 간다. 그래서 그냥 단순 재귀로 계속 연산해주면 된다. 재귀 실행조건은 간단하다.부모가 나보다 큰 경우이다. 이 경우에는 위에서 구현한 Swap을 실행시켜주고 다시 재귀로 upheap을 돌려주면된다. 재귀로 돌려줄 때는 자신이 이미 부모와 위치가 바뀌었으므로 부모인덱스를 인덱스로 넣어준다.downheap functionvoid downHeap(int idx) { int right = 2 * idx + 1; int left = 2 * idx; int small = right; if (right &amp;lt;= heap_size) { if(direction * Node[left] &amp;lt;= direction * Node[right]) { small = left; } }else if (left &amp;lt;= heap_size) { small = left; }else { return; } if(direction * Node[idx] &amp;gt; direction * Node[small]) { Swap(small, idx); downHeap(small); }}heap구현에서 가장 어려운 부분이지 않을까 싶은 downheap이다. 앞에서 말한 downheap의 조건을 우선 잘 적어보자. 자신이 리프노드에 도달하면 멈춘다. 자신보다 자신의 자식중 작은 자식이 자신보다 작지 않으면 멈춘다.이걸 잘 생각해보면 우선 우리가 해줘야 할 작업이 몇 가지가 있다. 어떻게 하면 지금 내가 있는 위치가 리프노드인지 알 수 있을까? 이건 어렵지 않다. 힙의 기본조건은 완전 이진트리라는 것이다. 우리는 이미 삽입과 삭제를 할 때, 힙에 저장된 노드 수를 기록하는 heap_size를 갖고 있다. 이진트리 성질에 따라서 만약 right나 left노드의 인덱스가 heap_size를 넘었다면 적어도 현재 노드에는 자식이 존재할 수 없다는 의미가 된다. 그러면 현재 노드는 자연스럽게 리프노드가 된다. 위의 코드에서는 첫번째 if~else if문이 그 역할을 한다. 자신의 자식 중 작은 것을 찾아야 한다. 만약 내가 리프노드가 아니고 자식이 있다는 것을 파악했다면, 이제 자식 중 작은 값을 찾아내서 나와 비교 후 자리를 바꿔줘야한다. 첫번째 if~else문에서 리프노드가 아님을 파악했다면, 내가 자식이 1개인지, 2개인지 알아볼 필요도 있다. right가 heap_size보다 작거나 같다는 의미는 나에게 자식이 2개가 있다는 의미가 된다. 완전 이진트리의 특징상 right node index는 left node index보다 무조건 클 수 밖에 없기 때문이다. 그리고 left가 없다면 right도 있을 수 없기 때문이다. 즉 right가 존재할 때만 left와 right의 값을 비교해준다. left만 있다면 비교할 인덱스는 left 하나 뿐이다. 그 후 upheap과 유사하게 재귀로 진행을 해주면된다. 우리가 고려해줄 것들은 이미 위쪽의 if~else문에서 다 고려해줬다.insert, pop functionvoid insert(int data) { Node.push_back(data); heap_size++; upHeap(heap_size);}int pop() { if(empty()) { return -1; } int tmp = Node[heap_size]; int ret = Node[root_idx]; Node.erase(Node.begin() + heap_size); Node[root_idx] = tmp; heap_size--; downHeap(root_idx); return ret;}우선 삽입함수부터 설명을 해보면, 간단하다. 앞서 말했듯이 우선 노드를 넣어주고 upheap과정을 통해 정렬을 진행해주면 된다.조금 복잡한 제거함수이다. 우선 제거관련이므로 당연하게 공백여부를 체크해준다. 그 후 마지막노드 값을 임시적으로 보관해준다. 이 값은 제거된 root에 대신 들어가게된다.말은 root를 없애는 것처럼 보이지만 실제로는 root의 값만 바꾸고 실제 제거되는 데이터 위치는 마지막 노드의 위치이다. vector function으로 마지막 노드부분을 제거해준다.그 후 root의 값을 마지막 노드 값으로 바꾸고 downheap을 진행해주면된다.당연히 반환 값은 root에 저장되었던 값.print functionvoid print() { if(empty()){ cout &amp;lt;&amp;lt; -1 &amp;lt;&amp;lt; &quot;\\n&quot;; }else{ for(int i = 1; i &amp;lt;= heap_size; i++) { cout &amp;lt;&amp;lt; Node[i] &amp;lt;&amp;lt; &quot; &quot;; } cout &amp;lt;&amp;lt; &quot;\\n&quot;; }}출력함수에서 주의할 점은 우리가 1번 인덱스부터 데이터를 처리했다는 것이다. 이 점을 주의하면서 heap size에 맞춰서 돌려주면된다." }, { "title": "[Data Structure] Binary Tree (이진트리) &amp; Tree 구현", "url": "/posts/BTree/", "categories": "DataStructure, C++", "tags": "DataStructure, Tree, Binary tree", "date": "2020-07-08 00:00:00 +0900", "snippet": "Binary Tree (이진 트리)이진트리 개념이진트리는 우리가 이전에 배운 트리의 특수한 형태이다. 일반 트리가 자식 수에 제한이 없다면, 이진트리는 1부모에 최대 2개의 자식 노드가 존재한다.보통 왼쪽 노드, 오른쪽 노드라고 부르며 왼쪽 노드가 오른쪽 노드보다 우선하는 성질을 가진다. 일반 트리에서 설명하지 않았지만 트리는 자식간에 순서가 있는 ordered pair이다.이진트리의 쓰임새는 아래와 같은 것들이 있다. 수식의 표현 트리 수식의 계산 트리 결정트리우선 결정트리부터 설명해보면 internal node는 결정요소를 갖고있고 external node는 해당 결정요소에대한 결정 값들을 갖고있다.후에 서술할 중위순회를 이용하면 수식을 표현할 수 있다.완전 이진트리의 성질완전 이진트리는 이진트리에서의 모든 internal node의 자식이 2개로 꽉 차있는 상태의 이진트리를 말한다. 이런 이진트리에서는 특별한 수식적 성질들이 있다. 우선 설명을 하기 전에 간단한 기호의 규칙들을 정해보겠다. n : 노드의 개수 e : 리프 노드의 개수 (number of external nodes) i : internal nodes의 개수 h : 높이위와 같이 정하고 완전 이진트리의 성질을 이야기해보자. $ e = i + 1$ 리프노드의 수는 내부노드 + 1이다. 왜 그럴까?모든 노드는 2개의 자식을 갖고 있다. 그래서 만약 여기서만 internal node가 n개면 총 2n개의 자식이 존재한다. (어떤 부모는 다른 부모의 자식일 수 있다.)이때, 모든 루트노드를 제외한 모든 노드는 부모를 갖고 있다. 결국 n-1개의 노드가 부모를 갖고 있다.그 뜻은 다른 말로 말하면 n-1개의 부모 노드는 적어도 n-1개의 자식을 확실하게 확보하고 있다.그니까 이게 뭔 말이냐면, 자식이 존재하는 노드를 internal node인데 root노드는 부모노드가 없는 노드이다. internal node가 있다는 것은 이 노드는 누군가의 자식이라는 거다. 즉 전체 internal node수인 n개에서 root노드를 제외하곤 부모와 연결하는 n-1개의 연결을 필요로 하게된다. 최종적으로 반드시 사용해야하는 n-1개를 2n개에서 빼면 n+1개의 자리가 남는다. 이 n+1개는 반드시 external nodes가 되게 된다. $ n = 2e - 1$ 전체 노드의 개수는 n개인데, $ n = i + e$ 의 성질을 갖는다. 1번 식에 해당 식을 정리해서 대입하고 다시 정리하면 2번 식이 나오게된다. $ h \\leq i$ 트리의 높이는 internal node의 개수보다 작거나 같다. 이건 생각보다 생각하기 간단하다. 모든 노드가 한 줄로 쭉 연결된 상태라고하면, h와 i의 값이 같다. h는 자식이 하나 뻗을 때마다 1개씩 증가하기 때문이다. $ h \\leq (n - 1) / 2$ 이 식은 3번과 1,2번 식을 정리하면된다.\\(n = 2e - 1 = 2(i + 1) - 1 = 2i + 1 \\\\\\therefore \\; i = (n - 1) / 2\\) 위의 계산과정을 3번에 대입하면 4번이 나온다. $ e \\leq 2^h$ 이건 3번의 증명과 좀 반대로 생각하고 좌우가 균형을 이후는 완전 이진트리를 그리면 된다. $ h \\geq log_{2}e, \\;\\;h\\geq log_{2}(n+1) - 1$ 5번 식을 양변로그를 취하고 4번의 식을 합쳐주면 두 식이 나오게 된다.이진트리 순회이진트리도 일종의 트리이다. 결국 순회가 존재하는데, 앞서 말했던 전위, 후위순회는 동일하게 진행되고 이진트리에만 있는 특수한 순회가 있다. 바로 중위순회 (inorder traversal)이다.중위순회는 전위, 후위순회와 조금 다른 특징이라면 다음 자식을 가기 전에 자신을 거치고 간다는 점이다. 즉, 왼쪽-&amp;gt;나-&amp;gt;오른쪽의 탐색순서를 거친다는 의미이다.Algorithm inOrder(v) if !v.isExternal() inOrder(v.left()) visit(v) if !v.isExternal() inOrder(v.right())중위순회는 다음과 같이 탐색한다.사실 가만히 놓고 보면 모든 순회는 재귀함수 구현이 주를 이룬다.수식 출력중위순회를 사용하면 수식 출력이 가능하다.위의 수식을 중위순회로 원래 수식으로 표현할 수 있다.Algorithm inOrder(v) if !v.isExternal() print(&#39;(&#39;) inOrder(v.left()) print(v.element()) if !v.isExternal() inOrder(v.right()) print(&#39;)&#39;)이렇게 코드를 작성하면 숫자 -&amp;gt; 수식의 형태로 탐색하게된다. 출력결과는 잘 생각해보길… 코드를 따라가면서 해석하는 것도 중요하다.Tree implementation (트리 구현)Node prototype#include &amp;lt;vector&amp;gt;using namespace std;class Node {private: Node* parent; int data; vector&amp;lt;Node*&amp;gt; children;public: Node() { parent = NULL; data = NULL; } Node(int d) { parent = NULL; data = d; } void insetChild(Node* c) { this-&amp;gt;children.push_back(c); return; } void deleteChild(Node* c) { for (int i = 0; i &amp;lt; children.size(); i++) { if (children[i] == c) { this-&amp;gt;children.erase(this-&amp;gt;children.begin() + i); break; } } } ~Node() {} friend class Tree;};이건 트리에 필요한 노드의 클래스이다. 트리에 들어가는 노드는 부모를 가리키는 노드 포인터와 자식 노드들을 저장하는 노드포인터 벡터가 존재한다. 트리에서 좀 더 쉽게 사용할 수 있는 insert와 delete가 있다.Tree prototype#include &amp;lt;vector&amp;gt;using namespace std;class Node {...};class Tree {private: int tmp = 0; int heiht = 0; Node* root; vector&amp;lt;Node*&amp;gt; node_list;public: Tree() { root = NULL; } Tree(int d) { root = new Node(d); } Node* getRoot(); void insert(int p, int c); void delNode(int data); int getHeight(); void printChild(int data); void countDepth(int data); void preorder(Node* n); void postorder(Node* n); Node* findNode(int data);}생성자는 2개를 만들었다. 일반적으로 생성하는 기본생성자는 root node가 NULL값이고 특정 데이터를 갖는 노드를 루트로 하려면 두번째 생성자를 해주면된다. 멤버변수에 대해서 간단히 설명하자면, root는 말 그대로 루트노드를 말한다, node_list는 현재 트리에 저장된 노드를 저장하는 벡터이다. 이 트리의 저장형태를 간단히 표시하면 아래와 같다.우측의 배열이 좌측의 트리에 있는 node_list에 저장된 값들이고 해당 노드들의 children을 보여준다.getroot, getheight functionNode* getRoot() { return root;}int getHeight() { return height;}두 함수는 간단하다. 그냥 요구하는 걸 반환해주면 된다.insert functionvoid insert(int p, int c) { bool exist = false; // 부모노드가 있는 지 확인한다. for(int i = 0; i &amp;lt; node_list.size(); i++) { if(node_list[i]-&amp;gt;data == p) { exist = true; Node* v = new Node(c); node_list[i].push_back(v); node_list[i]-&amp;gt;insetChild(v); v-&amp;gt;parent = node_list[i]; break; } } if(!exist) cout &amp;lt;&amp;lt; -1 &amp;lt;&amp;lt; &quot;\\n&quot;;}exist변수는 자식을 추가하려고하는 부모노드의 존재를 확인하는 플래그 변수이다. 기본적으로 Node 클래스에 만들어 둔 insertChild 함수를 활용한다.부모노드의 역할을 하는 노드에 있는 자식노드 리스트 벡터에 저장을 하는 원리이다. 현재 있는 노드리스트에서 찾고자 하는 부모노드를 탐색한다. 찾은 노드의 자식 노드리스트 벡터에 새로운 노드인 자식노드를 넣어준다. 새롭게 만든 자식노드의 부모포인터를 현재 노드로 지정해준다.delete functionvoid delNode(int data) { bool exist = false; for(int i = 0; i &amp;lt; node_list.size(); i++) { if (node_list[i]-&amp;gt;data == data) { exist = true; Node* p = node_list[i]-&amp;gt;parent; for(int j = 0; j &amp;lt; node_list[i]-&amp;gt;children.size(); j++) { // 삭제하고자하는 노드의 자식을 모두 삭제 노드의 부모로 연결해준다. node_list[i]-&amp;gt;children[j]-&amp;gt;parent = p; p-&amp;gt;children.push_back(node_list[i]-&amp;gt;children[j]); } p-&amp;gt;deleteChild(node_list[i]); delete node_list[i]; break; } } if (!exist) cout &amp;lt;&amp;lt; -1 &amp;lt;&amp;lt; &quot;\\n&quot;;}실제로 문제풀이 때 사용할 일이 없어서 구현은 안 했지만 이론상 설명을 하기위해서 직접 구현을 했다. 내가 배운 특정 노드 제거 방법은 제거하고자하는 노드의 자식들을 제거하는 노드의 부모 노드로 옮기는 과정으로 진행한다. 삭제하고자하는 노드가 있는지 확인한다. 노드가 있다면 해당 노드의 자식노드리스트 크기만큼 돌려준다. 옮긴 부모 노드의 자식 노드리스트 벡터에 해당 자식들을 추가해준다. 모든 작업이 끝나면 해당 노드를 부모 노드에서 제거해준다. 최종적으로 원하는 노드를 제거한다.count depth functionint countDepth(int data) { int d = 0; // return value; bool exist = true; for (int i = 0; i &amp;lt; node_list.size(); i++) { if (node_list[i]-&amp;gt;data == data) { exist = false; // depth 계산 시작 노드 탐색 완료 거꾸로 올라간다 for (Node* curr = node_list[i]; curr-&amp;gt;parent != NULL; curr = curr-&amp;gt;parent) { d++; } } } if(exist) return -1; return d;}노드의 깊이는 현재 노드가 루트 노드를 만나러 올라갈 때까지 카운팅을 올려주면 된다.노드를 찾고 역으로 타고 올라가면 된다. 노드리스트에서 찾고자하는 노드를 탐색한다. 노드를 발견하면 현재 노드를 기준으로 계속해서 부모노드로 올라간다. 부모노드가 NULL인 루트노드를 발견하면 카운트를 멈춘다.print child functionvoid printChild(int data) { bool exist = false; for(int i = 0; i &amp;lt; node_list.size(); i++) { if (node_list[i]-&amp;gt;data == data) { exist = true; int s = node_list[i]-&amp;gt;children.size(); if (s == 0){ cout &amp;lt;&amp;lt; 0 &amp;lt;&amp;lt; &quot;\\n&quot;; }else{ for(int j = 0; j &amp;lt; s; j++) { cout &amp;lt;&amp;lt; node_list[i]-&amp;gt;children[j]-&amp;gt;data &amp;lt;&amp;lt; &quot; &quot;; } cout &amp;lt;&amp;lt; &quot;\\n&quot;; } } break; } if(exist) cout &amp;lt;&amp;lt; -1 &amp;lt;&amp;lt; &quot;\\n&quot;;}특정 노드의 자식 노드를 출력하는 함수이다. 코드는 길어보이나 아이디어는 간단하다.필요로하는 노드를 node_list에서 찾고 그 노드의 자식 노드 벡터를 다 출력해주면 되는 것이다. 문제에서 요구하는 사항이 0이나 -1을 출력하는 것이 있어서 이것저것 많이 있게 되었다.하지만 프로그래밍은 원래 예외처리가 중요하므로 해줄 필요가 충분히 있다.find node functionNode* findNode(int data) { for(int i = 0; i &amp;lt; node_list.size(); i++) { if (node_list[i]-&amp;gt;data == data) { return node_list[i]; } } return NULL;}특정 노드를 탐색하는 함수이다. 앞서서 진행한 insert, delete, print등에 활용한 기반 함수를 활용하면 된다. 반복문으로 node_list에서 탐색하면 된다.preorder functionvoid preorder(Node* node) { cout &amp;lt;&amp;lt; node-&amp;gt;data &amp;lt;&amp;lt; &quot; &quot;; if(node-&amp;gt;children.size() != 0) { tmp++; // 자식이 있으므로 깊이 연산가능하다. for(int i = 0; i &amp;lt; node-&amp;gt;children.size(); i++) { Node* child = node-&amp;gt;children[i]; preorder(child); } if(tmp &amp;gt; height) height = tmp; tmp--; }}전위순회 출력결과를 실행해주는 함수이다. 전위순회를 하는 동시에 전체 트리의 높이를 계산해주는 기능이 들어가있다. 자신 노드의 값을 출력해준다. 자신에게 자식노드가 있다면 이제 전위순회를 시행한다. 각자 노드의 깊이를 계산해주는 임시 변수 tmp를 늘려준다. 자식 수만큼 반복문을 돌리며 자식에 대해서 전위순회를 재귀함수로 시행해준다. 만약 자식이 없는 리프노드에 도달할경우 다시 재귀 실행 이전으로 돌아오게 된다. 모든 반복문이 종료되고 높이를 갱신해준다. 자식이 있는 노드의 깊이 계산이 끝났으므로 재귀 실행 이전으로 돌아가기전에 높이를 줄여준다.if문 안에 있는 tmp를 늘리고 줄이는 순서의 위치는 향후에 그래프 탐색을 할 때 쓰는 DFS에서 사이클 내부에 존재하는 노드 개수 계산과도 큰 관련이 있으므로 기억해두면 좋다.postorder functionvoid postorder(Node* node) { if(node-&amp;gt;children.size() != 0) { for (int i = 0; i &amp;lt; node-&amp;gt;children.size(); i++) { Node* child = node-&amp;gt;children[i]; postorder(child); } } cout &amp;lt;&amp;lt; node-&amp;gt;data &amp;lt;&amp;lt; &quot; &quot;;}후위순회는 전위순회에서 출력 순서만 맨 뒤로 옮겨주면 된다. 원리는 전위순회와 동일하다.중위순회에 대한 내용은 티스토리 블로그 트리 설명에 올려질 예정이다." }, { "title": "[Data Structure] Tree (트리)", "url": "/posts/Tree/", "categories": "DataStructure, C++", "tags": "DataStructure, Tree", "date": "2020-07-07 00:00:00 +0900", "snippet": "Tree (트리)트리 개념트리는 non linear data structure이다. 앞서서 배운 리스트, 배열, 벡터는 모두 linear data structure이고 이런 선형 자료구조들은 원소들 간에 전/후 관계가 있다.그러나 non linearr data structure, 즉 비선형 자료구조는 원소들 간에 상/하 관계를 가지고 있다. 이런 것을 계층적 관계라고한다.일반 트리 (General tree)오늘은 일반적인 트리에 관해서만 이야기할 예정이다. 트리는 보통 general tree와 binary tree로 구분하는데, general tree는 자식노드의 수가 제한이 없는 트리를 말한다.binary tree는 말 그대로 이진트리로, 자식 노드가 딱 2개만 제한이 되는 트리이다. 굳이 말하면 이진트리도 일반트리에 포함되어있다.트리는 노드들간의 부모-자식 관계로 이루어진 것들이다. root : root node란 그림 상 가장 꼭대기에 있는 노드를 가리키며 정의로는 부모가 없는 노드를 말한다. Internal node : Internal node는 자식이 적어도 1개는 있는 노드를 말한다. 그림 상으로 보면 A, B, D, E가 있다. External node (Leaf node) : External node라고도 하고 Leaf node라고도 한다. Internal node와 반대로 자식이 없는 노드를 말한다. 보통 리프 노드라고 한다. 그림 상에서는 C, F, G, H, I, J가 있다. Ancestor : 조상 노드라고도 한다. 일반적으로는 본인을 포함한 루트 노드까지 경로에 있는 모든 노드들을 조상 노드라고 한다. 예를 들어 설명해보면, E노드의 조상은 E, B, A이다. Descendant : 후손 노드라고도 한다. 후손 노드는 해당 노드를 통해 갈 수 있는 모든 노드를 후손노드라고 일컫는다. 예를 들어 설명하면 그림 상 B노드의 후손은 E, F, G, I, J이다. depth of node : depth는 일반적으로 node에 대한 정의를 나타내며 자신을 제외한 조상 노드의 수를 말한다. 일반적으로 자기 조상의 depth + 1로 나타낸다.depth에서 나온 개념 중 하나가 level인데, 같은 depth를 가진 node들을 level이라고 한다. height : height는 node와 tree 어디에 붙여서 설명하냐에따라 약간씩 달라진다. tree의 height라고 말하면 트리의 높이를 말한다. 일반적으로 해당 트리의 노드 중 가장 큰 depth 값을 tree의 height로 한다.node의 height는 특정 노드를 루트 노드로 하는 subtree의 height를 말한다. sibling : 남매, 형제 노드라고도 하며 해당 노드와 같은 부모를 둔 노드들을 말한다. degree : 특정 노드의 children의 개수이다. 즉 직계 자식 노드들의 개수를 말한다.Tree Traversal (트리 순회)트리에서 중요한 것은 트리의 탐색 방식이다. 일반적인 트리에서는 전위, 후위 순회밖에 없다. 이진트리로 가면 중위 순회도 존재한다. 오늘은 전위, 후위 순회를 설명한다.참고로 순위를 진행할 때는 해당 노드를 방문했다는 표시를 해주는데, 이를 보통 visit으로 설정한다. 이는 순회 과정 중에 하나에 속하므로 중요한 작업이다.Preorder Traversal (전위 순회)전위순회란 자식 노드를 탐색하기전에 자신을 우선 방문하고 자식 노드를 방문하는 방식의 순회 방법이다. 전위순회는 일반적으로 재귀로 구현을 한다.나-&amp;gt;자식 순서라고 생각하면 되면 편하다. &amp;lt; pseudo code &amp;gt;Algorithm prerOrder(v) visit(v) for each child w of v prerOrder(w)전위순회의 의사코드는 다음과 같다. 일반적으로 visit의 소요시간을 $ O(1)$ 이라고 하면, 전체 순회에 걸리는 시간은 $ O(n)$ 시간이 걸린다. 순회에 걸리는 시간은 visit의 소요시간이 결정한다.위의 사진처럼 순회 순서가 결정된다.Postorder Traversal (후위 순회)후위순회란 자식노드를 먼저 탐색을하고 자신을 탐색하는 순회 방법이다. 사실 후위순회도 재귀로 구현한다. 후에 그래프 혹은 트리 탐색 알고리즘인 DFS가 후위순회의 원리이다.자식-&amp;gt;나 순서로 생각하면 된다.&amp;lt; pseudo code &amp;gt;Algorithm postOrder(v) for each child w of v postOrder(w) visit(v)후위순회의 탐색 순서는 아래 사진처럼 탐색한다." }, { "title": "[Data Structure] Vector and List (벡터와 리스트)", "url": "/posts/Vector/", "categories": "DataStructure, C++", "tags": "DataStructure, Vector, List", "date": "2020-07-06 00:00:00 +0900", "snippet": "Vector (벡터)벡터 개념벡터는 크기가 동적으로 변하는 배열이라고 생각하면 된다. Array list라고 부르기도 하며 다양한 데이터들이 배열의 형태로 저장된 연속체라고 생각하면 된다.삽입, 삭제, 접근 연산이 모두 index에의해 이뤄지게 된다.Vector ADT벡터의 추상자료형은 다음과 같다. at(integer i) : index i에대해서 데이터 값을 반환해준다. set(integer i, object o) : index i의 데이터를 o로 바꿔준다. insert(integer i, object o) : index i에 o를 삽입한다. earase(integer i) : index i에 있는 데이터를 제거해준다. size() : 벡터의 크기를 반환해준다. empty() : 벡터가 비었는 지를 알려준다.벡터의 시간복잡도 벡터는 위에서 말했듯이 링크드 리스트와 다르게 인덱스로 모든 데이터에 접근하여 삽입, 삭제가 이뤄진다. 그렇기 때문에 해당 인덱스의 데이터를 가져오거나, 설정해주는 at과 set은 $ O(1)$ 의 시간복잡도를 갖는다. 그러나 삽입연산과 삭제연산을 하는 inset와 remove는 $ O(n)$ 의 시간복잡도를 갖는데, 이유는 해당 인덱스의 값을 넣거나 삭제를 함과 동시에 그 이후의 데이터들을 모두 움직여줘야 하기 때문이다. 하지만 이것은 선형 배열로 구현할 경우이고, 원형 배열기반으로 구현하면 이 또한 $ O(1)$ 의 시간복잡도로 구현할 수 있다.자세한 설명은 구현에서 설명하겠다.Array-based Vector벡터를 구현할 때 보통 배열 기반으로 구현하게 된다. 기존 C++STL에 있는 벡터는 초기에 설정된 크기를 넘는 경우에는 이전에 있는 크기의 2배만큼으로 크기를 늘려주게된다. 이러한 방법을 사용해도 괜찮고, 원형 배열을 사용하는 경우도 있다. 원형 배열을 사용하는 방식은 간단히 설명하고 넘어가겠다.Vector by Circular Array원형 배열 기반의 벡터에 대해서 자세한 설명은 삽입연산과 삭제연산의 원리에 대해서만 설명하겠다. 그 외는 단순히 값을 반환하거나 바꿔주기만 하면 되는 간단한 대입연산이다.삽입연산배열 기반 벡터는 가장 앞의 인덱스를 알려주는 f와 가장 뒤의 인덱스+1을 알려주는 r이 있다. 일반적으로 데이터가 들어가 있을 때, f는 가장 앞의 데이터가 저장된 인덱스를 가리키고 있다.즉 사용자 입장에서는 0번 인덱스에 f가 있다. 하지만 실제로 사용되는 배열은 원형 배열이라 f의 인덱스가 반드시 0번이라는 보장은 없다.그래서 보통 삽입 연산의 방식은 A[(f + i) % n] = o의 형식으로 작성된다.문제는 가장 앞에 데이터를 삽입하는 경우인데, 이 경우에는 f-1로 이동시켜서 값을 넣어주면된다.하지만 0번 인덱스에서 -1을 하는 문제를 막기위해서 A[(f-1+n)%n]의 방식으로 작동시켜준다. 삭제연산도 유사한 원리로 작동한다.Vector by Normal Array일반적인 배열로 벡터를 구현을 할 경우는 앞서 말한 원리처럼 배열의 크기를 늘려주는 작업도 고려해야한다.배열의 크기를 늘리는 전략에는 2가지 전략이 있는데, Incremental strategy와 Doubling strategy가 있다.Incremental Strategy는 상수 크기만큼 배열의 크기를 증가시키는 것이다. 100-&amp;gt;200-&amp;gt;300…처럼 특정 상수 크기만큼 증가시킨다.Doubling Strategy는 이전 크기의 2배만큼 크기를 증가시킨다. 100-&amp;gt;200-&amp;gt;400 이런 식으로 증가시키며 일반적인 벡터의 크기 증가 방법이 이렇다.삽입연산일반 배열로 구현한 벡터의 삽입연산은 아래와 같이 작성할 수 있다.Algorithm inset(o) if n = N then N &amp;lt;- 2N {Doubling} A &amp;lt;- new array of size N for i&amp;lt;-0 to n-1 do A[i] &amp;lt;- S[i] delete S S &amp;lt;- A S[n] &amp;lt;- o n &amp;lt;- n+1우선 크기를 늘릴 배열 A를 새롭게 선언해준다. 당연히 선언 이전에 N의 크기는 2배로 늘려준다. 그 후 기존 배열을 삭제하기위해 기존 배열의 데이터를 새로 만든 A에 복사해준다. 그 후 S를 A로 대입해주면 된다. 이렇게 하면 Doubling Strategy를 활용한 기존의 벡터를 구현할 수 있다.List리스트는 앞선에서 배운 링크드 리스트관련의 C++ STL이다. 이미 C++ 기본 라이브러리에 구현이 되어 있으며 리스트는 인덱스가 아닌 position이라는 개념을 활용한다.Position ADT리스트는 벡터나 배열형태의 자료구조에서 사용되는 index를 사용하지 않고 memory상의 주소를 갖고있는 position이라는 개념을 사용한다. 해당 데이터가 저장되어있는 위치를 가져온다. 대표적인 예시로는… 배열의 cell도 position이다물론 배열은 position이 아니라 index라고 부른다. 링크드 리스트의 노드주소를 저장하고 있다는 점에서 문득 스쳐지나가는 개념이 하나 있다면 C++에대한 기초공부를 굉장히 잘했다고 볼 수 있다. C++에는 따로 멤버함수로 position을 가지고 있지 않고 주소값을 가지고 있는 포인터(pointer)를 갖고있다.Node List ADT리스트의 추상 자료형은 다음과 같은 것들이 있다. size(), empty() : 앞선에서도 많이 다룬 것들이다. iterator begin() : 리스트의 시작부분의 주소를 반환해준다. end() : 리스트의 마지막부분의 주소를 반환해준다. insertFront(e), insertBack(e) removeFront(), removeBack() insert(position, e) remove(position)이전에 다뤘던 ADT들과 조금 다른 것이 추가되었다.바로 iterator라는 개념이다. 실제로 벡터나 리스트관련을 STL로 사용할 때 자주 사용하게되는 개념이다.iterator는 일종의 navigater역할이다. position이 확장된 개념인데 주소 값을 지시하는 역할을 하게된다. 여기서 end()를 마지막 주소를 반환해준다고 했는데, 실제로는 마지막의 다음 주소를 가지고 있다. 그래서 보통 반복문으로 iterator를 변경할 때, 조건문에는 i.end()와 같지 않을 때까지라는 조건을 붙인다.가장 마지막의 2개 함수는 특정 iterator 위치에 값을 삽입하거나 삭제해주는 함수이다.Doubly Linked List (이중 연결 리스트)1일차에 우리는 Singly Linked List를 배웠다. 티스토리 블로그에는 간단하게 언급한 것 같은데, 여기서 언급했는 지는 기억이 잘 안난다.링크드 리스트에는 종류가 보통 3가지가 있는데, 우리가 처음에 배운 단순 연결리스트, 이중 연결리스트, 환형(원형) 연결리스트가 있다.이번에 다룰 내용은 실제 C++ STL로 사용되는 이중 연결리스트를 다뤄보겠다.이중 링크드 리스트에 사용되는 노드는 단일 링크드 리스트의 노드에 1개의 주소필드가 추가된다.기존의 노드는 next라는 다음 노드를 지시하는 주소필드가 존재했지만 새로 만드는 노드는 prev라는 이전 노드를 지시하는 주소필드가 존재한다.그렇기 때문에 단순 연결리스트의 한계인 중간에 데이터 삽입하기가 가능해진다.이중 연결리스트는 삽입과 삭제 함수의 일관성을 위해서 header와 trailer라는 dummy node가 존재한다. header의 prev와 trailer의 next는 모두 NULL값을 가리키고 있다.이중 연결리스트를 구현해보면서 자세한 원리를 설명하겠다.Doubly Linked list implementation (이중 연결리스트 구현)Node class and prototype#include &amp;lt;iostream&amp;gt;using namespace std;class Node {public: int data; Node* next; Node* prev; Node() { next = prev = NULL; data = 0; } Node(int e) { this-&amp;gt;data = e; this-&amp;gt;next = NULL; this-&amp;gt;prev = NULL; } ~Node() {}};class DLinkedList {public: Node* head; Node* tail; int size; DLinkedList(); void insertFront(int e); void removeFront(); void insertBack(int e); void removeBack(); bool empty(); void insert(iterator iter, int e); void remove(iterator iter); ~DLinkedList(){}}맨 앞이나 맨 뒤에서의 삽입, 삭제 연산은 간단하지만 중간에 삽입하는 함수를 구현하기 위해서는 iterator class를 구현해서 만들어줘야한다. 하지만 iterator를 구현하는게 이 글의 목적이 아니므로 iterator는 생략한다.initializer (생성자)DLinkedList() { this-&amp;gt;size = 0; this-&amp;gt;head = new Node(); this-&amp;gt;tail = new Node(); head-&amp;gt;next = tail; tail-&amp;gt;prev = head;}이중 링크드 리스트의 생성자에서는 앞서 말한 더미 노드가 구현이 된다. 그래서 head와 tail에 각각 새로운 노드를 만들어준다. 여기서 왜 head = tail = new Node();냐고 생각이 든다면 한 10초정도 잘 생각해봐라. 그래도 모르겠다면 음… 지금 이걸 공부할 때가 아닌 것 같다.empty functionbool empty() { return head-&amp;gt;next == tail;}예외처리를 위해 항상 먼저 만드는 empty함수이다. 원리는 항상 간단하다. head의 다음이 tail이면 빈 경우라고 생각하면된다.insertFront functionvoid insertFront(int e) { Node* v = new Node(e); v-&amp;gt;next = head-&amp;gt;next; v-&amp;gt;prev = head; head-&amp;gt;next-&amp;gt;prev = v; head-&amp;gt;next = v;}노드를 2개나 변경시켜야하므로 상당히 헷갈리는 과정이 있다. 노드를 연결할 때 순서가 꼬이면 자기자신으로 도는 경우가 발생할 수 있으므로 순서를 잘 생각해야한다.이전에 있던 노드를 먼저 변경하면 문제가 발생할 수 있으므로 새 노드를 우선 연결해주는게 좋다. 삽입을 위한 노드를 새로 만들어준다. 새 노드의 next를 head의 next 노드로 연결해준다. 새 노드의 prev를 head로 해준다. head노드의 다음 노드의 prev는 v를 가리킨다. head노드의 next는 v를 가리킨다.위의 순서에서 가장 헷갈리는 게 4번과 5번과정인데, 4번이 앞서야 하는 이유는 간단하다. 만약 head 노드의 next를 먼저 바꿔버리면 기존에 head의 다음 노드가 가리키는 이전 노드를 변경할 수 없게된다. 글로는 헷갈리니 아래의 사진을 참고해주길 바란다.removeFront functionvoid removeFront() { Node* old = head-&amp;gt;next; old-&amp;gt;next-&amp;gt;prev = head; head-&amp;gt;next = old-&amp;gt;next; delete old;}앞에서 삭제하는 함수는 어렵지 않다. 미리 삭제하는 노드를 저장해두고 그 노드를 활용해서 node들의 연결 관계를 설정해주면된다. 아마 insertFront보다는 덜 헷갈릴 것 같지만 헷갈린다면 그림을 그려서 확인하면 이해가 될 것이다.생각보다 글이 길어지고 있어서 insertBack과 removeBack은 생략하겠다. 한 번 직접 구현해보길 권한다. 원리는 front류 함수들과 비슷하다. 사실 제일 중요한건 단순 insert와 remove이기 때문이다.insert functionvoid insert(iterator iter, int e) { Node* v = new Node(e); Node* curr = iter.getNode(); v-&amp;gt;prev = curr-&amp;gt;prev; v-&amp;gt;next = curr; curr-&amp;gt;prev-&amp;gt;next = v; curr-&amp;gt;prev = v;}큰 동작원리는 insertFront와 동일하다. iter.getNode()는 해당 iterator가 지시하는 노드를 가져오는 함수이다. 이 함수의 정확한 활용도는 iter node의 앞에 데이터가 e인 노드를 삽입하는 것이다.동작원리가 유사하니 순서도 중요하게 맞춰줘야한다.remove functionvoid remove(iterator iter) { Node* del = iter.getNode(); del-&amp;gt;prev = del-&amp;gt;next; del-&amp;gt;next = del-&amp;gt;prev; delete del;}remove도 기존의 삭제 연산과 동작원리가 유사하다. 이쯤되면 약간 눈치빠른 사람들은 알 수 있는데, 삭제연산은 삽입연산에보다 좀 더 순서의 제한이 적다는 것을 눈치챌 수 있다.이유는 간단하다. 새롭게 만들어 주는 노드는 next와 prev를 변경하는 데에 자유로운데, 앞 뒤에 연결관계가 없기때문이다.삽입연산은 앞뒤 노드를 기준으로 새로운 연결관계를 만들어 줘야하지만 제거연산은 상대적으로 주소필드 변경이 자유로운 새 노드를 기준으로 동작되므로 삭제연산이 더 간단하다." }, { "title": "[Data Structure] Queue(큐)", "url": "/posts/Queue/", "categories": "DataStructure, C++", "tags": "DataStructure, Queue", "date": "2020-07-05 00:00:00 +0900", "snippet": "Queue (큐)큐 개념큐의 삽입과 삭제의 원리는 First-In First-Out(FIFO, 선입선출)로 작동한다. 가장 먼저 들어온 데이터가 가장 먼저 나가게된다. 큐의 실제 활용 예시는 다음과 같다. 대기열 공유자원의 접근권한, 순서 멀티 프로그래밍 과정Queue ADT큐의 추상자료형은 다음과 같다. enqueue(object) : 데이터를 큐의 제일 마지막에 삽입한다. dequeue() : 큐의 가장 앞에 있는 데이터를 제거한다. object front() : 큐의 가장 앞에 있는 데이터를 반환해준다. integer size() : 큐에 저장된 원소들의 개수를 반환해준다. boolean empty() : 큐가 비어있는 지를 확인해준다.Queue implementation (큐 구현)큐의 구현방식은 스택과 마찬가지로 2가지 방법이 있다. Array-based Queue Linked List-based Queue역시나 이번에도 2가지 방식 모두 구현해보겠다. 티스토리 블로그에는 링크드 리스트 구현만 할 예정이다.Array-based Queue (배열 기반 큐)ArrayQueue prototype#include &amp;lt;iostream&amp;gt;#include &amp;lt;string&amp;gt;using namespace std;class arrayQueue {public: int* Q; int capacity; int n; int f; int r; arrayQueue(int size); arrayQueue(); int size(); bool isEmpty(); int front(); int rear(); void enqueue(int data); void dequeue();}배열 기반 큐의 프로토타입 함수 종류들이다. 멤버변수에서 포인터 변수인 Q는 실제로 큐의 역할을 저장하는 변수이다. 배열 기반의 큐는 스택과 다르게 보통 원형 큐의 형태로 제작해서 Circular array로 구현을 하는데, 그래서 큐의 성질에 맞게 가장 앞의 인덱스를 알려주는 f와 가장 뒤의 인덱스를 알려주는 r변수를 갖고있다.arrayQueue initializer (생성자)arrayQueue(int size) { this-&amp;gt;n = 0; this-&amp;gt;Q = new int[size]; this-&amp;gt;capacity = size; this-&amp;gt;f = 0; this-&amp;gt;r = -1;}배열기반 큐의 생성자이다. 배열기반이므로 배열의 기본 크기를 정해준다. 그리고 스택에서 top의 index역할과 유사한 것이 r이므로 r은 -1로, f는 0으로 초기화 해준다. f가 이동하는 경우는 dequeue로 가장 앞의 원소를 삭제하는 경우이므로 0번 인덱스부터 시작해야한다.isEmpty, size functionint size() { return n;}bool isEmpty() { return n == 0;}사이즈와 공백 판단 함수는 항상 그래왔듯이 어려운 구현은 아니다.enqueue functionvoid enqueue(int data) { if (size() == capacity) { cout &amp;lt;&amp;lt; &quot;Queue is full\\n&quot;; }else { r++; Q[r % capacity] = data; n++; }}사실 원형 배열로 큐를 구현할 때 f와 r의 위치로 꽉 찼는지 아닌지를 구별하지만 귀찮은 것도 있고 공간을 하나 낭비하므로 n으로 개수를 명명해주는 변수를 만들어주겠다.환형 배열의 원리는 간단하다. mod의 원리는 활용해서 해당 위치에 데이터를 넣어주면 된다.dequeue functionvoid dequeue() { if (empty()) { cout &amp;lt;&amp;lt; &quot;Queue is empty\\n&quot;; }else { n--; f++; f = f % capacity; }}제거 연산은 더 간단하다. f의 인덱스를 변경해주기만 하면 된다. 그리고 혹시 f가 index over가 발생할 수 있으므로 mod연산이 된 것으로 바꿔준다. 어차피 capacity를 넘지 않는 이상 f는 본인 값이 유지가 된다.front, rear functionint front() { return Q[f];}int rear() { return Q[r];}간단한 함수다. 각 각 알맞는 값들을 인덱스로 반환해주면된다.Linked-List based Queue (링크드 리스트 기반 큐)LinkedQueue prototype#include &amp;lt;iostream&amp;gt;using namespace std;class Linkedlist {public: int n; // count of element int capacity; // size of queue Node* f; Node* r; Linkedlist() {} Linkedlist(int n) { this-&amp;gt;n = 0; this-&amp;gt;capacity = n; this-&amp;gt;f = NULL; this-&amp;gt;r = NULL; } int isEmpty(); int front(); int rear(); int size(); void enqueue(int e); void dequeue(); ~Linkedlist() {}};생성자 원리는 크게 일반적인 리스트와 다르지 않으니 생략하고 넘어간다.isEmpty, size functionbool isEmpty() { return n == 0;}int size() { return n;}이 두 함수는 배열기반과 큰 차이는 없다. 그대로 유지해주면 될 듯하다.enqueue functionvoid enqueue() { Node* v = new Node(e); if (n == capacity) { cout &amp;lt;&amp;lt; &quot;Full\\n&quot;; }else{ if (isEmpty()) { f = r = v; }else{ r-&amp;gt;next = v; r = v; } n++; }}실제로 enqueue함수는 링크드 리스트에서 addBack함수와 큰 차이는 없다. 실제로 원리가 리스트의 가장 뒤에 추가하는 것이니…비어있을 때와 아닐 때만 잘 구분해서 구현해주면된다.dequeue functionvoid dequeue() {Node* old = f; if (isEmpty()) { cout &amp;lt;&amp;lt; &quot;Empty\\n&quot;; }else{ f = f-&amp;gt;next; n--; delete old; }}링크드 리스트 큐의 dequeue는 removeFront와 같다. front의 위치를 옮겨주고 이전에 front의 역할을 동적할당 해제를 해주면 된다.front, rearint front() { if (isEmpty()) { return -1; }else { return f-&amp;gt;data; }}int rear() { if (isEmpty()) { return -1; }else { return r-&amp;gt;data; }}front와 rear는 비어있을 때의 조건을 해줄 필요가 있다. 나 같은 경우 구현문제에서는 자연수만 처리하므로 -1일 때를 빈 경우로 처리했다. 실제로는 NULL값으로 체크해도 될 것 같다." }, { "title": "[Data Structure] Stack(스택)", "url": "/posts/Stack/", "categories": "DataStructure, C++", "tags": "DataStructure, Stack", "date": "2020-07-04 00:00:00 +0900", "snippet": "Abstract Data Types (추상 자료형)추상 자료형 (ADTs)는 한마디로 말하면 알고리즘의 요약본이다. ADT는 correctness와 performance를 독립적으로 생각하게 해줄 수 있다.Correctness는 일반적으로 interface라고도 하는데, input이 들어왔을 때, output의 일치 정확도가 얼마나 높은 가를 말한다.Performance는 implementation이라고 하며 time complexity와 같은 것을 말한다.ADT를 짤 때는 해당 알고리즘의 기능만을 고려하며 성능과 자세한 구현을 고려하지 않는다.Stack (스택)스택 개념스택의 삽입과 삭제 원리는 Last-In First-Out(LIFO, 후입선출)로 작동한다. 말 그대로 가장 나중에 들어온 데이터가 가장 먼저 나가게 되는 구조이다.스택이 활용되는 예시는 아래와 같은 것들이 있다. 웹페이지 방문기록 (뒤로가기) : 가장 최근의 방문한 페이지를 우선으로 보여준다. 텍스트 편집 프로그램의 되돌리기 (ctrl+z)기능 C++프로그램에서 코드가 구동되는 시스템 : function call stack이라고 한다.Stack ADT스택의 추상 자료형은 다음과 같다. push(object) : object를 삽입한다. object pop() : object중 가장 마지막에 삽입된 것을 제거한다. object top() : 가장 마지막에 있는 object를 return해준다. integer size() : 저장된 object의 개수를 return해준다. boolean empty() : 스택이 비어있는 지 아닌 지를 반환해준다.위의 ADT를 고려해서 stack interface를 구현해보자.template &amp;lt;typename E&amp;gt;class Stack {public: int size() const; bool empty() const; const E&amp;amp; top() const throw(StackEmpty); void push(const E&amp;amp; e); void pop() throw(StackEmpty);};Stack implementation (스택 구현)스택을 구현하는 방식에는 2가지 방식이 있다. Array-based stack Linked List-based stack두 가지 방식 모두 구현을 해보겠다. 티스토리 블로그에는 아마 링크드 리스트 구현으로만 올라갈 듯 하다.Array-based Stack (배열 기반 스택)ArrayStack prototype#include &amp;lt;iostream&amp;gt;using namespace std;class arrayStack {public: int* S; int capacity; int t; arrayStack(int capacity); arrayStack(); int size(); bool empty(); int top(); void push(int e); int pop();};배열 기반 스택의 프로토타입 함수 종류들이다. 멤버변수에서 포인터 변수인 S는 실제로 저장 역할을 하는 배열이 될 것이다. 배열 기반 스택은 배열의 인덱스를 바꿔주면서 삽입, 삭제, 출력 연산을 진행하는데, 여기서 어떤 값이 가장 최근 인덱스인지를 알려주는 역할을 t변수가 할 것이다. top의 index역할이라고 생각하면 된다. 자세한 설명은 각 함수 구현과 함께 설명하겠다.arrayStack initializer (생성자)arrayStack(int capacity) { this-&amp;gt;capacity = capacity; this-&amp;gt;S = new int[capacity]; this-&amp;gt;t = -1;}스택의 생성자이다. 구현하려는 스택의 크기를 정해주면 그 크기에 맞춰서 스택이 형성된다.아무런 데이터가 없는 경우에는 top의 index를 -1로 지정해준다.push functionvoid push(int e) { if (t == capacity - 1) { cout &amp;lt;&amp;lt; &quot;stack is full\\n&quot;; return; } t++; S[t] = e;}push 함수는 구현이 어려운 편이 아니다. 하지만 후에 구현할 링크드 리스트와 다르게 배열구현 스택은 크기가 정해져 있으므로 스택이 꽉 찼을 때의 예외처리를 해 줄 필요가 있다. 스택이 찼는 지를 확인해준다. 만약 꽉 찼다면 경고해주고 함수를 종료한다. 스택이 차지 않았다면 top index인 t를 증가시켜준다. 증가된 배열의 t index 위치에 값 e를 넣어준다.스택이 꽉 찼는 지를 확인하는 방법은 어렵지 않다. top index가 배열의 크기보다 1개 작으면 되는 것이다. (왜 1개 작냐 생각이 든다면… 기초부터 다시 공부하는 것이 좋을거다.)empty functionbool empty() { return t == -1;}스택이 비었는 지를 확인하는 방법은 쉽다. 앞에서 언급했지만 top의 시작 index값은 -1이다. 즉 t가 -1이라는 것은 스택이 비어있다는 뜻과 같다.pop functionint pop() { if(empty()) { return -1; } int tmp = S[t]; t--; return tmp;}제거연산을 할 때는 반드시 공백여부를 확인해 줘야한다. 만약 스택이 비어있다면 -1을 반환해준다. 비어있지 않다면 우선 기존의 top 값을 저장해준다. top index를 1내려준다. 기존의 top값을 반환해준다.size and top functionint size() { return capacity;}int top() { if (empty()) return -1; else return S[t];}두 함수 모두 간단한 구현이다. 그냥 필요한 값을 반환만 해주면 되기 때문다.Linked list-based StackLinkedStack prototype#include &amp;lt;iostream&amp;gt;using namespace std;class linkedStack {public: int n; SLinkedList* S; linkedStack(); int size(); bool empty(); int top(); void push(int e); int pop();};linked list와 node에 대한 클래스는 이전 글을 확인하거나 tistory블로그에서 확인할 수 있다. 그 두개 코드가 같이 들어가면 너무 길어진다…여기서 n은 전체 스택의 원소개수가 된다.linkedStack initializer (생성자)linkedStack() { this-&amp;gt;S = new SLinkedList; this-&amp;gt;n = 0;}생성자는 간단하게 초기화만 해준다.empty functionbool empty() { return n == 0;}큰 원리는 배열기반과 비슷하다. n이 원소의 개수를 반환해주므로 n이 0개라면 스택이 비어있다는 의미가 되게 된다.push functionvoid push(int e) { S-&amp;gt;addFront(e); n++;}push 함수는 링크드 리스트 구현에서 사용한 addFront함수를 활용하면 된다. 스택은 최근에 들어온 값들만 확인하면 되기때문에 Front관련 함수들만 구현해서 활용하면된다. 링크드 리스트 S에 원소 e를 넣어준다. n 개수를 1 증가시켜준다.pop functionint pop() { if (empty()) { return -1; }else { n--; return S-&amp;gt;removeFront(); }}스택의 공백 여부만 확인을 잘 해주면된다. 우선 전체 원소수를 대표하는 n을 줄여준다. 그 후 S의 removeFront값을 반환해준다.size and top functionint size() { return n;}int top() { if (empty()) return -1; else return S-&amp;gt;front();}같은 원리로 동작해주면 된다." }, { "title": "[Data Structure] Analysis of Algorithms (알고리즘 분석)", "url": "/posts/AnalysisAlgorithm/", "categories": "DataStructure, C++", "tags": "DataStructure, Algorithm Anlaysis, Big-O notation, pseudocode", "date": "2020-07-03 00:00:00 +0900", "snippet": "Algorithm Analysis (알고리즘 분석)Asymtotic Analysis (점근적 분석)알고리즘을 비교, 분석할 때는 일반적으로 점근적 분석방법을 따른다. 점근적 분석방법은 아래의 과정들로 진행된다. 의사코드 (pseudo code) 연산자 개수 카운트 (primitive operation counting) input size n에 대한 함수로 표현 Big-O notation으로 표기위 순서에 있는 내용을 기준으로 이번 글을 다루겠다.Running time (실행시간)알고리즘의 개념적 정의는 아래와 같다. 0개 이상의 입력값이 주어질 때, 1개 이상의 출력이 나오는 절차나 방법이런 정의에서 알 수 있듯이 우리는 특정 입력 값의 크기에따른 출력의 결과를 계산해야한다. 출력결과가 나오기까지 걸리는 시간을 가장 최소로 만들어주는 것이 가장 이상적인 알고리즘 해결방식이다.일반적으로 알고리즘 구동시간은 Best case, Average case, Worst case로 구분을 한다.여기서 프로그래밍을 할 때 가장 중점적으로 바라보는 시간은 Worst case의 경우 이다.Worst case를 중점적으로 바라보는 이유는 그 어떤 시간보다도 가장 오래 걸리는 경우의 시간 이기때문이다. 이러한 접근은 알고리즘을 보수적으로 접근하는 것인데, 이렇게 보수적으로 알고리즘을 접근하면 알고리즘의 구동시간을 보장할 수 있다.쉽게 말하면 Worst case time은 “아무리 못해도 이 시간은 걸린다” 라는 의미를 담고있다.Pseudocode (의사코드)알고리즘을 작성하기 이전에 일반적으로 의사코드를 작성하는 순서를 갖는다.의사코드란 일반적인 문장보다는 구조적으로 적으며 실제 programming 보다는 덜 자세하게 적는 방식을 말한다. 아래가 그 예시이다.Algorithm arryMax(A, n) Input array A of n integers Output maximum element of A currentMax &amp;lt;- A[0] for i &amp;lt;- 1 to n - 1 do if A[i] &amp;gt; currentMax then currentMax &amp;lt;- A[i] return currentMax확실히 일반적인 코드보다는 간단한 구성을 취하고있다. 하지만 일반적인 문장을 풀어쓴 것보다는 프로그래밍 코드에 가깝게 보인다.Counting primitive operation (연산자 카운팅)의사코드로 코드를 구성했으니 이제 해당 코드가 어느정도의 시간이 걸릴지 계산을 해야한다. 계산을 하는 과정에서 연산자들을 카운팅 하게되는데, 대입연산, 계산연산, 비교연산 등이 있다. 위에 작성한 의사코드를 예시로 연산자 카운트를 해보겠다.Algorithm arryMax(A, n) currentMax &amp;lt;- A[0] for i &amp;lt;- 1 to n - 1 do if A[i] &amp;gt; currentMax then currentMax &amp;lt;- A[i] return currentMax currentMax &amp;lt;- A[0]에는 총 1번의 연산이 있는 것 같지만 사실 2번의 연산이 수행된다. 우선 대입연산 1번 과 Array index에 접근하는 연산 1번 이 있으므로 총 2번의 연산을 활용한다. for i &amp;lt;- 1 to n - 1 do에는 2n번의 연산이 수행된다. i가 1부터 n-1까지 진행을 하면 총 n-1번의 연산이 있는 것으로 착각할 수 있는데, 이것은 for문의 수행순서를 잘 뜯어보면 그렇지 않다는 것을 알 수 있다.for문의 구성은 for(int i = 1; i &amp;lt;= n-1; i++)과 같은 식으로 작성을 하는데, i = 1에서 대입연산 1번 , i &amp;lt;= n-1은 n-1까지 i를 비교하고 n에 와서 1번 더 비교를 한다. 결국 비교연산을 n번 수행한다. 그리고 i++인 i의 증가 연산이 총 i가 1에서 n-1까지 증가시키므로 n-1번 증가 연산 이 수행된다. 결국 총 2n번의 수행연산이 이루어진다. 그 다음 if A[i] &amp;gt; currentMax then은 index 연산 1번, 비교연산 1번으로 2번의 연산을 반복 횟수 n-1만큼 진행하므로 2(n-1)의 연산횟수를 가진다. 그 다음의 `currentMax &amp;lt;- A[i]`도 마찬가지로 작동한다. 마지막으로 return currentMax에서 1번의 연산이 수행되므로 이 알고리즘의 총 수행연산은 6n-1번 이다.Growth Rate그래프의 증가율은 매우 중요한 포인트이다. 증가율은 알고리즘의 연산 수에 달려있는데, 그 시간이 어떠냐에 따라 압도적인 차이를 보여주기도한다. if runtime is… time for n + 1 time for 2n time for 4n \\(c \\log{n}\\) \\(c \\log{(n+1)}\\) \\(c (\\log{n}+1)\\) \\(c(\\log{n}+2)\\) \\(cn\\) \\(c(n+1)\\) \\(2c n\\) \\(4c n\\) \\(c n \\log{n}\\) \\(c n \\log{n}+cn\\) \\(2c n \\log{n}+2cn\\) \\(4c n \\log{n}+4cn\\) \\(cn^2\\) \\(cn^2 + 2cn\\) \\(4cn^2\\) \\(16cn^2\\) \\(cn^3\\) \\(cn^3+3cn^2\\) \\(8cn^3\\) \\(64cn^3\\) \\(c2^n\\) \\(c2^{n+1}\\) \\(c2^{2n}\\) \\(c2^{4n}\\) 위에서 볼 수 있듯이 로그연산에서는 큰 증가를 보이지 않지만 다항연산과 지수연산에서는 굉장히 큰 증가폭을 보여주게된다.Constant Factors상수요소들인 단순 산수연산이나 대입연산과 같은 것들은 곱하기 연산들에 큰 영향을 미치지 못한다. 예를 들어 \\(10n^2 + 10^5\\)이라는 식이 있으면 아무리 10^5이 큰 수라고 해도 \\(n^2\\)에 들어가는 수가 어떻게 변하냐에 따라 더 영향이 크게된다.고등학교때 배운 극한에서 n을 무한대로 보낼때 최고차항만 고려해도 되는 것과 같은 이치이다.Big-O notation프로그래밍에서 시간이 얼마나 걸리냐를 나타내는 시간복잡도(time complexity)를 빅오 표기법으로 나타낸다. 우리가 계산하고자하는 함수를 \\(f(n)\\)이라고 하고 비교기준 함수역할을 \\(g(n)\\)이라고하면, \\(f(n) \\in O(g(n))\\)이다. 정확하게 수식으로 나타내면 아래와 같다.\\[f(n) \\leq cg(n) \\text{ for } n \\geq n_0\\]수식으로 볼 수 있듯이 \\(f(n)\\)이 가지는 upper-bound(상한)를 나타내주는 것이 바로 빅오 표기법이다. 그냥 단순히 위의 수식을 만족하는 c와 n이 존재하기만 하면 된다.효율적인 계산 아이디어 구상같은 역할을 하는 알고리즘이라도 우리는 이제 시간복잡도를 고려해야 할 필요가 있다. 사실 시간복잡도 뿐 아니라 공간복잡도(space complexity)도 있는데, 공간복잡도는 현대 기술력으로 어느정도 마무리가 되었다. 그래서 보통 시간복잡도에 공간복잡도가 따라온다고 한다.효율적인 계산을 고려하는 예시는 접두사 평균계산이 있다.Algorithm prefixAverages1(X, n) Input array X of n integers Output array A of prefix averages of X # operations A &amp;lt;- new array of n integers # n for i &amp;lt;- 0 to n - 1 do # n s &amp;lt;- X[0] # n for j &amp;lt;- 1 to i do # 1+2+...+(n-1) s &amp;lt;- s + X[j] # 1+2+...+(n-1) A[i] &amp;lt;- s/(i + 1) # n return A # 1위 의사코드는 1까지의 평균, 2까지의 평균, 3까지의 평균을 계산해주는 알고리즘이다.의사코드를 빅오 표기법으로 나타내면, \\(O(n^2)\\)이다. (\\(\\sum_{i=1}^{n}i=\\frac{n(n+1)}{2}\\)) 이 알고리즘은 현재 2차식의 시간을 가지고 있는데, 아이디어를 바꿔보면 선형시간으로 변경할 수 있다.Algorithm prefixAverages2(X, n) Input array X of n integers Output array A of prefix averages of X # operations A &amp;lt;- new array of n integers # n s &amp;lt;- 0 # 1 for i &amp;lt;- 0 to n - 1 do # n s &amp;lt;- s + X[i] # n A[i] &amp;lt;- s / (i + 1) # n return A # 1맨 처음의 의사코드는 2중 for문을 활용해서 합을 구한다. 하지만 두번째 의사코드는 이전까지의 합을 s에 저장해놓고 불러오는 식으로 계산을 하기때문에 for문 1개를 아낄 수 있어서 선형시간이 나타나게된다. 결국 \\(O(n)\\)의 시간이 걸린다.Big-Omega and Big-ThetaBig-O이외에도 시간복잡도를 나타내는 함수는 2가지가 더 있다. 바로 빅 오메가와 빅 세타 표기법이다. 우선 Big-Omega는 Big-O의 반대격인 lower-bound(하한)를 나타낸다. 수식으로 나타내면 \\(f(n) \\geq cg(n) \\text{ for } n \\geq n_0\\)이다. Big-Theta는 Big-O와 Big-Omega를 합쳐놓은 형태의 시간복잡도이다. 일반적으로 Big-O표기법으로 나타낸다고하는데 사실 우리가 사용하는 Big-O는 Big-Theta를 나타내는 것이다.수식으로 나타내면 \\(c&#39;g(n)\\leq f(n)\\leq c&#39;&#39;g(n)\\)이다." }, { "title": "[Data Structure] Linked List (연결 리스트)", "url": "/posts/LinkedList/", "categories": "DataStructure, C++", "tags": "DataStructure, Linked list", "date": "2020-06-30 00:00:00 +0900", "snippet": "Linked list (연결 리스트)링크드 리스트와 배열Linked list (연결 리스트)와 가장 많이 비교되는 자료구조에는 Array (배열)가 있다. 두 자료구조 모두 linear order data structure로, 선형 저장구조를 갖고 있다. 선형 저장구조를 갖고 있다는 의미는 데이터 간의 전후관계가 존재한다는 의미와도 같다.두 자료구조의 차이점에는 데이터 접근성과 size의 변동성이 있다. Array (배열) 배열은 임의의 인덱스에 빠르게 접근이 가능하다. 접근하고자 하는 index를 알기만 하면 O(1)시간에 접근이 가능하다. 배열을 선언할 때 memory를 선언하고 할당하기 때문에 제한된 메모리 크기를 갖는다. 그로인해 한 번 선언한 배열의 크기는 변경 할 수 없다는 단점이 있다. Linked list (링크드 리스트) 링크드 리스트는 새로운 데이터가 추가될 때, 메모리를 새롭게 할당해주므로 메모리 크기가 동적으로 변화할 수 있다. 데이터를 탐색할 때, 제일 앞의 노드부터 탐색을 하기때문에 특정 값을 찾기 위해서는 O(n)의 시간이 걸린다. Singly Linked list (단순 연결 리스트)단순 연결 리스트는 노드들의 연속 으로 구성되어있다. 각 각의 노드들은 Element(원소)와 Next(다음 노드 포인터)로 구성되어 있고 원소에는 저장하고자하는 object가 들어간다.연결 리스트는 가장 앞의 노드를 가리키는 head와 가장 마지막 노드를 가리키는 tail이 존재하고 가장 마지막 노드의 next포인터의 주소 값은 NULL값을 가진다. object가 저장되는 곳인 element와 다음 노드를 가리키는 node pointer인 next가 존재한다. element를 데이터필드, next를 주소필드라고 한다.Singly Linked list implementation (단순 연결리스트 구현)Singly Linked list code #include &amp;lt;iostream&amp;gt; #include &amp;lt;string&amp;gt; using namespace std; typedef string Elem; class StringNode { private: Elem elem; StringNode* next; friend class StringLinkedList; } class StringLinkedList { private: StringNode* head; StringNode* tail; public: StringLinkedList(Elem e); ~StringLinkedList(); bool empty(); Elem front(); void addFront(Elem e); void removeFront(); void addBack(Elem e); void removeBack(); };Linked list class의 prototype은 위의 코드와 같다. 자세한 함수 내용은 각각의 함수 설명과 함께 작성한다. 여기서 node의 자료형은 string으로 했는데, 원하는 값에 맞춰서 자료형을 변경해주면된다.Empty function bool empty() { retrun head == NULL; }list가 비어있는 지를 확인하는 함수이다. 가장 간단한 방법은 head가 가리키는 포인터가 NULL인지를 확인하는 것이다.Inserting at the Head void StringLinkedList::addFront(Elem e) { StringNode* v = new StringNode(e); if (empty()) { tail = v; } v-&amp;gt;elem = e; v-&amp;gt;next = NULL; head = v; }list의 가장 앞 부분에 데이터를 추가하는 함수이다. 기본 원리의 순서는 아래와 같다. 데이터를 추가할 새로운 노드 v를 만들어주고 값들을 지정해준다. 만약 리스트가 비어있다면, tail node는 새로 생성된 노드를 할당해준다. 이전에 있던 head node를 새로 만들어진 노드 v를 할당해준다. (데이터 삽입)Removing at the Head void StringLinkedList::removeFront() { if (empty()) { tail = NULL; return; } StringNode* old = head; head = old-&amp;gt;next; delete old; }list 가장 앞의 값을 제거해주는 함수이다. 삽입 함수에서 empty조건 확인은 있냐 없냐가 크게 문제가 되지 않지만 제거 함수에서는 반드시 empty조건 확인으로 예외처리를 진행해줘야한다. 리스트가 비어있는 지 확인한다. 만약 비어있다면 함수를 종료시켜준다. 리스트가 비어있지 않으면 기존의 head node를 새로 만드는 old node에 할당해준다. 이렇게 되면 old node와 현재의 head node는 동일하게 된다. 그 후 head node를 기존의 head node 다음 노드로 이동시켜준다. 이때, 기존의 head node 역할은 old node가 대체한다. (데이터 제거) 4. 기존의 head node역할을 하던 node인 old node를 메모리에서 제거한다. (실제로 메모리에서 제거)여기서 7번째 줄에서 어차피 head 지우는데 굳이 old를 만들어서 head를 옮겨야하나?라는 생각이 들 수 있는데, 여기서 old를 안 만들고 코드를 짜면 head = head-&amp;gt;next;로 하게 된다. 이렇게 되면 기존에 존재하던 head node 역할을 하는 node를 메모리에서 지워줄 수 없게 된다.Inserting at the Tail void StringLinkedList::addBack(Elem e) { StringNode* v = new StringNode; v-&amp;gt;elem = e; v-&amp;gt;next = NULL; if (empty()) { head = tail = v; }else { tail-&amp;gt;next = v; tail = v; } }이번에는 tail에 값을 삽입해주는 함수이다. 삽입할 값을 가지고 있는 새로운 노드 v를 만들어준다. 이때, v의 next node는 NULL이다. (마지막 노드 역할이기 때문이다.) 리스트가 비어있다면 head와 tail은 새로 만들어진 노드가 된다. 비어있지않다면 기존의 tail node의 다음 노드는 새로 만든 v노드로 해준다. (데이터 삽입) tail node를 삽입한 노드 v로 해준다. (실제로 데이터 삽입)Removing at the Tail void StringLinkedList::removeBack() { if (empty()) return; StringNode* current = head; if (current == tail) { head = tail = NULL; delete current; }else { while (current-&amp;gt;next != tail) { current = current-&amp;gt;next; } tail = current; delete til-&amp;gt;next; tail-&amp;gt;next = NULL; } }링크드 리스트에서 데이터를 제거할 때는 위에서 언급한 링크드 리스트의 성질에 의해서 head부터 시작해서 tail의 바로 앞 노드까지 이동해야한다. 배열처럼 바로 인덱스로 이동할 수 없기때문에 tail의 remove는 $ O(n)$ 시간이 걸리게 된다. head에서 데이터를 제거해줄 때처럼 예외처리를 진행해줘야한다. 리스트가 비어있는 지 확인한다. 만약 비어있다면 함수를 종료시켜준다. tail node의 바로 앞까지 이동할 노드의 역할인 current node를 만들고 head node와 동일하게 만들어준다. 만약 시작부터 current와 tail이 같으면 리스트의 길이가 1이라는 의미기때문에 head가 곧 tail의 역할이다. 그래서 head node와 tail node를 둘 다 NULL로 바꾸고 current node(기존의 head이자 tail)를 메모리 제거해준다. 그 외의 경우는 current node를 이동시기는데, 반복기준은 curren node의 next node가 tail node가 되기 전까지만 반복해서 이동을 해준다. tail node 바로 전까지 이동을 했다면, tail node를 current node로 바꿔준다. 이렇게 이동한 tail node의 next node는 기존의 tail node이므로 새롭게 바꾼 tail node의 next node를 메모리 제거해준다. 마지막으로 새롭게 지정된 tail node의 next node를 NULL로 지정해준다." }, { "title": "R Programming 12주차", "url": "/posts/R12/", "categories": "Statistics, R programming", "tags": "R, linear_regression", "date": "2020-06-24 00:00:00 +0900", "snippet": "월요일회귀분석 1 (Regression) 회귀분석 1 회귀분석을 하는 이유는 크게 2가지 정도로 구분할 수 있다. 두 변수 사이에는 선형으로 표현되는 관계가 있는가? -&amp;gt; 사회적관점 한 변수를 통해 다른 변수를 예측할 수 있을까? -&amp;gt; 공학적관점 회귀분석의 변수는 2가지가 있다.\\(y = \\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2} + \\epsilon\\) 독립변수 (independent variable) 설명변수 (explanatory variable)이라고도 부르기도 하며 종속변수에 영향을 주는 변수이다. 위의 식에서는 $ x_1$와 $x_2 $가 독립변수이다. 독립변수가 한 개면 단순회귀, 여러 개면 다중회귀분석이 된다. 종속변수 (dependent variable) 반응변수 (response variable)이라고 부르며 관심대상이 되는 결과를 나타내는 변수다. 위의 코드에서는 $ y $가 종속변수에 해당한다. 회귀계수 (regression coefficient) 위 공식에서 $ \\beta_0 , \\beta_1 , \\beta_2 $를 회귀계수라 부르고 곱해져있는 독립변수에대한 기울기 정도로 해석이 가능하다. 두 변수 사이의 관계 측정도구 산점도 (scatter plot) 이전에 10주차에서 다뤘던 산점도이다. 두 변수의 대략적 관계파악에는 효과적인 도구지만 정확한 수치적 관계파악은 어렵다. 공분산 (covariance) \\(\\sigma_{XY} = \\text{Cov}(X,Y) = E[(X - E(X))(Y - E(Y))]\\) 두 변수 사이의 연관성과 방향은 알 수 있으나 크기의 비교가 어렵다. 상관계수 (correlation) \\(\\rho = \\text{Corr}(X,Y)=\\frac{\\sigma_{XY}}{\\sigma_{X}\\sigma_{Y}}\\) 공분산을 각각의 표준편차로 나눈 값으로 상대적인 비교가 가능하다. 이때, 상관계수는 단순회귀분석에서 회귀계수와 관련성이 높다. 여기 있는 변수들의 자세한 설명은 10주차 정리본을 참고하길… 회귀분석 2 (Regression) 단순회귀분석 (Simple linear regression) \\(Y=\\beta_0 + \\beta_{1}X_{1} + \\epsilon\\) 간단한 설명은 앞에서 했고 여기서는 $ \\epsilon $이 무엇이고 무슨 역할인지를 말해보겠다. $ \\epsilon $은 오차항이라고 부르며 학부수준에서는 유일하게 회귀분석에서 확률적 성질을 갖는 값이다. 오차항 $ \\epsilon$은 $i.i.d\\;N(\\delta, \\sigma^2) $를 따른다고 가정되어있다. 여기서 $ i.i.d\\; N() $는 independent and identically distribution으로 독립이면서 같은 분포인 정규분포를 말한다. 선형회귀분석에서 뿐 아니라 중회귀분석에서도 오차항의 존재로 평균선기준에서 위아래로 데이터가 분포하게 된다. 중회귀분석 (Multiple linear regression) \\(Y=\\beta_0 + \\beta_{1}X_{1} + \\beta_{2}X_{2} + ... + \\beta_{p}X_{p} + \\epsilon\\) 중회귀분석에서 $ Y, x_{1}, x_{2}, …, x_{p} $가 모두 확률적 성질을 갖게되면 결합분포 (joint distribution)를 활용해야하나, 학부수준에서는 x가 주어졌을 때 Y의 조건부 기댓값만을 고려하므로 확률적 성질을 갖는 것은 오차항이 유일하다.데이터 표준화 데이터 표준화 \\(Z = \\frac{X - \\overline X}{s}\\) $ Y $를 설명하기 위해서 설명변수인 $ X_1, … X_p $들 사이에 variable이 크거나 하면 회귀계수의 영향력을 설명하기가 부담스럽거나 어려운 경우가 많다. 이를 해결하기 위해 데이터의 표준화 과정을 거치게 된다. R에서 함수는 scale(vector, center = T or F. scale = T pr F)이다. center는 위의 공식에서 $\\overline X$의 유무를 결정하고 scale은 $s$의 유무를 결정한다. center = T, scale = F : $ X - \\overline X $ center = F, scale = T : $ X / S $ 표준화를 하게되면 스케일이 통일되서 회귀계수에 대한 변수의 영향력이 감소된다. 결과적으로 전체적인 변수가 비슷한 범위 내에서 움직이므로 회귀계수가 클수록 강하게 Y에 영향을 줄 수 있다. 기존의 회귀분석 수식에서 $ \\beta_0 $의 추정량이 0으로 고정된다. 따라서 절편이 없는 회귀 모형을 나타내게 된다. linear model function linear model function (lm( )) 회귀분석을 구현해주는 함수가 R에서 존재한다. lm(formula, options) 의 기본 파라미터를 갖는다. formula에는 회귀모형이 들어가게 된다. formula 작성방법 모형 formula 형식 $ y_i = \\beta_0 + \\beta_{1}x_{i} + \\epsilon_{i}$ y~x $ y_i = \\beta_{1}x_{i} + \\epsilon_{i} $ y~x-1 or y~0+x $ y_i = \\beta_0 + \\beta_{1}x_{i1} + \\beta_{2}x_{i2} + \\epsilon_{i} $ y~x1 + x2 $ y_i = \\beta_0 + \\beta_{1}x_{i1}x_{i2} + \\epsilon_{i} $ y~x1:x2 $ y_i = \\beta_0 + \\beta_{1}x_{i1} + \\beta_{2}x_{i2} + \\beta_{3}x_{i2}x_{i3} + \\epsilon_{i} $ y~x1*x2 lm( ) indy = 8indx = 200x = expr_dat[,indx]y = expr_dat[,indy]fit = lm(y~x)summary(fit)plot(x,y,pch=16)abline(fit,col=2,lwd=1.5) lm()을 사용하면 회귀분석 결과 분석이 가능한 데이터를 반환해준다. 아래는 lm function 출력 결과이다. 일단 Residuals는 한국어로는 잔차라고 하는데, $ r_i = y_i - \\widehat y_i $의 값을 갖는다. Coefficient가 있는데, 각각의 회귀 계수에 대한 가설검정 결과를 보여준다. 양측검정 결과를 보고 해당 회귀계수가 유의한지 아닌지를 결정할 수 있는데, 여기서 절편은 유의성을 보고 제외할 지 말지 결정할 때 주의를 해야한다. 위의 코드를 예로 설명을 해보면, $ Y=\\beta_0 + \\beta_{1}x + \\epsilon $의 공식과 양측검정 결과를 확인해보면 절편은 유의하지 않고 $ \\beta_1 $은 유의한 것으로 보인다. 절편이 유의하지 않은 회귀계수라고 함부로 제거할 수 있을까? 절편이 아닌 독립변수에 있는 유의하지 않은 회귀계수를 제거하는 것은 크게 상관이 없다. 하지만 절편이 유의하지 않다고해서 함부로 지우는 것은 문제가 발생할 수 있다. 만약 $ x $와 $ E(Y|x) $에서 $ x=0 $일때, 반드시 (0,0)을 지날 필요가 있다면, 절편을 제외 하는 것이 제약조건으로 들어간다. 하지만 (0,0)을 지나는 것이 의미가 없으면 유의하지 않더라도 절편은 남겨둔다. abline()에 lm함수 결과를 넣으면 lm함수의 결과를 활용해서 직선을 그려주게된다. names(fit)을 활용해서 필요한 데이터를 가져올 수 있다. $ \\beta_0 + \\beta_{1}x $를 한 것을 알려주는 것이 fit$fitted.values이다. 예측값 계산 예측값 계산 pred1 = predict(fit, newdata = data.frame(x=2.3)) pred1 est = coef(fit);x1 = 2.3 y1 = est[1] + est[2]*x1 y1 pred2 = predict(fit, newdata = data.frame(x = c(1,2.2,6.7))) pred2 predict 함수를 사용하면 lm에서 사용한 formula인수의 형태에 맞춰서 신규 x값에대한 y값을 추정할 수 있다. 회귀모형 비교 회귀모형 비교 일반적으로 회귀분석을 할 때는 변수선택이 중요한 과정에 속하게 된다. 이를 Best subset selection이라고 하는데, 변수가 n개라고 하면 이 변수로 가능한 모든 회귀모형의 갯수는 $ 2^n $개가 된다. 그래서 변수 선택을 적당히 잘해줘야한다. 이 글에서는 reduced model과 full model을 비교할 것이다. full model 기준 $ Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon $에서 $ \\beta_2 x_2 $를 넣어야하는가 말아야하는가를 알아야할 필요가 있다. 두 회귀모델을 만들고 anova()함수에 넣어서 비교를 할 수 있다. 이 경우 $ H_0 : \\beta_2 = 0 $이고 $ H_1 : \\beta_2 \\neq 0 $이다. 예시코드로 설명을 해보겠다. x2 = c(1,2.2,6.7) indx = c(10,30,200) indxr = c(10,200) y = est[1]+ est[2]*x2 xf = expr_dat[,indx] xr = expr_dat[,indxr] y = expr_dat[,indy] fit1 = lm(y~xf) fit2 = lm(y~xr) anova(fit2, fit1) anova함수 실행결과는 다음과 같다. F-test결과로 나온 p-value를 보면 0.1912로 유의수준 0.05보다 크기때문에 위에서 제시한 귀무가설을 기각할 수 없다. 이거는 결과적으로 해석해보면 회귀계수 $ \\beta_2 $가 0이어도 전체적인 회귀모델에는 크게 상관이 없다고 볼 수 있다. 문자열로도 회귀모델을 비교할 수 있다. ftxt = paste0(uq_names[indy], &#39;~&#39;, paste0(uq_names[indx], collapse=&quot;+&quot;)) ftxtr = paste0(uq_names[indy], &#39;~&#39;, paste0(uq_names[indxr], collapse=&quot;+&quot;)) ftxt ftxtr colnames(expr_dat) = gsub(&quot;[ .]&quot;,&quot;&quot;,uq_names) lm_dat = data.frame(expr_dat) fit1 = lm(as.formula(ftxt), data=lm_dat) fit2 = lm(as.formula(ftxtr), data=lm_dat) anova(fit2, fit1) 위의 코드 출력결과는 아래와 같다. 여기서 이제 p-value가 역시나 축소모형을 지지하므로 ANXA8은 모형에서 제외해도 큰 상관이 없음을 알 수 있다. 사실 위의 코드는 첫번째 예시 코드와 동일한 코드이다. 단지 formula 해석의 편의성이 문자열로 작성하면 훨씬 편리해지는 것을 알 수 있다. 모형진단 - 오차항의 가정 오차항 가정 오차항의 등분산성$ (\\widehat y_i, e_i) $의 산점도를 활용한다. 등분산성이란 $ e_i $를 기준으로 데이터가 얼마나 균일한 간격으로 퍼져있는 지를 말한다.여기서 $ e_i$는 잔차로 $r_i = y_{i} - \\widehat y_{i} $이다. 오차항의 독립성잔차를 사용해서 패턴이 존재하는 지를 확인한다. 만약 데이터를 봤는데, 패턴이 존재하면 오차항이 독립적이지 않을 경우가 많다. 그래서 패턴이 있는 지 없는 지를 확인해야한다. 오차항의 정규성x축을 정규분포의 quantile로, y축 자료의 quantile로 두고 점을 찍어봤을 때, 직선에 가깝게 나올 경우 데이터가 정규분포를 따른다고 볼 수 있다. 잔차의 산점도 예제 rsd = resid(fit2) # resid return e ft = fitted(fit2) # fitted return y_hat plot(ft, rsd, type = &#39;p&#39;, pch=16) hist(rsd, breaks=20) 실행결과를 보면 아래와 같다. 히스토그램을 보면 정규분포를 따름을 알 수 있고, 산점도를 보면 패턴이 있다고 보기는 어렵다. QQ plot qqnorm(rsd) qqline(rsd, col=2, lwd=2) 잔차의 qq plot을 찍어보면 아래와 같이 나오게 된다. 당연히 위에서 정규분포를 따름을 확인했으므로 qq plot은 직선을 띄게 된다. " }, { "title": "R Programming 11주차", "url": "/posts/R11/", "categories": "Statistics, R programming", "tags": "R, Distribution, hypothesis", "date": "2020-06-23 00:00:00 +0900", "snippet": "월요일분포함수 분포함수 일반적으로 분포함수를 사용하는 방법은 (키워드 + 분포이름)의 형식으로 사용한다.키워드는 다음과 같이 종류가 있다. d : Density function (밀도 함수) q : Quantile function (분위수 함수) p : Cumulative Density function (누적밀도함수) r : Generate Random Numbers from such distribution (난수 형성) 정규분포함수 (Normal distribution) 정규분포함수 dnorm(x, mean=m, sd=s) dnorm()은 $ X \\sim N(m, s^2) $을 따르는 정규분포 곡선의 함수를 $ f(x) $라 하면, 해당 함수의 x에 해당하는 값을 반환해준다. qnorm(x, mean=m, sd=s)는 $ X \\sim N(m, s^2) $을 따르는 정규분포 곡선에서 $ \\text{P}(\\text{X} \\leq x)=\\text{x} $인 $ x $를 반환해준다. 쉽게 말하면 정규분포확률에 해당하는 값을 반환해준다. pnorm(x, mean=m, sd=s)는 $ X \\sim N(m, s^2) $를 따르는 정규분포곡선에서 $ \\text{P}(\\text{X} \\leq x) = F(x)$라 할때, $ F(\\text{x}) $를 반환해준다. rnorm(num, mean=m, sd=s)는 $ X \\sim N(m, s^2) $인 정규분포 내에서 num개만큼의 난수를 만들어준다. 균일분포함수 (Uniform distribution) 균일분포함수 우선 균일분포란, 아래와 같이 $ p(x) $의 그래프가 정의된 구간에서 같은 높이로 주어지는 분포를 말한다. 일반적으로 구간 $ \\theta_1, \\theta_2 (\\theta_1 &amp;lt; \\theta_2) $에서 정의된 경우, $ U(\\theta_1, \\theta_2) $로 나타낸다. 좀 더 수식적으로 접근을 하면 다음과 같다. \\(if\\;\\;\\text{X}\\sim U(a,b),\\;\\; Let\\;f(x)=c\\) \\(\\int_{a}^{b}f(x)dx=1\\) \\(\\therefore c=\\frac{1}{b-a}\\) 위의 이미지로 설명예시를 하겠다. dunif(x, min=0, max=1)는 그림과 같이 균일분포가 주어졌을때, x의 함수값을 알려준다. 즉 쉽게 말하면 dunif는 위의 수식에서 $ c $ 값을 반환해준다고 생각하면된다. qunif(x, min=0, max=1, lower.tail=T or F)는 분위수를 계산해준다. 이때, lower.tail의 값에따라 분위수가 바뀌는데, 누적분포함수(CDF, Cumulative Distribution Function)를 활용해서 분위수를 계산한다. lower.tail=T이면 $ \\text{P}(\\text{X}\\leq x) $의 확률값을 반환해주고 lower.tail=F이면 $ \\text{P}(\\text{X} &amp;gt; x) $의 확률값을 반환해준다. punif(x, min=0, max=1, lower.tail=T or F)는 $ \\text{F}(x) $를 CDF라하면, lower.tail=F일때는 $ 1-\\text{F}(x) $를 반환해준다. lower.tail=T이면 CDF 그 자체를 계산해준다. runif(num, min, max)는 $ [min, max] $의 정의역 범위에서 num개의 난수를 만들어준다. 초기하분포 (Hypergeometric distribution) 초기하분포 초기하분포란, N개의 특성값으로 이루어진 유한모집단이 두가지 속성만 갖는다. 관심속성을 갖는 특성값이 전체 N개 중에서 D개 포함되어 있다고 할 때, N개 중에서 n개의 표본을 추출하여 관심속성의 특성값이 나오는 수의 분포를 말한다. 수식으로는 아래와 같이 나타낼 수 있다. \\(\\text{X}\\sim HG(N, D, n) \\text{ means}\\) \\(\\text{P}(\\text{X}=x)=P(x) = \\frac{(_{\\;x}^{D})(_{\\;n-x}^{N-D})}{(_{n}^{N})}\\) 사실 이게 이렇게 말하면 어려워보이는데 흔히 자주 보는 확률문제랑 똑같다. 이런 유형의 대표적인 문제가 다음과 같다. ex) 주머니 속에 흰 공 5개, 검은 공 3개가 있다. 주머니에서 공3개를 비복원 추출로 꺼낼 때, ~~~ 흰 공이 x개일 확률은? : dhyper(x, D, N-D, n, log=T or F)는 $ HG(N, D, n) $일때, $ P(\\text{x}) $와 같다. 흰 공이 x개 이상일 확률은? : phyper(x, D, N-D, n, lower.tail = T or F, log.p=T or F)는 누적분포를 계산해준다. 역시나 lower.tail의 역할은 동일하다. 비복원 추출로 공을 뽑은 확률이 p일때, 몇 번 추출했는가? : qhyper(p, D, N-D, n, lower.tail = T or F, log.p = T or F)는 dhyper의 반대동작으로 생각하면 된다. 특정확률이 나오기 위해서는 몇번의 추출이 이뤄지는 지를 알아내준다. rhyper(nn, D, N-D, n)는 해당 초기하분포를 기반으로 nn번 모의 실험을 진행하는 것이다. nn값이 클수록 rhyper로 나온 값으로 확률 계산을 하면 dhyper에 수렴한다. 이항분포 (Binomial distribution) 이항분포 이항분포는 베르누이시행 (독립시행)을 반복하여 관심사건이 몇 번 나왔는 지를 활용한 확률분포다. 수식으로 표현하면 다음과 같다. \\(\\text{if}\\;\\text{X}\\sim B(n,p)\\) \\(p(x) = (_{x}^{n})p^x(1-p)^{(n-x)}\\) \\(\\sum_{x=0}^{n}p(x)=\\sum_{x=0}^{n}(_{x}^{n})p^x(1-p)^{(n-x)} = (1+(1-p))^n=1\\) dbinom(x, n, p, log =F or T)는 $ p(\\text{x}) $값을 구해준다. pbinom(x, n, p, lower.tail = T, log.p)는 $ P(\\text{X} \\leq \\text{x}) $와 같다. qbinom(p, n, p, lower.tail = T, log.p)는 역시나 dbinom의 반대 과정이다. rbinom(nn, n, p)은 역시나 모의추출로 난수형성이다. 카이제곱분포 (Chi-squared distribution) 카이제곱분포 카이제곱분포(chi-squared distribution)는 정규분포와 관련이 깊다. k개의 서로 독립적이고 표준정규분포를 따르는 확률변수 X를 제곱한 값들을 합하였을 때 분포이며 k는 자유도라고하고 카이제곱분포의 parameter역할을 하게된다. 보통 독립성 검정을 할 때 사용하고 모집단의 분산에 대한 추정과 검정을 할 때 사용한다. 아무래도 분산비교를 하는 분포다보니 전체적인 분포의 형태는 F-분포의 형태를 띄게 된다. 자유도라는 개념이 참 와닿지는 않는데, 검색하다가 괜찮은 예시가 있어서 가져왔다. ‘자유도’를 아주 간단한 예를 들어 설명하자면, 5개의 숫자 평균이 3이라고 해봅시다. 이 때 5개 중 4개의 숫자는 마음대로 고를 수 있지만, 마지막 1개의 숫자는 반드시 정해져 있습니다. 이런 경우에 자유도가 4라고 합니다. 수식으로 나타내면 아래와 같다. \\(V = {Z_1}^2+{Z_2}^2+{Z_3}^2+...+{Z_k}^2 \\Rightarrow V \\sim \\chi^2(k)\\) \\(E(V) = k,\\quad Var(V)=k^2\\) \\(\\frac{\\sigma^2}{(n-1)s^2}\\sim\\chi^2(n-1)\\) 함수 사용법은 나머지 함수들과 동일한데, 좀 다른 parameter가 추가적으로 있다. ncp라는 parameter인데, Non centrality parameter로 평균이 0이 아닌 경우에 처리를 해주는 parameter이다. (사실 카이제곱분포는 무슨 말인지 아직도 잘 안 와 닿는다… 그래서 차마 설명을 못적겠다.) t-분포 (Student’s t-distribution) t-분포 t분포는 일반적인 상황에서 정규분포계산을 위해서 존재하는 분포이다. 정규분포에서는 일반적으로 모분산($ \\sigma^2 $)을 알고있다고 가정한다. 하지만 현실은 모분산을 알이 위해서 표본추출을 진행하는데, 이때 표본분산($ s^2 $)이 사용된다. 이 경우에는 표준정규분포인 z값을 사용할 수 없기때문에 표본확률분포 중 하나인 T-통계량인 t-분포를 사용한다. 자유도를 모수로 갖고있기 때문에 자유도가 높을수록(=표본의 수가 많을수록) 중심극한의 정리에 의해서 정규분포에 근사하게 된다. 수식은 보통 아래와 같이 나타낸다. \\(Z\\sim N(0,1),\\; V\\sim\\chi^2(k),\\;Z\\text{ and }V\\text{ is independent}\\) \\(\\Rightarrow T=\\frac{Z}{\\sqrt{V/k}}\\sim t(k)\\) dt(x, df, log=F or T)는 일반적인 t-분포의 값을 보여준다. 여기서 df는 degrees of freedom으로 자유도를 나타낸다. 즉 표본 갯수를 말해준다. 기본적으로 t-분포는 정규분포의 틀을 가지고 있기때문에 함수들의 반환값은 norm function과 유사하게 해석이 가능하다. F-분포 (F-distribution) F-분포 F-분포는 두 정규모집단의 분산을 비교할 때 사용하는 분산 분석에 사용한다. 수식은 다음과 같다. \\(V_1\\sim\\chi^2(k_1),\\quad V_2\\sim\\chi^2(k_2),\\quad V_1,V_2 \\text{ is independent}\\) \\(\\Rightarrow F=\\frac{V_1/k_1}{V_2/k_2}\\sim F(k_1,k_2)\\) \\(\\text{ : F-distribution with degrees of freedom }(k_1,k_2)\\) df(x, df1, df2, log=F or T)의 형태로 사용된다. 카이제곱분포와 함께 아직 이해가 안되는 분포이다. 그나마 카이제곱분포보다는 이해가 되는데, 일단은 어디에 쓰이는 지는 이해가 된다. 분산분석이란 일반적으로 문제를 위해서 만들어진 정규분포는 평균이 다르고 분산값이 같은 경우가 많았다. 하지만 현실의 데이터는 평균도 다르고 분산도 다를 수 밖에 없다. 분산이 다르게 될 경우 두 모집단을 비교하기가 어렵다. F-분포 식을 보면 두 집단의 분산을 나누는 것을 알 수 있는데, F-분포 그래프를 그렸을때, 1 부근의 값이 많이 나올수록 서로 비슷하거나 연관성이 있다고 보게 된다. 중심극한정리 (Central Limit Theorem) 중심극한정리 df = 4 niter = 1000 xm &amp;lt;- rep(0, niter) for (i in 1:niter) { X &amp;lt;- rchisq(100, df=4) xm[i] = (mean(X)-4)/(sqrt(2*4)/sqrt(100)) } hist(X, breaks=20, main = expression(chi^2~(4)), col = &#39;lightblue&#39;) x11() hist(xm, breaks=20, main = expression(over(bar(X)-mu, sigma/sqrt(n))),col = &#39;gray&#39;, xlab=&#39;normalized sample mean&#39;) 가설검증 시에 굉장히 중요한 이론이다. 정규분포관련 공부할 때 가장 많이 언급되는 말이 있다. 표본평균들의 분포는 정규분포를 이룬다. 이 말이 곧 중심극한정리를 한 문장으로 요약한 것이다. 코드를 간단하게 설명을 해보자. niter는 총 사용되는 표본평균 갯수이다. 이때, 표본평균은 자유도가 4인 카이제곱분포를 100개 모의추출로 활용한다. xm에 들어가는 값은 $\\frac{\\overline{X}-\\mu}{\\sigma/\\sqrt{n}}$가 되게 된다. 목요일가설검정이론 가설검정이론 \\(H_0 : \\mu = \\mu_0\\text{ (Null hypotheis)}\\) \\(H_1 : \\mu = \\mu_1\\text{ (Alternatice hypothesis)}\\) 통계학에서는 어떤 상황을 판단할 때, 가설을 세우고 가설검정을 진행한다. 이때 가설은 보통 2가지가 있는데, 귀무가설($ H_0 $, Null hypothesis)과 대립가설($ H_1 $, Alternative hypothesis)가 있다. 대립가설은 검정과정을 통해 귀무가설이 기각된 것을 확인하면 대립가설이 채택된다. 통계학 수업의 전반적인 흐름은 크게 3가지로 나눠진다. 모집단 표본 추정 모집단을 공부하면서 전체 데이터의 정보를 다루게 되고, 표본 과정을 통해 표본 데이터를 다루는 법을 배운다. 모집단의 데이터를 알기 위해서 통계학적 계산을 하므로 표본으로 얻은 데이터를 통해 모집단을 추정하게 된다. 추정과정에서 가설이 필요한 이유는 간단히 말하면 우리가 가진 표본평균이 정확한 값이 아니기 때문이다. 표본을 배울 때 가장 먼저 배우는 것 중 하나가 바로 표본평균($ \\overline X $)이다. 물론 표본평균의 평균은 모평균의 평균과 같지만 각각의 표본평균은 실제 모평균과 다른 값들을 갖게된다. 즉 표본평균은 일종의 변수인 것이다. 표본평균을 평균이라고 두고 가설검정을 하게 되면 기준점이 너무 불안정하기 때문에 경우에 따라서 대립가설의 기각 여부가 달라진다. 그래서 우리는 $ \\mu = \\mu_0 $라고 가정을하고 가설검정을 하게된다. 그렇다면 왜 귀무가설의 기각여부를 확인하는 것일까? 위의 그래프에서 표본평균들이 정규분포 상에서 굉장히 확률이 낮은 구간에서 포착이 되었을 때, “이 가설이 맞을 가능성이 낮다”고는 확실하게 말할 수 있다. 하지만 과연 $ \\mu_0 $에 가깝다고 이 가설이 맞을 가능성이 높다고 단정하기는 어렵다. 왜냐하면 사실 $ \\mu_0 $도 결국에는 가설로 세운 정확하지 않은 수치 값이다. (모든 표본을 다 조사하는 건 현실에선 불가능 하니까… 그리고 일반적으로는 모평균에서 약간의 차이를 가진 값으로 보는 것 같다.) 만약, 우리가 대립가설로 세운 $ \\mu_1 $에 우연하게 표본평균들이 가깝게 나온다면 그것만으로 확실하게 귀무가설이 틀렸다고 할 수 있을까? 아마 쉽게 단정짓기는 어려울 것이다. 보통은 대립가설이 귀무가설이 아니다~ 라는 식으로 흘러가기 때문에 좀 더 실질적으로는 이해가 어려운 편은 아니다. 그래서 가설검정에서 보통 귀무가설이 틀린 강한증거 라는 방향으로 얘기를 하게된다. 가설검정 조합 종류는 아래와 같다.\\(\\text{단순가설 vs 단순가설}\\)\\(H_0 : \\mu=\\mu_0,\\quad H_1 : \\mu = \\mu_1\\)\\(\\text{}\\)\\(\\text{단순가설 vs 복합가설}\\)\\(H_0 : \\mu=\\mu_0,\\quad H_1 : \\mu \\neq \\mu_0\\quad(H_1 : \\mu &amp;gt; \\mu_0)\\)\\(\\text{}\\)\\(\\text{복합가설 vs 복합가설}\\)\\(H_0 : \\mu \\leq \\mu_0,\\quad H_1 : \\mu &amp;gt; \\mu_0\\)제일 마지막의 두 조합은 2번째 조합으로 대치할 수 있음이 알려져있다. 오류 (error) 오류 제 1종 오류 (type I error, \\(\\alpha\\) ) : $ \\text{P}(H_{0}기각 | H_{0}사실) $ 제 2종 오류 (type II error, \\(\\beta\\) ) : $ \\text{P}(H_{0}기각 | H_{1}사실) $ 단측, 양측검정 단측검정 (one-sided text) 양측검정 (two-sided text) 검정통계량 검정통계량 \\(z = \\frac{\\overline X - \\mu_{0}}{\\sigma / \\sqrt{n}}\\approx N(0,1)\\) \\(\\text{}\\) \\(t = \\frac{\\overline X-\\mu_0}{s/\\sqrt{n}}\\sim t(n-1)\\) z-통계량은 정규화가 된 상태에서 표준정규분포에 적용해서 검정할 때 사용하고, t-통계량은 분산을 모를 때 사용하는 것이다.임계값과 p-value 임계값 임계값은 기각역을 지정해주는 값이 된다. 보통 검정통계량이 임계값에 들어가게 된다. 유의확률 (p-value) 유의확률은 검정통계량으로부터 계산되어 나온 값이다. 분포 그래프에서 검정통계량을 이용해서 대립가설을 지지하는 방향으로 나올 확률을 말한다. 즉 기각이 되었을 때 어느정도 수준으로 기각을 하는 지를 말해준다. 귀무가설이 신뢰구간을 벗어날 확률이다. 가설검정코드 가설검정코드 t1 = t.test(expr_dat[,1], mu=7, conf.level = 0.95, alternative = &quot;two.sided&quot;) t2 = t.test(expr_dat[,1], mu=6.5, conf.level = 0.95, alternative = &quot;less&quot;) t3 = t.test(expr_dat[,1], mu=6.5, conf.level = 0.95, alternative = &quot;greater&quot;) t1; t2; t3 t-검정을 하는 함수이다. alternative = c(&quot;two sided&quot;, &quot;less&quot;, &quot;greater&quot;)로 각각 수식으로는 다음과 같다. “two sided” : $H_1 : \\mu \\neq m_0$ “less” : $\\mu &amp;lt; \\mu_0$ “greater” : $\\mu &amp;gt; \\mu_0$ 여기서 나름 중요한 부분은 conf.level 파라미터이다. 생긴 것 그대로 신뢰구간(confidence level)을 정해주는 것인데, 이게 왜 중요하냐면 t-분포를 활용한 검정에서는 유의수준($\\alpha$)을 활용한다. 유의수준이란, 제 1종 오류를 범할 확률의 최댓값을 말하며 일반적으로 $\\alpha$값은 5%나 1%를 사용한다. 보통 1종 오류를 2종 오류보다 심각하게 다룬다. 억울한 죄인을 만들 수 있기때문이다. (이 말은 가설검정을 진행할 때, 결과가 이 정도까지 벗어나면 귀무가설이 오류라고 인정하는 수준이다.) 근데 신뢰구간 공식은 (1-$\\alpha$)%로 정의되므로 신뢰구간과 유의수준은 직접적인 연관성이 존재한다. 위의 코드를 실행한 결과를 보고 해석하는 방법을 말해보자. t1을 실행하면 위처럼 나오는데, 하나 하나 뜯어보겠다. 해석을 위한 데이터의 이름 은 expr_dat[,1]이다. 데이터들을 이용해서 계산한 t-통계량 값은 0.39073이고 자유도(df, degrees of freedom) 는 441이다.그 다음에 나오는 값이 p-value 인데, p-value는 귀무가설을 잘못 기각할 확률을 말하기 때문에 p-value가 유의수준보다 작으면 귀무가설을 기각한다. 이게 좀 헷갈리는데 쉽게 말하면 p-value가 5%라는 것은 내가 귀무가설을 기각하는 게 틀릴 확률이 5%라고 계산되어 나온 것이다! 즉 내가 아무렇게나 기각해버려도 5%정도 틀린다는 것이다.결과적으로 p-value가 유의수준 5%보다 크기 때문에 기각할 수가 없다.결과 값에서 95% 신뢰구간을 보면 [6.968453, 7.047203]이므로 이 구간 내에 평균치가 존재하므로 기각이 어렵다. 그리고 mean of x는 흔히 말하는 $\\overline x$이다.(사실 아직도 잘 모르겠다.) 두 집단간의 모평균 가설 검정 두 집단간의 가설 검정 \\(H_0 : \\mu_1 = \\mu_2\\text{ (Null hypotheis)}\\) \\(H_1 : \\mu_1 \\neq \\mu_2,\\quad \\mu_1 &amp;gt; \\mu_2,\\quad \\mu_1 &amp;lt; \\mu_2\\text{ (Alternatice hypothesis)}\\) 두 집단에 대한 분석을 진행할 경우에는 두 집단이 독립적인지 쌍으로 이루어진 것인지 확인할 필요가 있다. pair를 이루는 경우는 두 표본평균의 차를 활용해서 검정을 진행한다. 쌍을 이룬다는 의미가 무엇이냐면 대표적인 예시로는 어떤 약을 먹기 전과 먹은 후의 변화 같은 경우가 쌍을 이룬 데이터다.두 집단 가설검정 코드 두 집단 가설검정 코드 n=25 x=rnorm(n, mean=1, sd=1) y=x+rnorm(n, mean = 0.5, sd=1) t1 &amp;lt;- t.test(x,y,alternative = &quot;two.sided&quot;,paired = T, var.equal = F) t2 &amp;lt;- t.test(x,y,alternative = &quot;less&quot;,paired = T,var.equal = F) t3 &amp;lt;- t.test(x,y,alternative = &quot;greater&quot;,paired = T, var.equal = F) t1;t2;t3 paired 변수를 설정해주면 해당 데이터가 쌍을 이루는 지 아닌 지를 알 수 있다. " }, { "title": "R Programming 10주차", "url": "/posts/R10/", "categories": "Statistics, R programming", "tags": "R", "date": "2020-06-11 00:00:00 +0900", "snippet": "월요일상자그림 (boxplot) boxplot mat = expr_dat[,c(3,4,7,8)] x11() install.packages(&quot;gplots&quot;) library(gplots) boxplot(mat,col=rainbow(4,alpha=0.6)) res = boxplot(mat,plot=F) res$out[res$group==1] 상자그림이란 자료의 분포를 한눈에 보여주는 그래프이다. 이상점(outlier), 사분위수(Quartile), 중위수(median) 등 데이터셋의 정보를 한눈에 볼 수 있다. 간단하게 상자그림을 설명해보면… 그림 상의 사각형 상자는 1분위수($Q_1$)~4분위수($Q_3$)까지를 보여준다. 이때 1분위수는 25% 데이터, 4분위수는 75% 데이터를 보여준다. 상자 내부의 검은 선은 중위수(median)로 해당 데이터 셋의 정확히 50%위치의 값을 보여준다. 상자로부터 위아래로 뻗은 직선(수염 이라고 한다.)은 최대와 최소까지의 범위를 표현해준다. 만약 최대와 최소가 각각 $Q_3 + 1.5IQR, Q_1 - 1.5IQR $ 범위 안에 있다면, 해당 값까지만 표시해주면 된다. 하지만 그 범위 밖의 값이 존재한다면 수염을 $1.5IQR$ 범위까지 표현해주고 그 이 후는 점을 찍어서 해당 값들은 이상점(outlier)으로 표시해준다. boxplot은 gplots 패키지에 있다. plot이 True (default)라면 상자그림이 생성이 되고 상자그림을 그리기 위한 필요 정보들이 반환된다. False값이면 상자그림은 반환되지 않고 필요정보만 반환된다. 선그림 (plot) plot pop_dat = read.csv(file = &#39;table_2_2.csv&#39;) x11() plot(pop_dat[,1],pop_dat[,2],type = &#39;l&#39;,xlab=&#39;연도&#39;,ylab = &#39;인구수&#39;) plot(pop_dat[,1],pop_dat[,2],type = &#39;b&#39;,xlab=&#39;연도&#39;,ylab = &#39;인구수&#39;) 일반적인 선그래프를 그려주는 함수이다. boxplot과 마찬가지로 gplots패키지에 있으며 함수 작동 형식은 plot(x, y, type, other options)이다. type은 크게 2가지 종류가 있다. type이 ‘l’이면 lines only의 의미로 선만 표시해준다. type이 ‘b’면 both points and lines의 의미로 선과 데이터의 점을 모두 표시해준다. 산점도 (scatter plot) scatter plot ind1=8; ind2=12 plot(expr_dat[,ind1], expr_dat[,ind2], type=&#39;p&#39;, pch=16, xlab=uq_names[ind1], ylab = uq_names[ind2]) cor_mat = cor(expr_dat) which.max(cor_mat[ind1,-ind1]) ind1=8; ind2=200 plot(expr_dat[,ind1], expr_dat[,ind2], type=&#39;p&#39;, pch=16, xlab=uq_names[ind1], ylab = uq_names[ind2]) 데이터의 관계를 볼 수 있는 그래프이다. 두 변수 사이의 상관관계를 확인해야하는데, 이때 산점도 그래프를 확인해볼 가치가 있다. 인공지능 공부에서 선형회귀에서 경사하강법을 사용 할 때 데이터의 방향성을 산점도 그래프의 형태로 확인하는 경우가 있다. 데이터의 전체적인 형태가 직선을 띌수록 두 변수의 상관관계가 높다. 참고록 pch가 16이면 채워진 원으로 표시해준다. 산점도 행렬 (scatter plot matirx) scatter plot matrix ind = c(2,8,12,200) pairs(expr_dat[,ind]) pairs(expr_dat[,ind], &quot;Expression Data&quot;, pch = 21, bg = c(&quot;red&quot;,&quot;blue&quot;)[gr_ind]) 산점도 그래프는 두 변수의 관계를 보여준다. 하지만 우리가 다루고 있는 데이터인 Expr_dat처럼 여러가지의 변수가 존재하는게 제일 일반적인 데이터 셋이다. 그래서 우리가 원하는 데이터들의 조합으로 산점도 그래프들을 모아서 보는 것이 편하다. 이때 사용하는 것이 산점도 행렬을 만들어주는 함수인 pairs()이다. 위의 사진이 산점도 행렬의 예시이다. (검은 부분은 검은색 원이 아니라 원이 많이 겹쳐버려서 까맣게 보이는 것이다.) 우선 대각성분들은 모두 자신과 자신의 조합이므로 의미가 없어서 해당 변수를 보여준다. 산점도 행렬들의 집합을 보면 유의미한 상관관계를 갖고 있는 변수조합은 (ABCE1, EIF4E)와 (EIF4E, ABCE1)임을 확인할 수 있다. iris data iris Data ind = c(2,8,12,200) pairs(expr_dat[,ind]) pairs(expr_dat[,ind], &quot;Expression Data&quot;, pch = 21, bg = c(&quot;red&quot;,&quot;blue&quot;)[gr_ind]) dev.off() iris x11() pairs(iris[1:4], pch=c(0, 3, 4)[as.numeric(iris$Species)]) 위에서 말했듯이 산점도 그래프는 머신러닝에 사용되는 경우가 많다. iris라는 데이터셋을 사용해서 판별분석을 연습할 수 있다. iris데이터에는 3가지의 아이리스 종류가 존재하는데, sepal과 petal의 정보로 구분을 할 수 있다. 그러기 위해서 우선 학습을 위해 어떤 변수 조합이 유의미한 지를 확인해볼 필요가 있다. 이는 선형회귀분석을 위해서 필요하기도 하다. 나름 의미가 있어보이는 데이터 셋은 (1,3), (2,4), (4,3), (3,4) 정도이다. (2,4)는 선형의 데이터도 아닌데 왜 의미가 있죠? 라고 묻는 사람들이 있을 수 있는데, 이 과정을 하는 이유는 구분 을 위해서이다. (2,4)의 데이터셋은 종별로 구분이 정확하게 잘 이뤄지고 있는 편이므로 판별분석에서는 충분히 가치가 있다. 위치의 측도 위치의 측도 평균 (mean, 산술평균) \\[\\overline{X}=\\frac{x_1+...+x_n}{n}=\\frac{\\sum_{i=1}^{n}x_i}{n}\\] 일반적으로 평균이라고 말하는 값이다.특이값(outlier)의 영향을 크게 받는다.사용함수 : mean(vector, na.rm = T or F) 중위수 (median, 중앙값) \\[median = \\begin{align} \\begin{cases} X_{((n+1/2))} \\quad \\text{if n is odd} \\newline\\frac{1}{2}(X_{(n/2)}+x_{(n/2+1)}) \\quad \\text{if n is even}\\end{cases} \\end{align}\\] 데이터의 순위에 관한 정보만을 이용한다.데이터셋 중앙부분의 값을 기록하는 것이기 때문에 특이값(outlier)의 영향을 덜받는다. 이를 통계학에서는 robust라고 한다.사용함수 : median(vector) 최빈값 (mode)데이터셋에서 가장 빈도가 높은 값이다.R에는 따로 정해져 있지는 않아서 정의를 해야한다. x = c(1,2,3,1,2,5,5,3,3,3,2)tb_x = table(x); tb_xas.numeric(names(tb_x)[which.max(tb_x)])Mode = function(vec) { tb = table(vec) return(as.numeric(names(tb)[which.max(tb)]))}Mode(x) 분위수 (quantile)특정 퍼센티지에 위치한 값을 말한다. 사용함수는 quantile()이다.quantile(data_set, percentage, type)의 형식으로 파라미터를 적어준다.type은 특정 방법론을 대표하는데, 일반적으로 따로 기입하지 않는 경우도 많다.사용함수 : quantile(data_set, percentage, type) 산포의 측도 산포의 측도 표본분산 (sample variance) \\[\\begin{array}{cc}For\\;X_{1}, X_{2}, ...., X_{n}, \\newline \\newline \\widehat{\\sigma^2}=\\frac{1}{n-1}\\sum_{i=1}^{n}(X_i - \\overline{X})^2\\end{array}\\] 표본을 뽑아서 분산을 계산한다.모분산 : $\\sigma^2 = Var(X) = E((X-\\mu)^2)$모표준편차 : $\\sigma = \\sqrt{Var(X)}$사용함수 : var(vector), sd(vector) 변동계수 (coefficient of variation, 변이계수)$CV = \\frac{s}{\\overline x}\\times 100(\\%) $자료의 측정단위에 의존하지 않는 상대적 산포의 측도이다.측정단위가 다르거나 평균에 큰 차이가 있는 자료들의 산포를 비교할 때 사용한다.사용함수 : 없음 왜도와 첨도 (skewness and excess kurtosis) 왜도와 첨도 왜도 (skewness)왜도는 비대칭의 정도를 보여준다. 일반적으로 0보다 큰 지 작은지를 확인하는데,왜도가 0보다 작으면 우측으로 데이터가 치우친 것이고 왜도가 0보다 크면 좌측으로 데이터가 치우친 것이다. 첨도 (excess kurtosis)그래프가 얼마나 뾰족한 형태를 가지는 지 알 수 있다.\\(\\begin{cases}&amp;lt;3 : \\text{flat (left distribution)} \\newline=3 : \\text{Normal distribution} \\newline\\text{&amp;gt; }3 : \\text{steep (right distribution)}\\end{cases}\\)사용함수 : moment package 내부에 skewness(vector), kurtosis(vector) 데이터 요약 (summary) 데이터 요약 (summary) x &amp;lt;- rnorm(100) summary(x) y &amp;lt;- c(&#39;red&#39;, &#39;blue&#39;, &#39;red&#39;,&#39;white&#39;) summary(y) f.y &amp;lt;- factor(y); summary(f.y) 말 그래도 데이터들의 정보를 요약해준다. 첫번째 데이터는 최대 최소, 4분위수들을 제공해준다. 만약 두번째 처럼 벡터요소가 들어가면 벡터의 정보가 반환된다. 그래서 벡터의 factor type적 요소로 제공을 해주면 자세한 해당 벡터의 정보를 반환해준다. 교차표 or 분할표 (cross table or contingency table) 교차표 or 분할표 table(mtcars$cyl)table(mtcars$am)table(mtcars$cyl, mtcars$am)table(mtcars$cyl, mtcars$am, mtcars$gear) table 함수에 특정 변수를 넣으면 해당 변수들로 분할표를 만들어준다. 데이터 변수를 1개만 넣으면 해당 변수의 도수분포표를 보여주고 2개를 넣으면 2차원 분할표를 보여준다. 변수의 조합으로 나오는 값들을 보여준다.변수를 3개 넣으면 3차원 분할표를 보여주는데, 3차원 큐브 형식의 데이터를 보여주는 것이 아니라 2차원 분할표에서 끝값만 변경해서 보여준다. 공분산 및 상관계수 (Covariance and Correlation) 공분산 (Covariance) cov(expr_dat[,1], expr_dat[,5])cov(expr_dat[,c(1,5,8)])var(expr_dat[,1]) \\(\\sigma_{xy} = \\mathrm{Cov}(X,Y) = E[(X-E(X))(T - E(Y))]\\)두 변수 사이의 연관성의 방향을 알 수 있지만 크기의 비교는 어렵다.공분산을 통해 두 변수의 음의 상관성, 양의 상관성을 확인할 수 있다. 표본 공분산\\(s_{xy}=\\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\overline x)(y_i - \\overline y)\\) 상관계수 (Correlation) cor(expr_dat[,1], expr_dat[,5]) \\(\\rho = \\mathrm{Corr}(X,Y)=\\frac{\\sigma_{XY}}{\\sigma_X\\sigma_Y}\\)공분산을 각각의 표준편차로 나눈 값으로 상대적인 크기 비교가 가능하다. " }, { "title": "R Programming 9주차", "url": "/posts/R9/", "categories": "Statistics, R programming", "tags": "R, Web_Crawling", "date": "2020-06-07 00:00:00 +0900", "snippet": "월요일데이터 전처리 1 (Data Cleaning 1) Data Cleaning 1 raw_dat = read.csv(file = &quot;Ex_data.csv&quot;, header = T, stringsAsFactors = F) head(raw_dat[,1:20]) dim(raw_dat) gr_ind = gl(2, 221) gr_ind dat_mat &amp;lt;- t(as.matrix(raw_dat[,-1])) dim(dat_mat) rownames(dat_mat) &amp;lt;- paste0(&quot;s&quot;,1:nrow(dat_mat)) colnames(dat_mat) &amp;lt;- raw_dat[,1] head(dat_mat[,1:20]) csv파일로 저장된 Ex_data.csv파일은 Lung Cancer Microarray data 이다. 특정 유전자의 발현량이 폐암에 영향을 미치는 가에 대해 분석할 때 사용하는 데이터이다. 이 데이터파일을 열어보면 행에는 DNA유전자 배열이 있고 열에는 해당 DAN구조 칩에 대한 sample들이 존재한다. 데이터 전처리 과정에서는 데이터를 정리하고 확인하는 작업을 진행한다. 우선적으로 데이터 셋자체를 보고 변수의 목록을 확인한다. 그 후 집중하고자하는 데이터 변수를 결정해야한다. 그 후에는 결측치와 이상치를 제거하거나 대체하는 방식을 진행해야한다. 이렇게 진행되는 것을 탐색적 데이터 분석(EDA, Exploratory Data Analysis) 라고 한다. 하지만 일반적으로 column에는 65,000개의 정보가 들어가고 row에는 100만개의 데이터가 들어가게 된다. 이때 유전자 정보를 변수로 쓰기때문에 column에 적는 경우가 많은데 유전자 정보는 정리하면 32만개 정도가 된다. 이러한 문제를 해결하기 위해서 우선 row에 입력하고 행렬데이터를 transpose(전치)시키게된다. DNA배열을 다루기 때문에 핵심적인 데이터는 문자의 형태를 띄게 된다. 이때 stringsAsFactors를 사용해야 Factor요소로 들어오게 될 때 발생하는 문제를 막을 수 있다. 가져온 데이터의 앞 부분 데이터를 보고 제대로 가져왔는 지 확인을하고 dim()을 통해 전체 데이터 수가 알맞게 가져와졌는지도 확인한다. 앞서 말했던 방법처럼 이제 원하는 데이터를 형식에 알맞게 기록해야한다. column입력 갯수에 맞춰서 Transpose Matrix(전치 행렬)를 만들어야한다. 그렇기 때문에 t(as.matrix(raw_dat[,-1]))을 해준다. 이때 첫번째 column을 제외하고 만드는 이유는 1번 열은 유전자데이터의 이름들이 있기 때문이다. 이 부분은 앞서 말했듯이 string type이기 때문에 따로 관리를 해준다. sample의 이름을 굳이 관리한다기 보다는 보기 쉽게 s로 이름을 만들어준다. 그리고 다시 열이름은 원래 raw_dat의 1열 이름들의 형식을 입력해주면 된다. 이로써 데이터 전처리 과정이 끝났다. 결측치 처리 (Missing Value Handling) Missing Value Handling indx &amp;lt;- which(is.na(dat_mat),T) indx col_ind = indx[,2] col_m = apply(dat_mat[,col_ind],2, mean, na.rm = T) col_m dat_mat[indx] = col_m sum(is.na(dat_mat)) dim(dat_mat) 이제 데이터를 정제하는 과정인 전처리 과정이 끝났으니 결측치를 처리해줘야한다. 결측치 처리가 중요한 이유는 데이터 분석에서 자주 나오는 단어인 outlier(이상치)들이 생각보다 데이터 분석에서 많은 문제를 일으킨다. outlier들은 데이터의 전체적인 구조를 변화시키는 결과를 일으킬 수도 있다. 보통 결측치인 NA들은 특정 값으로 대체를 하게 되는데 이 과정에서 결측치와 이상치들을 잘못 만질경우 전체 데이터에 손상이 일어날 수 있다. 이러한 이유로 결측치와 이상치를 처리를 잘 해줘야한다. 우선 which()명령어와 is.na()를 활용해서 결측치의 인덱스를 알아낸다. 이렇게 알아낸 정보들은 결측치가 어디에 있는지를 갖고 있는 데이터가 된다. 이후 apply()를 활용해서 결측치가 존재하는 인덱스의 값은 그 열의 평균으로 값을 변경해준다. 이를 평균대치법(Mean Imputation)이라고 한다. 이후 sum(is.na())로 전체 데이터에 존재하는 결측값의 수를 확인해본다. 데이터 전처리 2 (Data Cleaning 2) Data Cleaning 2 uq_names &amp;lt;- unique(colnames(dat_mat)) p &amp;lt;- length(uq_names); n &amp;lt;- dim(dat_mat)[1] expr_dat &amp;lt;- matrix(0,n,p) for (i in 1:p) { expr_dat[,i] = apply(as.matrix(dat_mat[,colnames(dat_mat) == uq_names[i]]), 1,mean) cat(&quot;\\n&quot;,i,&quot;-th step&quot;) } colnames(expr_dat) &amp;lt;- uq_names rownames(expr_dat) &amp;lt;- rownames(dat_mat) head(expr_dat[,1:20]) dim(expr_dat); sum(is.na(expr_dat)) 가져온 데이터를 잘 살펴보면 중복데이터들이 존재한다. 이런 중복데이터도 처리를 해줄 필요가 있다. 두 데이터의 평균으로 값을 대표해서 적어준다. 이때 이러한 데이터를 처리할 때는 idea가 중요하다. unique한 유전자인가? unique한 유전자의 갯수만큼 평균값 계산을 해준다. 선택된 유전자가 원래 데이터에서 어떤 위치에 있는지 확인한다. unique()를 사용하면 데이터에서 중복된 값을 처리해줄 수 있다. 이렇게 중복된 이름을 우선 제거하고 uq_names에 저장해준다. (1번 과정) expr_dat라는 임시 데이터를 원본 데이터 크기만큼 만들어준다. 그 후 for문을 사용해서 expr_dat에 원본 데이터 값을 갖는 데이터들의 평균치로 채워준다. 이 과정에서 열의 값을 확인한다. uq_names의 i번째 인덱스 값과 원본 데이터의 열의 i번째 값이 같은 부분들을 확인해주는 과정을 진행한다. (2, 3번 과정) 이렇게 기본적인 데이터 전처리와 결측치 대치 과정이 끝났다. 탐색적 데이터 분석 (EDA, Exploratory Data Analysis) EDA EDA는 위에서 진행한 데이터 분석 과정을 일컫는다. 데이터 구조와 특징을 우선적으로 분석하고 분석한 결과를 통해 데이터 변환, 축소등의 데이터 손질을 하는 데이터 분석기법이다. 일반적으로 EDA 기법은 2가지 정도가 있다. 그래프를 이용한 탐색 (Search by Graph) 다양한 그래프로 데이터를 요약하고 특징을 관리할 수 있다. 통계표 및 통계량을 통한 탐색 (Search by Statistics data) 평균, 분산, 분위수 등의 분포 파악을 위한 모수 추정값을 요약한다. 데이터 결측값과 특이값 (Missing Value &amp;amp; Outlier) Missing Value &amp;amp; Outlier 결측값(Missing Value) 이란 데이터 수집과정에서 수집이 되지않았거나 손실이 발생을 해서 데이터 일부가 누락되어 측정된 것을 말한다. 이런 결측치를 대치하는 방법도 다양하게 존재한다. 특이값(Outlier) 은 일상적으로 접하는 데이터에서 대부분의 데이터가 따르는 경향에서 크게 벗어나 있는 데이터를 말한다. 특이값은 분석 모형 결과에 크게 영향을 줄 수 있기 때문에 전처리 과정에서 식별해서 모형에서는 제외한다. 특이값이 발생하는 것이 사실 이상한 것은 아니다. 중요한 것을 왜 특이값(Outlier)이 발생했는가? 이다. 특이값이 발생한 이유에 따라서 해당 특이값이 유의미한지, 무의미한지를 결정해주기 때문이다. 목요일막대그래프 (barplot) barplot dat = read.table(file=&quot;Ex211_tab.txt&quot;, header = T, sep = &#39;\\t&#39;, fileEncoding = &quot;CP949&quot;,encoding = &quot;UTF-8&quot;) head(dat) dat$Job attach(dat);Job search() quartz() plot(Job, main=&quot;직업의 막대그림&quot;, ylab=&quot;인원수(명)&quot;,ylim=c(0,15), xlim=c(0,5)) box() freq = table(dat[,6]) barplot(freq) detach(Job) 데이터의 특정 변수 값을 $연산자를 통해 직접적으로 컨트롤할 수 있는 권한을 얻을 수 있다. 이런 번거로움을 덜기 위해서 attach()를 활용하면 원하는 데이터 셋의 변수에 변수이름만으로 직접 접근을 할 수 있다. 활용 후에 detach()로 원래대로 돌려주는 것이 좋다. quartz()나 x11()을 활용하면 별도의 디스플레이에 그래프를 띄울 수 있다. 나는 맥북이라 x11()을 사용하기 위해서 quartz()를 쓰므로 quartz로 명령어를 쓴다. 일반적으로는 x11()사용해도 괜찮다. plot()을 쓸때 어떤 값을 활용할 지 정하고 제목과 x,y축의 이름을 정해줄 수 있다. 이때 ylim과 xlim을 사용하면 x,y축의 최대 / 최소 한계 값을 지정해줄 수 있다. plot은 일반적인 그래프로 나타내준다. barplot()을 쓰면 막대그래프 형식으로 나타난다. 이때 내부에 들어갈 값은 빈도수를 확인할 수 있는 값이어야한다. barplot2 barplot2 data(&quot;VADeaths&quot;) install.packages(&quot;gplots&quot;) library(gplots) x11() barplot2(t(VADeaths), beside = TRUE, col = gray(seq(0.4,0.9,length=5)), legend=colnames(VADeaths), ylim = c(0,100)) title(main = &quot;Death Rates in Virginia&quot;, font.main=4) hh &amp;lt;- t(VADeaths)[,5:1] mybarcol &amp;lt;- &quot;gray20&quot; cil &amp;lt;- hh * 0.85 ciu &amp;lt;- hh * 1.15 mp &amp;lt;- barplot2(hh, beside = T, col = gray(seq(0.4,0.9,length = 5)), legend = colnames(VADeaths), ylim = c(0,100), main = &quot;Death Rates in Virginia&quot;, font.main = 4, sub = &quot;Faked 95 percent error bars&quot;, cex.names = 1.5, plot.ci = T, ci.l = cil,ci.u = ciu,plot.grid = T) box() barplot2 실험데이터 분석을 할 때 쓰기 좋은 함수이다. “gplots”패키지에 들어있으며 패키지 설치 진행후 사용을 해야한다. barplot2에서 beside가 T인 경우는 일반적은 막대그래프의 형태이다. F인 경우에는 하나의 막대그래프에 누적형태로 나타나게 된다. legend는 막대하나하나에 어떤 의미가 있는지를 보여주는 형식을 출력해준다. plot에 신뢰구간도 보여줄 수 있다. 내부 parameter에 plot.ci는 plot confidence interval의 약자이다. 즉 신뢰구간 설정 여부를 말해준다. 이 값을 TRUE로 해준 경우 ci.l과 ci.u 값들을 설정해줘야한다. 원 그래프 (pie chart) pie chart pie(freq, main = &quot;직업의 원그림&quot;) pie(rep(1,24), col = rainbow(24), radisu = 0.9) pie.sales &amp;lt;- c(0.12,0.3,0.26,0.16,0.04,0.12) lbl = c(&quot;Blueberry&quot;,&quot;Cherry&quot;,&quot;Apple&quot;,&quot;Boston Cream&quot;, &quot;Other&quot;,&quot;Vanilla Cream&quot;) names(pie.sales) = paste0(lbl, &quot; (&quot;, pie.sales*100, &quot;%)&quot;) pie(pie.sales, col = rainbow(length(pie.sales))) 일반적인 원형그래프를 보여준다. frequency를 당연하게 입력해줘야한다., 해당 비율만큼 알아서 원 그래프의 면적을 채워준다. names를 쓰면 원 그래프의 항목 이름을 설정해줄 수 있다. 히스토그램 (histogram) histogram x &amp;lt;- expr_dat[,10] x11() hist(x, breaks = 20, col = &quot;gray&quot;, main = uq_names[10]) hist(x, breaks = 20, freq = F ,col = &quot;lightblue&quot;, main = uq_names[10]) 일반적인 히스토그램을 출력해준다. breaks는 상자의 갯수를 지정해준다. 이때 frequency는 default가 T인데, F로 설정할 경우 히스토그램을 밀도값으로 보여준다. 데이터의 갯수가 아니라 해당 데이터가 차지하는 밀도로 보여준다. 일종의 상대도수그래프와 같다. hist함수의 plot을 F로 변경해주면 그래프는 그려지지 않고 그래프를 그릴 때 필요한 데이터들을 모두 보여준다. " }, { "title": "R Programming 7주차", "url": "/posts/R7/", "categories": "Statistics, R programming", "tags": "R", "date": "2020-06-07 00:00:00 +0900", "snippet": "월요일function function # 함수 정의 방식 1 googoo81 &amp;lt;- function(x){ cat(&quot;\\n&quot;) for (i in 1:9) { cat(x,&quot; * &quot;,i,&quot; = &quot;,x*i,&quot;\\n&quot;) } cat(&quot;\\n&quot;) } # 함수 정의 방식 2 wd_count = function(x, sep=&quot; &quot;) { temp = gsub(&quot;[(),.?!/]&quot;,sep,x) temp = unlist(strsplit(temp,sep)) xrm = c(&quot;&quot;,&quot;a&quot;,&quot;an&quot;,&quot;the&quot;) temp = temp[!(temp %in% xrm)] # temp[temp!=&quot;&quot;] return(table(tolower(temp))) } R에서 사용자 정의 함수를 작성할 수 있다. 함수 정의 방식은 일반적으로 2가지이다. function_name &amp;lt;- function() {}과 function_name = function() {}이다. 일반적으로 함수의 parameter안에 들어가는 값은 default값을 지정해 줄 수 있다. 단, default가 없을 경우 에러코드가 함수 실행 시에 발생한다. 함수에 return값을 설정해주는 방식으로 보통 많이 활용한다. 하지만 R에서의 사용자 지정 함수는 함수에서 기록한 마지막 값을 자동으로 반환해준다. 변수 변수 a &amp;lt;- c(1,3,5,6) mean.k &amp;lt;- function(x,k) return(mean(x^k)) mean.k(a,2) mean.k2 &amp;lt;- function(x, k = 3) return(mean(x^k)) mean.k2(a,2); mean.k2(a) noact &amp;lt;- function(x, type = 1) { if(type == 1) a[1] &amp;lt;- 3 if(type == 2) a[1] &amp;lt;&amp;lt;- 3 return(a) } 변수는 일반적으로 2가지로 나눠진다. 지역변수와 전역변수. 지역변수의 지역변수와 전역변수의 차이는 Life cycle에 달려있다. 함수에 선언된 변수는 지역변수로 설정이 된다. 즉 해당함수가 종료되는 동시에 메모리에서 해제가 된다. 하지만 함수가 아닌 곳에 선언된 변수는 전역변수가 된다. 그 결과 전체 프로그램이 종료될 때까지 변수는 메모리에서 해제되지 않는다. 전역 변수를 설정하고 싶다면 다음과 같은 기호를 써주면된다. a[1] &amp;lt;&amp;lt;- 3으로 작성해주면 지역변수가 설정되는 함수 내부에서도 외부의 전연변수 값을 변경할 수 있다. " }, { "title": "R Programming 6주차", "url": "/posts/R6/", "categories": "Statistics, R programming", "tags": "R", "date": "2020-05-23 00:00:00 +0900", "snippet": "월요일if~else / ifelse if~else / ifelse x &amp;lt;- 1:5 y &amp;lt;- -2:2 if(any(x&amp;lt;0)) print(x) if (any(y&amp;lt;0)) { print(abs(y)) cat(&quot;\\n y contains negative values&quot;) } x &amp;lt;- 1:5 if(length(x)==5) { if (sum(x)==15) { cat(&quot;\\n Vector x length=&quot;,length(x),&quot;, sum = &quot;,sum(x)) }else { cat(&quot;\\n Vector x length !=&quot;,length(x)) } } x = c(10,3,6,9) y=c(1,5,4,12) ifelse(x&amp;gt;y,x,y) score = c(80,75,40,98) grade=ifelse(score &amp;gt;= 50,&quot;pass&quot;,&quot;fail&quot;) data.frame(score,grade) 그냥 단순한 조건문이다. 자세한 설명은 다른 언어들의 if문과 동일하므로 생략. 이때 조금 특이한 조건문이 하나가 있다. 바로 ifelse문인데 이 조건문은 논리벡터에 적용이 되는 조건문이다. 함수의 기본입력 형식은 ifelse(condition, true ouput, false output)의 형태이다. 약간 느낌은 삼항연산자와 비슷하게 보면 될 듯하다. switch switch x&amp;lt;-c(1,3,2,5,2) i &amp;lt;- 2 switch (i,mean(x),median(x),sd(x),var(x)) type &amp;lt;- &quot;mean&quot; switch(type, mean=mean(x),sd=sd(x),var=var(x)) 보통 C++에서 쓰는 switch case문과 유사하다. 좀 다른 점이라면 해당 case에 들어가는 것이 한줄의 코드에 다 들어간다는 점이다. 함수 기본입력 형식은 switch(function index,func1,func2,...)이다. 목요일for statement for statement for (i in 1:(nfile-1)) { idx = (cut * (i - 1) + 1):(cut * i) write.table(dat[idx,], file = paste0(f_pre, i, f_post), sep = &#39;\\t&#39;) } idx = (cut * i + 1):n write.table(dat[idx,], file = paste0(f_pre,i + 1, f_post), sep = &#39;\\t&#39;) 일반적인 loop statement이다. 반복문의 형태는 시작값:끝값의 형태로 입력해주면 된다,. 위의 코드는 특정 파일을 10개씩 잘라서 데이터에 저장하는 반복문 코드이다. while statement for statement ch = c(&quot;A/B/C/D/F&quot;, &quot;A/AA&quot;, &quot;BB/B&quot;, &quot;Quit&quot;, &quot;CC/C&quot;) xp = list() i = 1 while (ch[i] != &quot;Quit&quot; &amp;amp; i &amp;lt;= length(ch)) { xp[[i]] = unlist(strsplit(ch[i],&#39;/&#39;)) print(xp[[i]]) i = i + 1 } table(unlist(xp)) 일반적인 while문과 동일하다. 설명은 생략… " }, { "title": "R Programming 5주차", "url": "/posts/R5/", "categories": "Statistics, R programming", "tags": "R", "date": "2020-04-13 00:00:00 +0900", "snippet": "월요일Input 값 받아오기 Input 값 받아오기 a = readline(&quot;Input any integer: &quot;) a b = readline(&quot;Input two integer with comma (ex: 1,2) : &quot;) strsplit(b,&quot;[,]&quot;) as.numeric(unlist(strsplit(b,&quot;[,]&quot;))) readline은 입력 값을 받아오는 함수이다. readline코드가 실행되면 콘솔창에 입력을 하면 해당 변수에 값이 저장된다. 이때 strsplit와 응용해서 사용하면 구분자로 입력이 가능하게 만들 수 있다. 당연히 저장값을 리스트이기 때문에 unlist와 as.numeric을 사용해야한다. 데이터 파일 가져오기 데이터 파일 가져오기 x1 = scan(file = &quot;input_noh.txt&quot;, what = numeric()); x1[1:5] x2 = scan(file = &quot;input_noh.txt&quot;, what = character()); x2[2:5] x3 = scan(file = &quot;input_noh.txt&quot;); x3[1:5] x4 = scan(file = &quot;input_h.txt&quot;, what = character()); x4[1:5] x = matrix(as.numeric(x4[-(1:2)]),ncol = 2,byrow = T) x[1:3,] dat = read.table(file = &quot;input_noh.txt&quot;) dat2 = read.table(file = &quot;input_noh.txt&quot;,header=T) dat3 = read.table(file = &quot;input_h.txt&quot;,header = F) dat4 = read.table(file = &quot;input_h.txt&quot;,header = T) dat5 = read.table(file = &quot;input_h.txt&quot;,header = F, stringsAsFactors = F) scan명령어를 사용하면 file에 입력한 데이터를 가져올 수 있다. what에 입력한 타입의 형식으로 가져오는데, character()은 numeric()보다 더 많은 양의 데이터를 가져온다. 이때 하나의 유형만 불러올 수가 있다. read.table도 scan처럼 파일의 데이터를 가져오는 명령어이다. 하지만 다른 점이라면 scan은 입력의 형식을 고려하지않고 1차원적으로 데이터를 가져온다. 하지만 read.table은 자료의 데이터 입력 형식을 고려해서 테이블 형태를 잡아서 가져온다. 그리고 scan과 다르게 여러 유형으로 저장이 가능하다. 데이터 파일 입력하기 데이터 파일 입력하기 write.table(dat,file = &quot;test1.txt&quot;,row.names = T, col.names = T, quote = T, sep = &quot;\\t&quot;) write.table(dat,file = &quot;test2.txt&quot;,row.names = F, col.names = F, quote = F,sep = &quot;\\n&quot;) write.table(dat,file = &quot;test3.csv&quot;, sep=&quot;,&quot;) write.table(dat, file = &quot;test4.txt&quot;, row.names = T,col.names = T, quote = T,sep = &quot; &quot;) write.table을 사용하면 데이터를 입력하여 저장할 수 있다. 파일 입출력과 같다고 보면된다. 구분자를 입력하면 해당 구분자로 데이터가 입력이 된다. 예를 들면 csv파일(comma-separated values)은 구분자를 ,로 해주면 된다. 목요일read.xlsx read.xlsx Sys.getenv(&#39;JAVA_HOME&#39;) Sys.setenv(&quot;JAVA_HOME&quot;= &#39;/Library/Java/JavaVirtualMachines/jdk-14.0.1.jdk/Contents/Home&#39;) system(&quot;java -version&quot;) install.packages(&quot;xlsx&quot;) library(xlsx) read.xlsx는 함수의 이름 그대로 엑셀을 불러오는 함수이다. 이 함수를 쓰기 위해서는 Java가 깔려있어야한다. java version을 확인해주시고 요즘은 jdk가 14이상이 깔려있으니 그냥 진행하셔도 된다만…. Mac OSX를 쓰시는 분들은 에러가 나는 경우가 있다. 맥용 자바가 설치가 안되어 있으면 install.packages(&quot;xlsx&quot;)에서 에러가 발생한다. 이는 Mac OSX용 구버전 자바를 사용하면 해결이 된다. 이게 환경변수를 변경하면 다른 자바 프로그램들이 꼬여버릴 수가 있기때문에 설치 후 환경변수 설정해주시고 다시 jdk14를 설치하면 적용되면서 jdk를 상위 버전이 유지가 된다. write.xlsx write.xlsx write.xlsx(x1, file=&#39;test.xlsx&#39;, sheetName=as.character(1), col.names=T, row.names=F,append=F,showNA=F) write.xlsx(x1, file = &#39;test.xlsx&#39;, sheetName = as.character(2), col.names = T, row.names = F, append = T, showNA = F) for (i in 3:10) { write.xlsx(x1, file = &#39;test.xlsx&#39;, sheetName = as.character(i), col.names = T, row.names = F, append = T, showNA = F) } 엑셀파일을 쓰는 함수이다. 앞선에 언급한 read.table()과 유사한 형태를 띄고 있다. 약간은 read.table과 scan을 합쳐놓은 느낌이다. 두 개의 특징을 어느정도 섞어놓았고 우리가 흔히 생각하는 엑셀파일의 형식을 가지고 파일이 만들어진다고 생각하면 된다. dplyr dplyr install.packages(&quot;dplyr&quot;) library(dplyr) 데이터 처리에 특화된 R 패키지이다. library에는 꽤 자주 사용하는 데이터 처리 및 분석 함수들이 있다. filter(dataset, conditon1, condition2, ...) 지정한 조건식에 맞는 데이터를 추출한다. 보통 sample들을 필터링할 때 자주 사용하고 행 중심의 데이터 처리와 정리를 다룬다. select(dataset, var1, -var2, ...) 특정 데이터 셋에서 선택 변수들을 추출한다. 제외할 수도 있다. filter가 행 중심의 데이터 추출이라면 select는 열 중심의 데이터 추출 방식이다. 보통 여러가지 변수를 추출할 때는 제외하는 열만 적는게 의미가 있다. mutate(dataset, var_name=expression, ...) 데이터 셋에서 새 변수를 정의해서 추가한다. arrange(dataset, var1, desc(var2)) 데이터 셋을 변수에 따라 오름차순으로 정렬해준다. desc를 써주면 당연히 해당 데이터셋에 대해서는 내림차순으로 정렬해준다. 아마 먼저 적은 변수를 기준으로 동일 데이터 값이면 후순위에 적은 기준으로 정렬하는 것 같다. summarize(dataset, name1=func()) 데이터 셋에서 특정 함수에 대해서 데이터를 집계해준다. 반환 형은 1개의 행값을 갖는 데이터 프레임이다. 보통 특정 데이터를 가진 상태에서 평균, 분산과 같은 값을 새로 데이터에 추가할 때 많이 사용한다. group_by(dataset, variable) 데이터 셋을 그룹화 시켜준다. 지정한 variable을 기준으로 그룹화가 진행된다. 보통 데이터 처리 초반에 많이 쓰는 편이다. pipe(%&amp;gt;%) pipe는 함수라기 보단 연산자?라고 보면된다. 계속해서 할당연산자를 주면서 코딩을 하면 눈에 잘 들어오지 않는 경우가 많다. 이때 pipe를 쓰면 하나의 데이터에 어떤 데이터 처리과정을 순차적으로 진행했는지를 알 수 있다. 그리고 좀 더 간편하게 볼 수 있다. 결측값 확인 결측값 확인 is.na(x) sum(is.na(x)) which(is.na(x)) which(is.na(x),arr.ind = T) which(is.na(x),T) 데이터라는 것에서 가장 중요한 것은 중복과 결측치이다. 결측치는 데이터를 처리해서 정리했을 때 오차를 발생시키는 대표적인 문제 값들이다. 그래서 보통 결측치가 존재하는 지를 확인하고 해당 결측치를 평균 값으로 대체하거나 제외시키는 결측치 보정방법들이 존재한다. 이를 위해서 우선 결측치들을 확인해야하는데, 이때 쓰는 함수가 is.na()이다. 해당 값이 결측치(NA)면 T, 아니면 F를 반환해준다. 보통 sum과 병행해서 전체 결측치의 개수를 알아내거나 which를 병행해서 결측치 위치들을 알아내서 대체할 때 많이 사용한다. " }, { "title": "R Programming 4주차", "url": "/posts/R4/", "categories": "Statistics, R programming", "tags": "R", "date": "2020-04-09 00:00:00 +0900", "snippet": "월요일paste paste myname &amp;lt;- &quot;Jason Bourne&quot; paste(&quot;My name is&quot;, myname, sep=&quot; &quot;) file_id = 1533 paste(&quot;Dataset_&quot;,file_id,&quot;.txt&quot;, sep=&quot;&quot;) paste(&quot;A&quot;,&quot;B&quot;,collapse = &#39;/&#39;) paste(c(&quot;A&quot;,&quot;B&quot;), collapse = &#39;/&#39;) paste(c(&quot;A&quot;,&quot;B&quot;), 1:2, collapse = &#39;/&#39;) paste(c(&quot;A&quot;,&quot;B&quot;), 1:2, sep=&#39;?&#39;, collapse = &#39;/&#39;) paste함수의 기본 구분자는 스페이스바 공백으로 되어있다. 여러가지 인수들을 하나의 문자열로 이어붙이는데, 이때 인수 구분을 sep에 설정한 구분자로 붙여준다. collapse 옵션은 벡터로서 문자열을 합칠 때 적용되는 문자열이다. 7번째 코드를 보면 paste(c(&quot;A&quot;,&quot;B&quot;), 1:2, collapse = &#39;/&#39;)라고 적혀있다. 이 코드의 출력값은 A 1/B 2이다. 문자열 분석 문자열 분석 test &amp;lt;- c(&quot;abcdefg&quot;, &quot;AFFY1245820&quot;) nchar(test) f_name &amp;lt;- &quot;AFFY1245820&quot; substr(f_name,5,nchar(f_name)) y = c(&quot;ax_1234&quot;, &quot;ax_3456&quot;) substr(y,4,7) strtrim(&quot;ABCDEF&quot;,3) strtrim(rep(&quot;abcdef&quot;,3), c(1,4,10)) x &amp;lt;- c(as = &quot;asfef&quot;, qu = &quot;qwerty&quot;, &quot;yuiop[&quot;, &quot;b&quot;, &quot;stuff.blah.yech&quot;) strsplit(x, &quot;e&quot;) strsplit(x, &quot;[a-e]&quot;) strsplit(x, &quot;[aleu]&quot;) unlist(strsplit(&quot;a.b.c&quot;, &quot;.&quot;)) unlist(strsplit(&quot;a.b.c&quot;, &quot;[.]&quot;)) unlist(strsplit(&quot;a.b.c&quot;, &quot;.&quot;, fixed = TRUE)) 간단한 문자열 분석 함수들이다. nchar는 해당 문자열의 갯수를 반환해준다. substr은 문자열의 일부분을 추출하는 함수이다. 시작위치와 마지막위치를 반환해주면 해당 길이만큼 추출해준다. 일반적으로 공통적인 파일이름을 가지고 넘버링만 다른 경우에서 넘버링을 추출하거나 할 때 사용한다. strtrim은 문자열을 잘라내는 역할을 한다. 언뜻보면 substr과 비슷하다고 느낄 수 있다. 하지만 strtrim은 특정 위치를 잘라내는게 아닌 앞에서 부터 잘라낸다. strsplit은 가장 많이 쓰는 문자열 분석 함수이다. 특정 문자열을 정규 표현식을 기준으로 자르게 된다. 이때 반환 값은 리스트 형태로 반환하게 된다. 일반적으로 strsplit에는 3개의 인수를 건드린다. split, fixed, perl을 건드린다. 이때 3개 모두 FALSE인 경우는 default로 Extended regular expression(확장 정규 표현식)을 사용한다. 확장 정규 표현식이란 일반적으로 Front-End에서 사용하는 정규표현식을 말한다. fixed가 TRUE인 경우는 문자표현 그대로 사용한다. perl이 TRUE인 경우에는 perl-like regular expression을 이용한다. 목요일경로 설정 경로 설정 getwd() setwd(&#39;Documents&#39;) getwd() dir.create(&quot;example2&quot;) getwd() unlink(&quot;example2&quot;,recursive = T) dir() 일반적으로 통계 데이터를 분석할 때는 대규모 파일을 사용하는 경우가 많다. csv파일, xlsx파일들로부터 데이터를 가져온다. 이때, 해당경로로 이동을 해야한다. 그 때 사용하는 명령어들이다. getwd()는 현재 경로를 반환해준다. setwd(경로)는 해당 경로로 이동을 하게 해준다. dir.create(경로)는 directory create로, 현재 경로에 특정이름의 폴더를 만든다. 폴더를 지울 때는 unlink로 해당 폴더를 지운다. 내부 파일이 있는 폴더를 지울 때는 반드시 recursive를 TRUE로 해준다. 현재 작업 폴더 관련 함수 현재 작업 폴더 관련 함수 test = dir(&quot;/Users/kibeompark/Documents&quot;) test t1=grep(&quot;^[A-Ca-c][a-z]&quot;,test) t1 t2=grep(&quot;[.]pkg$&quot;,test,value=T) t2 t3=grep(&quot;^[BbCc].+pkg$&quot;,test,value=T) t3 이제 작업 폴더에 무슨 파일이 있는 지 확인하고 찾을 수 있다. 우선 dir을 통해서 특정 경로의 파일 이름들을 문자열 벡터로 변환을 해준다. grep를 통해서 특정 정규표현식에 매칭되는 파일의 인덱스 위치를 반환해준다. 예제 코드는 A-C,a-c로 시작하고 a-z가 들어간 파일들을 찾는다. 5번째, 7번째 코드처럼 작성을 하면 특정 확장자 파일을 찾을 수 있다. " }, { "title": "R Programming 3주차", "url": "/posts/R3/", "categories": "Statistics, R programming", "tags": "R", "date": "2020-04-02 00:00:00 +0900", "snippet": "월요일벡터 인덱스 네이밍 벡터 인덱스 네이밍 vec &amp;lt;- vector() vec &amp;lt;- 1:10 vec2 &amp;lt;- c(&quot;abc&quot;, &quot;def&quot;) vec3 &amp;lt;- c(T,F,T,F) names(vec2) &amp;lt;- c(&quot;first&quot;,&quot;second&quot;) vec2[&quot;first&quot;] 벡터의 인덱스에 이름을 부여할 수 있다. 인덱스에 이름이 부여되면 파이썬처럼 텍스트 인덱스를 쓸 수 있다. Matrix(행렬) Matrix (행렬) x1 &amp;lt;- matrix(1:20,nrow=5,ncol=2,byrow=T) x2 &amp;lt;- matrix(1:10,5,2,byrow = F) print(x1) print(x2) cbind(x1,x2) rbind(x1,x2) A &amp;lt;- matrix(1:12,4,3) rownames(A) &amp;lt;- c(paste0(&quot;n&quot;, 1:4)) colnames(A) &amp;lt;- c(paste0(&quot;x&quot;, 1:3)) print(A) A[1,] A[,2] A[,-3] B &amp;lt;- matrix(1,4,3) A + B a &amp;lt;- matrix(1:6,2,3); b &amp;lt;- matrix(1,3,2) a%*%b A &amp;lt;- matrix(1:4,2,2) solve(A) A &amp;lt;- matrix(1:12,4,3) t(A) A &amp;lt;- matrix(1:9,3,3) sum(diag(A)) A &amp;lt;- matrix(1:4,2,2) det(A) eigen(A)$values matrix()를 활용하면 행렬을 만들 수가 있다. 이때 중요한 옵션은 byrow옵션이다. byrow의 default값은 FALSE이다. byrow가 TRUE인 경우는 행렬 데이터를 채울 때 행을 우선으로 채우고 FALSE인 경우 열을 우선으로 채우게 된다. cbind(); rbind()는 두 행렬을 어떤 기준으로 이어 붙이는지를 결정해 준다. cbind()는 앞의 행렬에 뒤 행렬을 행기준으로 열을 붙이고 rbind()는 앞의 행렬에 뒤 행렬을 열기준으로 행을 붙인다. rownames와colnames를 활용하면 행과 열에 이름을 부여할 수 있다. 인덱스를 정확한 좌표로 기록하지 않고 A[1,]과 같이 입력하면 해당 데이터의 1행을 모두 출력해준다. 이때 인덱스에 -가 들어가면 해당 열, 혹은 행을 제외하고 출력해주는 명령어가 된다. 기본 행렬 연산을 모두 지원한다. 간단한 행렬 합과 곱을 모두 지원하는데 당연하게 합, 곱 연산시의 기본 조건은 지켜야한다. A%*%B는 두 행렬의 곱연산을 나타내는데, 이때 반드시 지켜야하는 조건은 ‘앞의 열수 = 뒤의 행수’이다. 만약 단순하게 A*B를 하면 두 행렬의 원소별 곱을 지원하기도 한다. solve()는 역행렬을 반환해준다. t()는 행렬의 transpose(전치)를 반환해준다. diag()를 활용하면 대각행렬을 만들 수 있다. 함수 내부에 어떤 값이 들어가냐에따라 역할이 달라진다. 숫자 -&amp;gt; n x n 단위행렬 벡터 -&amp;gt; 해당 벡터가 대각성분인 대각행렬 행렬 -&amp;gt; 해당 행렬의 대각성분 반환 det()를 활용하면 행렬의 determinant를 알려준다. eigen()을 활용하면 eigen value혹은 eigen vector를 알 수 있기때문에 행렬의 고유값과 고유벡터를 알 수 있다. $연산자를 활용해서 values와 vectors를 각각 반환 가능하다. list list lst1 = list(a = 1:10, b =matrix(1:4,2,2)) lst2 &amp;lt;- list() lst2[[1]] &amp;lt;- matrix(1:10,5,2) lst2[[2]] &amp;lt;- lst1 lst1$a lst1$b lst1[1] lst1[[1]] lst2[[1]] lst2[[1]][3] lst1[[2]] lst1[2] lst2[2] lst2[1] lst2[[2]][[1]] R에서 다양한 데이터를 한꺼번에 담을 수 있는 가장 포괄적 객체이다. 모든 객체를 담을 수 있으며 동시에 다양한 type으로 담을 수 있다. 변수 이름을 부여해서 데이터를 관리할 수도 있으며 데이터 저장인덱스를 불러올 수도 있다. 목요일factor type factor type grade &amp;lt;- c(&quot;A&quot;, &quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;B&quot;, &quot;B&quot;) f.grade &amp;lt;- factor(grade) f2.grade &amp;lt;- factor(grade, order = T) lev &amp;lt;- c(&quot;C&quot;, &quot;B&quot;, &quot;A&quot;) f3.grade &amp;lt;- factor(grade, levels = lev, order = T) levels(f2.grade) levels(f3.grade) 범주형 자료분석에 유용한 객체이다. 보통 명목형, 순서형자료에 사용한다. level을 가지고 있기때문에 순서를 구분이 가능하다. factor생성할 때, order이 T인 경우 순서형, F인 경우 명목형으로 저장한다. 얼핏 보면 벡터와 굉장히 유사한 것을 알 수 있지만 factor는 지정된 인덱스를 벗어난 값은 접근이 불가능하다. data.frame data.frame x1 = 1:4; x2 &amp;lt;- c(&quot;kim&quot;,&quot;lee&quot;,&quot;jung&quot;,&quot;park&quot;) dat = data.frame(x1,x2) dat2 = data.frame(num=x1, name=x2) dat3 = data.frame(x1,x2,stringsAsFactors = F) dat[1]; dat[2] dat[[1]]; dat[[2]] data.frame은 행렬과 리스트가 합쳐진 것이라 생각하면 편하다. 데이터프레임에 저장할 변수는 길이가 동일해야한다. 다르면 계속해서 에러가 난다. 조심해야하는 점이 한 가지 있다. 숫자를 다루게 될 경우 문자와 섞이게 되거나, 문자로 적힌 숫자인 경우에는 문자로 인식해서 factor타입으로 변형을 시키게되는데, 이때 이 값으로 연산을 하면 값으로 계산을 하는 게 아니라 factor에 있는 level로 계산을 하게 된다. 이를 막기 위해서 stringsAsFactors = F를 이용하면 막을 수 있다. array array x1 &amp;lt;- array(1:24,dim=c(4,3,2)) x2 &amp;lt;- array(1:32, dim=c(2,2,4,2)) n차원의 데이터 저장방법이다. 행렬과 유사한 형태이다. dim을 통해서 n x m과 높이, 반복수 등을 정할 수 있다. 정확하게 말하면 반복보다는 차원의 개수를 지정해준다. 3차원 직육면체 큐브를 몇개 만들 것인가 지정해준다고 생각하면 편하다. 위의 코드로 예시를 들면 (상상해시길) 1번 코드는 1~24를 저장하는 행렬을 배열로 만든다. 이때, 행렬의 기본 형식을 4x3이고 큐브의 층은 2층이다. 2번 코드는 1~32를 저장하는 행렬을 배열로 만든다. 행렬의 기본 형식은 2x2이고 층수는 4개이다. 이런 직육면체 큐브를 2개를 만들면 된다. apply apply x_mat &amp;lt;- matrix(rnorm(100),20,5) x_mat apply(x_mat,2,mean) apply(x_mat,1,sum) apply는 반복문의 사용을 극도로 줄여준다. 1을 사용하면 모든 행, 2를 사용하면 모든 열에 함수를 적용시킨다. " }, { "title": "R Programming 2주차", "url": "/posts/R2/", "categories": "Statistics, R programming", "tags": "R", "date": "2020-03-26 00:00:00 +0900", "snippet": "월요일type check and type casting type check and type casting real &amp;lt;- 3.5 as.integer(real) intg &amp;lt;- 3 is.integer(intg) test &amp;lt;- c(TRUE,FALSE,TRUE) is.logical(test) as.numeric(test) as를 활용하면 type casting, 즉 형변환을 할 수 있다. is를 활용하면 type check가 가능하다. 반환형은 true, false다. Vector Vector vec &amp;lt;- c(1,3,4,2,5) vec vec[7] = 5 vec R에서 c()함수를 활용해서 할당하면 기본적으로 벡터로 저장이 된다. 만약 위의 코드처럼 단일 type으로 할당하면 문제가 없다. 하지만 여러가지 type으로 벡터를 할당하면 type 우선순위에 따라 자동으로 type casting이 이뤄진다. Type casting power는 char &amp;gt; integer &amp;gt; logical 순이다. assign assign assign(&quot;x&quot;,1:10) x name=paste0(&quot;x&quot;,1:200) name for (i in 1:200) { assign(name[i],1:10) } assign함수는 말 그대로 변수에 값을 할당하는 함수다. 효율이 낮아서 자주사용하는 함수는 아니다. 흔히 쓰이는 경우는 위의 예제코드처럼 반복문을 통해 특정 규칙을 가진 변수를 할당할 때 사용한다. 근데 사실 이 마저도 paste0()함수를 활용하면 해결된다. 목요일기본적인 수학적 함수 활용 기본적인 수학적 함수 활용 x &amp;lt;- c(1,2,3, 1); y &amp;lt;- c(1,2,3,4) v &amp;lt;- 2 * x + y + 1 print(v) 3 ^ 2 %% 4 3 * 2 %% 4 log(exp(1)) range(x) sum(x) prod(x) var(x) vari &amp;lt;- sum((x-mean(x))^2)/(length(x)-1) print(vari) complex(real=-17,imaginary = 0) complex(3,1) complex(3,10,-2) 기본적으로 쓸 수 있는 수학적인 함수 method들이다. range()는 해당 벡터 데이터의 최대, 최소값을 리턴한다. 그 외에 sum();prod()는 순서대로 해당 변수의 전체 합, 전체 곱을 리턴해준다. 통계적인 데이터인 분산, 평균등을 계산 할 수도 있고 complex(real, imaginary)를 쓰면 복소수를 나타낼 수 있다. " }, { "title": "R Programming 1주차", "url": "/posts/R1/", "categories": "Statistics, R programming", "tags": "R", "date": "2020-03-19 00:00:00 +0900", "snippet": "월요일R 기본 문법 R 기본 문법 install.packages(&quot;caTools&quot;) library(caTools) search() install.packages(&quot;dplyr&quot;) library(dplyr) 기본적으로 R에서 패키지를 설치하고 라이브러리로 해당 패키지의 함수를 불러오는 코드를 배웠다. 목요일내장 함수 사용법 내장 함수 사용법 help.start() help(solve) ?solve example(solve) help.search(&quot;string&quot;) R의 내장함수들의 사용법을 알기위한 명령어들이다. R 기본 연산 R 기본 연산 x &amp;lt;- 3; y = 15 n &amp;lt;- 100 p = 50 할당 연산은 &amp;lt;- 또는 =으로 가능하다. " } ]
