<!DOCTYPE html><html lang="en" data-mode="light" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.2.1" /><meta property="og:title" content="[Paper Review] Deep Variational Autoencoder with Shallow Parallel Path for Top-N Recommendation (VASP)" /><meta property="og:locale" content="en" /><meta name="description" content="Deep Variational Autoencoder with Shallow Parallel Path for Top-N Recommendation (VASP) (2021)" /><meta property="og:description" content="Deep Variational Autoencoder with Shallow Parallel Path for Top-N Recommendation (VASP) (2021)" /><link rel="canonical" href="https://cow-coding.github.io/posts/VASP/" /><meta property="og:url" content="https://cow-coding.github.io/posts/VASP/" /><meta property="og:site_name" content="Coding Gallery" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-04-04T13:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="[Paper Review] Deep Variational Autoencoder with Shallow Parallel Path for Top-N Recommendation (VASP)" /><meta name="twitter:site" content="@twitter_username" /><meta name="google-site-verification" content="TXb5nWUV5Ag0O8EVGMTUS11ZVi7BYOensPMiRdQGNRg" /> <script type="application/ld+json"> {"description":"Deep Variational Autoencoder with Shallow Parallel Path for Top-N Recommendation (VASP) (2021)","url":"https://cow-coding.github.io/posts/VASP/","@type":"BlogPosting","headline":"[Paper Review] Deep Variational Autoencoder with Shallow Parallel Path for Top-N Recommendation (VASP)","dateModified":"2022-04-07T05:58:01+09:00","datePublished":"2022-04-04T13:00:00+09:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://cow-coding.github.io/posts/VASP/"},"@context":"https://schema.org"}</script><title>[Paper Review] Deep Variational Autoencoder with Shallow Parallel Path for Top-N Recommendation (VASP) | Coding Gallery</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Coding Gallery"><meta name="application-name" content="Coding Gallery"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4761810170892865" crossorigin="anonymous"></script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/my_img/icebear.jpeg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Coding Gallery</a></div><div class="site-subtitle font-italic">마침표를 찍고 조금 더 멀리</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/cow-coding" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/twitter_username" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['kbp0237','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>[Paper Review] Deep Variational Autoencoder with Shallow Parallel Path for Top-N Recommendation (VASP)</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>[Paper Review] Deep Variational Autoencoder with Shallow Parallel Path for Top-N Recommendation (VASP)</h1><div class="post-meta text-muted"><div> By <em> <a href="https://github.com/cow-coding">Park Kibum</a> </em></div><div class="d-flex"><div> <span> Posted <em class="timeago" date="2022-04-04 13:00:00 +0900" data-toggle="tooltip" data-placement="bottom" title="Mon, Apr 4, 2022, 1:00 PM +0900" >Apr 4, 2022</em> </span> <span> Updated <em class="timeago" date="2022-04-07 05:58:01 +0900 " data-toggle="tooltip" data-placement="bottom" title="Thu, Apr 7, 2022, 5:58 AM +0900" >Apr 7, 2022</em> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="2422 words"> <em>13 min</em> read</span></div></div></div><div class="post-content"><h1 id="deep-variational-autoencoder-with-shallow-parallel-path-for-top-n-recommendation-vasp-2021">Deep Variational Autoencoder with Shallow Parallel Path for Top-N Recommendation (VASP) (2021)</h1><hr /><h2 id="논문-소개">논문 소개 <a href="#논문-소개" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-src="/image/paper/vasp/vasp1.png" alt="" data-proofer-ignore><em>출처 : Deep Variational Autoencoder with Shallow Parallel Path for Top-N Recommendation (VASP)</em></p><center> <a href="https://arxiv.org/abs/2102.05774"><bold>Deep Variational Autoencoder with Shallow Parallel Path for Top-N Recommendation (VASP)</bold></a> </center><p><br /></p><p>VASP는 2021년 2월에 나온 따끈따끈한 논문입니다. 추천 시스템에서 Benchmark 상위를 차지하고 있는 모델이며 추천 시스템에서 크게 효과를 보인 EASE와 VAE를 활용한 모델입니다. 실제 논문 상에서 엄청 특별한 수식적 접근이나 뛰어난 연구적 접근이 있지는 않다고 생각하지만 2개의 모델을 앙상블해서 좋은 효과를 보였다고 생각됩니다.</p><hr /><h2 id="abstract">Abstract <a href="#abstract" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><ul><li>EASE 알고리즘은 Top-N 추천 task에서 좋은 성능을 보였고 VAE는 최근 추천 시스템에서 관심도가 높음<li>이 논문에서는 EASE를 현대의 신경망 기술로 학습하여 성능을 향상시킨 <strong>Neural EASE</strong>를<li>identity(여기서는 Input으로 보임)에 과적합되지 않으면서 다중 비선형 레이어의 이점을 보여주는 deep autoencoder인 <strong>FLAVE</strong>를 제시함<li>FLAVE와 Neural EASE를 병렬적으로 학습하는 방식을 활용</ul><hr /><h2 id="introduction">Introduction <a href="#introduction" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><ul><li>RS에서는 수백만의 유저와 아이템을 다루는 큰 규모의 데이터를 다루는 기술<li>이런 상황에서 content는 dynamic하게 변화하고 실시간으로 처리되는 데이터에 대해 충분히 <strong>빠르게 학습</strong>해야하고 <strong>높은 재현률</strong>을 보여줄 필요가 있음<li>EASE는 단순한 선형모델임에도 identity에 과적합하는 문제를 잘 해결하고 explainable한 결과를 제공함<li>이를 활용해서 <strong>복잡한 비선형 패턴을 처리하는 deep autoencoder</strong>의 잠재력을 활용하며 <strong>EASE처럼 설명가능한</strong> 모델을 구축할 것<li>전통적인 dropout을 통한 과적합 방지는 매우 deep한 구조에서는 효과가 좋지 않음<li><h3 id="문제제기">문제제기 <a href="#문제제기" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><li>The overfitting towards identity<ul><li>identity에 overfitting되는 문제 발생 $\rightarrow$ EASE에서 해결</ul><li>deep architecture에서는 dropout이 overfitting 해결에 큰 도움이 되지 않음<ul><li>새로운 방식의 data augmentation을 진행해야함</ul><li>추천 시스템의 근원적인 문제인 niche item의 interaction이 부족한 것을 해결할 필요가 있음</ul><h3 id="문제-해결-방향">문제 해결 방향 <a href="#문제-해결-방향" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><ul><li>Top-N 추천 : focal loss를 활용한 long-tail 문제 방지<li>과적합 방지 : 간단한 data augmentation 기술 활용<li>앙상블 : 아다마르 곱(<strong>element-wise</strong>)을 활용한 여러모델 학습<li><strong>VASP는 deep VAE와 Neural EASE를 결합하여 함께 학습함</strong><ul><li>이 과정에서 선형적 특징과 비선형적 특징을 모두 모델링함</ul></ul><hr /><h2 id="related-work">Related Work <a href="#related-work" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><ul><li>다양한 관련된 연구들에 대해 장단점을 나열하고 있음<li>핵심적인 모델들 위주로 정리</ul><h3 id="autorec">AutoRec <a href="#autorec" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><ul><li>2016년에 나온 AutoRec은 autoencoder 기반의 CF 모델을 활용하여 좋은 성능을 보였음<li>이 모델에서는 <strong>explicit rating</strong>을 활용했음</ul><h3 id="multi-vae">Multi-VAE <a href="#multi-vae" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><ul><li>2018년에 나온 Variational AutoEncoder (VAE)를 활용한 CF 모델.<li>Multi-VAE라고도 함<li><strong>Multinomial log-likelihood 데이터 분포</strong>를 활용한 모델</ul><h3 id="ease">EASE <a href="#ease" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><ul><li><em>Embarassingly Shallow Autoencoder for Sparse Data</em>에서 공개한 모델<li>deep architecture에 반대되는 hidden layer가 없는 Autoencoder 모델<li>SOTA 모델을 달성한 모델로 유명함</ul><h3 id="wide--deep">Wide &amp; Deep <a href="#wide--deep" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><ul><li>단순하고 shallow하거나 복잡한 architecture를 갖는 다양한 모델들이 나타남<li>Wide &amp; Deep이라 불리는 위의 2가지 특징을 조합한 모델이 등장<li>두 특징을 조합하는 과정에서 <strong>joint training</strong>이라 불리는 기술을 사용함<li>Wide &amp; Deep에서 사용한 아이디어를 활용해서 <strong>item attribute가 아닌 interaction을 사용</strong>해서 비선형 패턴을 찾는 방식을 설계함</ul><h3 id="other-fields">Other fields <a href="#other-fields" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><ul><li>CV나 NLP에서 사용하는 딥러닝 기술이 추천 시스템에 적용되어 좋은 성능을 보임<li>대표적인 예는 Residual Network와 RecVAE가 있음<li>이런 점에서 <strong>Focal Loss (FL)</strong> 를 추천 시스템에 적용함<ul><li>Focal Loss는 object detection에서 나타나는 불균형한 class에 적용하는 loss function<li><strong>Class imbalance는 추천 시스템의 niche item interaction</strong>과 동일하게 생각할 수 있음<li>이런 점에서 CF에서 나타는 cold start problem을 해결할 수 있을 것임</ul></ul><hr /><h2 id="model-architecture">Model Architecture <a href="#model-architecture" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-src="/image/paper/vasp/vasp4.png" alt="" width="550" data-proofer-ignore></p><ul><li>논문에서 VASP의 구조는 크게 세부분으로 나눠짐<li>EASE 모델을 사용하는 <strong>Neural EASE</strong><li>Variational AutoEncoder를 사용하는 <strong>FLVAE</strong><li>두 모델의 결과를 <strong>Hadamard product를 활용해서 합치는</strong> VASP 부분</ul><h3 id="기본-notation">기본 notation <a href="#기본-notation" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3>\[\begin{aligned} &amp; u \in \{ 1, ... , U \} : \text{user} \\ &amp; i \in \{ 1, ... , I \} : \text{item} \\ &amp; \mathbf{x}_u = [ x_{u1}, x_{u2}, ..., x_{uI} ]^\intercal \in \mathbb{N}^I : \text{user u interaction history} \\ &amp; \hat{\mathbf{x}}_u = [ x_{u1}, x_{u2}, ..., x_{uI} ]^\intercal \in \mathbb{N}^I : \text{user u predicted ratings} \end{aligned}\]<h3 id="neural-ease">Neural EASE <a href="#neural-ease" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p><img data-src="/image/paper/vasp/vasp5.png" alt="" data-proofer-ignore></p><ul><li>EASE 논문에 따르면 EASE의 수식은 다음과 같음</ul>\[\hat{\mathbf{x}_u} = W \cdot \mathbf{x}_u, \quad W \in \mathbf{R}^{|I| \times |I|}\]<ul><li>identity overfitting 방지를 위해 가중치 행렬인 <strong>$W$의 대각성분은 모두 0으로 제한함</strong><li>EASE 논문에 따르면 $\mathbf{x}_u$와 $\mathbf{x}_u$의 square loss는 closed-form을 띄므로 목적함수로 사용할 수 있음<li>FLVAE와 함께 학습을 하고 backpropagation을 진행해야 하므로 <strong>single layer perceptron</strong>으로 EASE를 구현함<li>layer의 대각 weight 성분은 반드시 0으로 제한하며 bias node는 없음</ul><p><img data-src="/image/paper/vasp/vasp6.png" alt="" data-proofer-ignore><em>Neural EASE의 loss function 성능표</em></p><ul><li>논문에 따르면 <strong>cosine proximity</strong>를 사용하는 것이 Neural EASE loss function에서 가장 좋은 것으로 확인됨<li>PyTorch로 구현하는 경우 <code class="language-plaintext highlighter-rouge">torch.nn.functional.cosine_similarity</code>를 사용하면 됨</ul><h3 id="multivae-with-focal-loss-flvae">MultiVAE with focal loss (FLVAE) <a href="#multivae-with-focal-loss-flvae" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p><img data-src="/image/paper/vasp/vasp7.png" alt="" width="500" data-proofer-ignore></p>\[\begin{matrix} \mathbf{z}_u \sim N(0, \mathbf{I}_k) \\ \pi(\mathbf{z}_u) \varpropto exp\{f_{\theta}(\mathbf{z}_u)\} \\ \mathbf{x}_u \sim Mult(N_u, \pi(\mathbf{z}_u)) \end{matrix}\]<ul><li>latent representation $\mathbf{z}_u$는 평균이 0, 표준편차가 $\mathbf{I}_k$인 가우시안 분포에서 추출<li>신경망 $f_{\theta}(\cdot)$은 아이템 $I$에 대한 확률분포 $\pi(\mathbf{z}_u)$를 유도함<li>interaction history $\mathbf{x}_u$는 유도된 $\pi(\mathbf{z}_u)$를 활용한 다항분포에서 유도</ul>\[\log p \geq \mathbb{E}_q[ \log p(\mathbf{x}_u\mid\mathbf{z}_u) - KL(q(\mathbf{z}_u|\mathbf{x}_u) \Vert p(\mathbf{z}_u)) ]\]<ul><li>VAE의 목적은 average marginal likelihood인 $p(\mathbf{z}_u \mid \mathbf{x}_u) = \int p(\mathbf{x}_u \mid \mathbf{z}_u)p(\mathbf{z}_u)dz $ 를 최대화하는 것이 목적<li>$f_\theta (\cdot)$은 신경망이므로 $p(\mathbf{z}_u \mid \mathbf{x}_u)$ 를 유도하는 것이 어렵기 때문에 위의 식인 <strong>ELBO로 근사해서 유도</strong><li>niche item problem을 해결하고자 <strong>focal loss</strong>를 활용하는데, 이는 다음과 같이 정의함</ul>\[FL(p_t) = -\alpha_t(1-p_t)^\gamma \log(p_t), \quad p_t = \begin{cases} \hat{x}_{ui} &amp; if \quad x_{ui} = 1 \\ 1 - \hat{x}_{ui} &amp; otherwise \end{cases}\]<ul><li>이때, focal loss에 사용되는 $\alpha$와 $\gamma$는 hyperparameter<li>이에 따라 ELBO를 다음과 같이 재정의할 수 있음</ul>\[\log p \geq \mathbb{E}_q[ \alpha_t(1 - p(\mathbf{x}_u \mid z_u))^{\gamma}\log p(\mathbf{x}_u\mid\mathbf{z}_u) - KL(q(\mathbf{z}_u|\mathbf{x}_u) \Vert p(\mathbf{z}_u)) ]\]<h3 id="vasp">VASP <a href="#vasp" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p><img data-src="/image/paper/vasp/vasp8.png" alt="" data-proofer-ignore></p><ul><li>model $m$에 대해서 $m(\cdot) : \mathbf{x}_u \rightarrow \hat{\mathbf{x}}_u$ 로 정의<li>이때 model $m$의 output에는 sigmoid function 처리를 진행해서 $\hat{x}_{uI} \in &lt; 0, 1 &gt; $로 처리<li><strong>$\odot$으로 표시하는 Hadamard product를 활용한 joint learning을 제시함</strong></ul>\[m_n(\mathbf{x}_u) = \bigodot^{n}_{j=1}m_j = m_1(\mathbf{x}_u) \odot m_2(\mathbf{x}_u) \odot \cdots \odot m_n(\mathbf{x}_u)\] \[m_n(\mathbf{x}_u) = \hat{x}_{nu} = \bigodot_{j=1}^{n} \hat{x}_{ju} , \quad \hat{x}_{nu} \in \langle 0, 1 \rangle\]<ul><li>기존의 wide &amp; deep에서 사용한 joint training은 모델의 결과를 합하는 logical OR을 활용하는 반면 VASP에서는 <strong>Hadamard product를 사용한 logical AND</strong> 방식을 사용해서 모델을 결합함<ul><li>Hadamard product를 사용하면 2개의 모델이 모두 동의(1에 근접)해야 값에 의미가 존재함</ul><li>최종적으로 <strong>NEASE와 FLVAE를 element-wise 곱으로 나타냄</strong></ul>\[\begin{matrix} m_{VASP}(\mathbf{x}_u) &amp; = &amp; m_{FLVAE}(\mathbf{x}_u) \odot m_{EASE}(\mathbf{x}_u) \\ &amp; = &amp; \hat{\mathbf{x}}_{FLVAE_u} \odot \hat{\mathbf{x}}_{EASE_u} \end{matrix}\] \[\hat{\mathbf{x}}_{EASE_u} = \sigma(W \cdot \mathbf{x}_u)\]<h3 id="data-augmentation">Data augmentation <a href="#data-augmentation" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p><img data-src="/image/paper/vasp/vasp2.png" alt="" data-proofer-ignore><em>Split-Brain Autoencoders: Unsupervised Learning by Cross-Channel Prediction</em></p><ul><li>논문 참조 : <a href="https://arxiv.org/pdf/1611.09842.pdf">Split-Brain Autoencoders: Unsupervised Learning by Cross-Channel Prediction</a><li>Autoencoder에서 핵심은 입출력의 과적합을 방지하는 것<li><em>Split-Brain Autoencoders</em>에 따르면 gray-scale representation을 학습하는 영역과 color-channel을 학습하는 영역으로 분할되어 훈련을 진행함<li>두 영역은 각각 교차로 예측을 수행하여 원본 사진으로 복구를 함<li>아래는 논문에서 제시한 예시 사진</ul><p><img data-src="/image/paper/vasp/vasp3.png" alt="" data-proofer-ignore><em>Split-Brain Autoencoders: Unsupervised Learning by Cross-Channel Prediction</em></p><ul><li>VASP에서는 전처리 단계로 자동화된 data augmentation을 사용하는 1개의 신경망을 활용할 것<li>전체 데이터를 절반으로 나눠서 각자 학습하는 형태로 훈련</ul><p><img data-src="/image/paper/vasp/vasp9.png" alt="" data-proofer-ignore></p><ul><li>앞서 설명한 Split-Brain Autoencoders: Unsupervised Learning by Cross-Channel Prediction의 아이디어를 사용해서 data augmentation을 진행<li>모든 훈련 epoch마다 input $x_u$를 무작위로 $x_{Au}$와 $x_{Bu}$ 2개의 부분으로 나눔</ul>\[\begin{aligned} x_{Aui} = \begin{cases} 0 &amp; if \quad x_{ui} = 0 \\ 1 - x_{Bui} &amp; otherwise \end{cases} \\ x_{Bui} = \begin{cases} 0 &amp; if \quad x_{ui} = 0 \\ 1 - x_{Aui} &amp; otherwise \end{cases} \end{aligned}\]<ul><li>학습과정에서 $x_{Aui}$와 $x_{Bui}$는 서로 교차로 입출력으로 학습을 진행함</ul><hr /><h2 id="experimental-step">Experimental step <a href="#experimental-step" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><ul><li>실제 구현때 참고할 실험 환경<li>데이터셋은 MovieLens 20M과 Netflix Prize를 활용함<li>전체 데이터의 80%를 학습에 활용하고 20%를 prediction으로 사용<li>metric은 NDCG@k와 Recall@k로 진행</ul><h3 id="모델-구현">모델 구현 <a href="#모델-구현" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><ul><li>Neural EASE<ul><li>kernel constraint를 활용한 대각행렬을 0으로 제한하는 dense layer를 활용<li>실험과정에서는 loss function을 3개를 활용함 <a href="#neural-ease">Neural EASE</a> 참고<li>data augmentation이 사용되지 않음</ul><li>Variational AE<ul><li>FLVAE는 encoder와 decoder에 모두 dense하게 연결된 residual network를 활용<li>output에는 sigmoid function 사용<li>Data augmentation 사용<li>focal loss hyperparameter $\alpha_t$ = 0.25, $\gamma$ = 2.0을 사용함</ul><li>VASP<ul><li>EASE와 FLVAE를 hadamard product를 활용해서 결합<li>hyperparameter는 FLVAE와 동일</ul></ul><h3 id="hyperparameters">Hyperparameters <a href="#hyperparameters" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><ul><li>latent space : 2048<li>hidden layer : 4096<li>encoder<ul><li>7 residual hidden layer</ul><li>decoder<ul><li>5 residual hidden layer</ul><li>epoch : 50<li>초기 세팅<ul><li>learning rate : 0.00005<li>bath size : 1024</ul><li>이후 learning rate 0.00001로 낮추고 20 epochs 추가 학습<li>이후 learning rate 0.000001로 낮추고 20 epochs 추가 학습</ul></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/paper-review/'>Paper Review</a>, <a href='/categories/recommender-system/'>Recommender System</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/naver/" class="post-tag no-text-decoration" >NAVER</a> <a href="/tags/boostcamp/" class="post-tag no-text-decoration" >BoostCamp</a> <a href="/tags/ai-tech/" class="post-tag no-text-decoration" >AI Tech</a> <a href="/tags/vae/" class="post-tag no-text-decoration" >VAE</a> <a href="/tags/vasp/" class="post-tag no-text-decoration" >VASP</a> <a href="/tags/ease/" class="post-tag no-text-decoration" >EASE</a> <a href="/tags/paper-review/" class="post-tag no-text-decoration" >paper review</a> <a href="/tags/recommender-system/" class="post-tag no-text-decoration" >recommender system</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=[Paper Review] Deep Variational Autoencoder with Shallow Parallel Path for Top-N Recommendation (VASP) - Coding Gallery&url=https://cow-coding.github.io/posts/VASP/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=[Paper Review] Deep Variational Autoencoder with Shallow Parallel Path for Top-N Recommendation (VASP) - Coding Gallery&u=https://cow-coding.github.io/posts/VASP/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=[Paper Review] Deep Variational Autoencoder with Shallow Parallel Path for Top-N Recommendation (VASP) - Coding Gallery&url=https://cow-coding.github.io/posts/VASP/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/coursera2_3/">[MLOps Specialization / Step2] Labeling Data</a><li><a href="/posts/module/">[BoostCamp AI Tech / 심화포스팅] torch.nn.Module 뜯어먹기</a><li><a href="/posts/list/">[Deep Dive Python] 2. List</a><li><a href="/posts/variable/">[Deep Dive Python] 1. Python의 객체와 변수 개념</a><li><a href="/posts/final7/">[BoostCamp AI Tech / Final] Day91 - Airflow setting 및 배치 파이프라인 설계</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/ai-tech/">AI Tech</a> <a class="post-tag" href="/tags/boostcamp/">BoostCamp</a> <a class="post-tag" href="/tags/naver/">NAVER</a> <a class="post-tag" href="/tags/deep-learning/">Deep Learning</a> <a class="post-tag" href="/tags/mlops/">MLOps</a> <a class="post-tag" href="/tags/data-engineering/">Data Engineering</a> <a class="post-tag" href="/tags/project/">Project</a> <a class="post-tag" href="/tags/basic/">Basic</a> <a class="post-tag" href="/tags/python/">Python</a> <a class="post-tag" href="/tags/recommender-system/">Recommender System</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/ngcf/"><div class="card-body"> <em class="timeago small" date="2022-03-14 13:00:00 +0900" >Mar 14, 2022</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[Paper Review] Neural Graph Collaborative Filtering (2019)</h3><div class="text-muted small"><p> Neural Graph Collaborative Filtering (2019) 논문 소개 출처 : ACM2019 Neural Graph Collaborative Filtering [ACM 2019] Neural Graph Collaborative Filtering NGCF는 GNN의 최초 등장 논문은 아닙니다. 이전에 이미 GNN에 대...</p></div></div></a></div><div class="card"> <a href="/posts/alexnet/"><div class="card-body"> <em class="timeago small" date="2022-02-09 00:00:00 +0900" >Feb 9, 2022</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[Paper Review] AlexNet (2012)</h3><div class="text-muted small"><p> AlexNet (2012) GitHub : AlexNet Implementation 논문 소개 출처 : NIPS2012 ImageNet Classification with Deep Convolutional Neural Networks (a.k.a AlexNet) [NIPS2012] ImageNet Classification with Dee...</p></div></div></a></div><div class="card"> <a href="/posts/day12/"><div class="card-body"> <em class="timeago small" date="2022-02-04 23:00:00 +0900" >Feb 4, 2022</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[BoostCamp AI Tech] Day12</h3><div class="text-muted small"><p> Day12 Review 당신은 오늘 하루 어떻게 살았나요? 시각화 강의 Basic Seaborn Basic, Advanced 심화 포스팅 : jekyll blog &amp;amp; MathJax 오늘 하지 못한 것들 없당~ 내일은 어떤 것을 할까? Coursera MLOps 강의 자금 지원 확인 AlexNet P...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/day50_pstage2/" class="btn btn-outline-primary" prompt="Older"><p>[BoostCamp AI Tech / P Stage 2] Day50 - Project Day 10</p></a> <a href="/posts/day51_pstage2/" class="btn btn-outline-primary" prompt="Newer"><p>[BoostCamp AI Tech / P Stage 2] Day51 - Project Day 11</p></a></div><div id="disqus_thread" class="pt-2 pb-2"><p class="text-center text-muted small"> Comments powered by <a href="https://disqus.com/">Disqus</a>.</p></div><script type="text/javascript"> var disqus_config = function () { this.page.url = 'https://cow-coding.github.io/posts/VASP/'; this.page.identifier = '/posts/VASP/'; }; /* Lazy loading */ var disqus_observer = new IntersectionObserver(function (entries) { if(entries[0].isIntersecting) { (function () { var d = document, s = d.createElement('script'); s.src = 'https://cow-coding.disqus.com/embed.js'; s.setAttribute('data-timestamp', +new Date()); (d.head || d.body).appendChild(s); })(); disqus_observer.disconnect(); } }, { threshold: [0] }); disqus_observer.observe(document.querySelector('#disqus_thread')); /* Auto switch theme */ function reloadDisqus() { /* Disqus hasn't been loaded */ if (typeof DISQUS === "undefined") { return; } if (document.readyState == 'complete') { DISQUS.reset({ reload: true, config: disqus_config }); } } const modeToggle = document.querySelector(".mode-toggle"); if (typeof modeToggle !== "undefined") { /* modeToggle.addEventListener('click', reloadDisqus); // not pretty for 'color-scheme' */ window.matchMedia('(prefers-color-scheme: dark)').addEventListener('change', reloadDisqus); } </script></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center text-muted"><div class="footer-left"><p class="mb-0"> © 2023 <a href="https://github.com/cow-coding">Park Kibum</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/ai-tech/">AI Tech</a> <a class="post-tag" href="/tags/boostcamp/">BoostCamp</a> <a class="post-tag" href="/tags/naver/">NAVER</a> <a class="post-tag" href="/tags/deep-learning/">Deep Learning</a> <a class="post-tag" href="/tags/mlops/">MLOps</a> <a class="post-tag" href="/tags/data-engineering/">Data Engineering</a> <a class="post-tag" href="/tags/project/">Project</a> <a class="post-tag" href="/tags/basic/">Basic</a> <a class="post-tag" href="/tags/python/">Python</a> <a class="post-tag" href="/tags/recommender-system/">Recommender System</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { loader: {load: ['[tex]/color']}, chtml: { scale: 1.2 }, tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ], packages: {'[+]':['color']} } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=UA-163727422-1"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-163727422-1'); }); </script>
