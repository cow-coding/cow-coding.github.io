<!DOCTYPE html><html lang="en" data-mode="light" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.2.1" /><meta property="og:title" content="[BoostCamp AI Tech / Level 1 - DL Basic] Day13 - Optimization" /><meta property="og:locale" content="en" /><meta name="description" content="DL Basic : Optimization" /><meta property="og:description" content="DL Basic : Optimization" /><link rel="canonical" href="https://cow-coding.github.io/posts/day13_3_optimization/" /><meta property="og:url" content="https://cow-coding.github.io/posts/day13_3_optimization/" /><meta property="og:site_name" content="Coding Gallery" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-02-07T13:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="[BoostCamp AI Tech / Level 1 - DL Basic] Day13 - Optimization" /><meta name="twitter:site" content="@twitter_username" /><meta name="google-site-verification" content="TXb5nWUV5Ag0O8EVGMTUS11ZVi7BYOensPMiRdQGNRg" /> <script type="application/ld+json"> {"description":"DL Basic : Optimization","url":"https://cow-coding.github.io/posts/day13_3_optimization/","@type":"BlogPosting","headline":"[BoostCamp AI Tech / Level 1 - DL Basic] Day13 - Optimization","dateModified":"2022-05-16T14:24:47+09:00","datePublished":"2022-02-07T13:00:00+09:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://cow-coding.github.io/posts/day13_3_optimization/"},"@context":"https://schema.org"}</script><title>[BoostCamp AI Tech / Level 1 - DL Basic] Day13 - Optimization | Coding Gallery</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Coding Gallery"><meta name="application-name" content="Coding Gallery"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4761810170892865" crossorigin="anonymous"></script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/my_img/icebear.jpeg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Coding Gallery</a></div><div class="site-subtitle font-italic">마침표를 찍고 조금 더 멀리</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/cow-coding" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/twitter_username" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['kbp0237','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>[BoostCamp AI Tech / Level 1 - DL Basic] Day13 - Optimization</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>[BoostCamp AI Tech / Level 1 - DL Basic] Day13 - Optimization</h1><div class="post-meta text-muted"><div> By <em> <a href="https://github.com/cow-coding">Park Kibum</a> </em></div><div class="d-flex"><div> <span> Posted <em class="timeago" date="2022-02-07 13:00:00 +0900" data-toggle="tooltip" data-placement="bottom" title="Mon, Feb 7, 2022, 1:00 PM +0900" >Feb 7, 2022</em> </span> <span> Updated <em class="timeago" date="2022-05-16 14:24:47 +0900 " data-toggle="tooltip" data-placement="bottom" title="Mon, May 16, 2022, 2:24 PM +0900" >May 16, 2022</em> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="1703 words"> <em>9 min</em> read</span></div></div></div><div class="post-content"><h1 id="dl-basic--optimization">DL Basic : Optimization</h1><hr /><h2 id="introduction">Introduction <a href="#introduction" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><ul><li>Model 학습의 기본적인 원리는 <strong>Gradient Descent</strong><li>Gradient Descent는 1차 미분의 결과의 최적화 알고리즘<ul><li>1차 미분 결과의 최적화는 local minimum을 찾는 것</ul></ul><hr /><h2 id="important-concepts-in-optimization">Important Concepts in Optimization <a href="#important-concepts-in-optimization" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><h3 id="generalization">Generalization <a href="#generalization" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p><img data-src="/image/boostcamp/dlbasic/optim/gen1.png" alt="" width="500" data-proofer-ignore></p><ul><li><strong>Generalization</strong>은 모델의 성능으로 사용함<li>좋은 generalization performance는 <strong>Network의 성능이 train과 비슷한 것</strong>을 의미<ul><li>하지만 기본적인 train set 성능이 낮으면 generalization performance가 좋아도 모델 성능 자체가 좋지 않을 수 있음</ul><li><strong>Overfitting(과적합)</strong> 과 <strong>Underfitting</strong>을 방지하는 것이 generalization의 가장 핵심적인 포인트</ul><h3 id="cross-validation">Cross-validation <a href="#cross-validation" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><ul><li>K-Fold validation이라고도 함<li>학습 data가 적은 경우의 문제를 해결하고자 등장<li>전체 train data를 n등분하여 k번째 등분을 validaation data로 설정하고 n-k개의 데이터를 train으로 설정하여 학습을 하는 과정<li>n개의 valid set으로 검증하는 과정과 같음<li>일반적으로 cross-validation을 통해 최적 hyper parameter를 탐색하고 최종적으로 전체 데이터로 모델을 학습함</ul><h3 id="bias--variance">Bias / Variance <a href="#bias--variance" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><ul><li>Variance : 유사 input에 대해 prediction이 얼마나 비슷한가?<li>Bias : 전반적인 prediction이 target과 얼마나 떨어져있는가?</ul>\[\begin{aligned} \mathbb{E}\left[ (t-\hat{f})^2 \right] &amp; = \mathbb{E}\left[ (t-f + f-\hat{f})^2 \right] \\ &amp; = \mathbb{E}\left[ (f - \mathbb{E}[\hat{f}]^2)^2 \right] + \mathbb{E}\left[ (\mathbb{E}[\hat{f}] - \hat{f})^2 \right] + \mathbb{E}[ \epsilon ] \end{aligned}\]<ul><li>위 공식은 cost 에 대한 bias와 variance, noise의 관계성을 나타낸 것<li>공식에 따라 bias, variance, noise는 tradeoff 관계를 가짐</ul><h3 id="booststraping">Booststraping <a href="#booststraping" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><ul><li>데이터 셋에서 <strong>subsampling을 통해 여러 데이터 셋을 형성하고 그에 맞는 여러 모델들을 만드는 기법</strong><li>형성된 model들의 예측값들을 보고 <strong>예측의 consensus(일관성)</strong>로 전반적인 uncertainty를 판단<li>보통 앙상블(Ensemble)기법에 많이 활용됨</ul><h3 id="bagging-vs-boosting">Bagging VS Boosting <a href="#bagging-vs-boosting" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><ul><li><strong>Bagging (Bootstrapping aggregating)</strong><ul><li>여러 모델들의 학습결과를 합쳐서 판단하는 방법<li>가장 좋은 결과를 선택하는 <strong>voting</strong>이나 평균치를 연산하는 <strong>averaging</strong>을 활용해서 정답을 도출</ul><li><strong>Boosting</strong><ul><li>모델을 여러개 만드는 것은 동일하지만 연속적으로 연결시켜 <strong>이전에 좋지 않은 결과를 보이는 case를 집중적으로 학습</strong>함<li>여러개의 weak learner를 합쳐서 1개의 strong model을 형성</ul></ul><hr /><h2 id="gradient-descent-methods">Gradient Descent Methods <a href="#gradient-descent-methods" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><ul><li><strong>Stochastic Gradient Descent</strong> : 1번에 1개의 데이터를 활용해서 gradient를 update<li><strong>Mini-batch gradient Descent</strong> : batch-szie sample을 활용해서 gradient를 update<li><strong>Batch Gradient Descent</strong> : 전체 데이터를 활용해서 gradient를 update<li>보통 min-batch 방식이 효과가 좋은 것으로 알려져있음</ul><h3 id="batch-size">Batch-Size <a href="#batch-size" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p><img data-src="/image/boostcamp/dlbasic/optim/batchsize.png" alt="" width="550" data-proofer-ignore></p><ul><li>batch-size를 적절히 잘 조절하는게 학습과정의 핵심<li>이때 보통 작은 batch-size가 더 효과가 좋은데, 이는 small-batch method는 <strong>flat minimizer</strong>이기 때문임<ul><li>그림을 보면, flat minimum에서는 prediction이 target에서 벗어나도 실제 값이 큰 차이를 보이지 않음<li>하지만 sharp minumum은 prediction이 약간만 벗어나도 매우 큰 오차를 갖게되는 것을 볼 수 있음</ul></ul><h3 id="gradient-descent--momentum">Gradient Descent &amp; Momentum <a href="#gradient-descent--momentum" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3>\[\begin{aligned} W_{t+1} \leftarrow W_{t} - \eta g_{t} \end{aligned}\]<ul><li>$\eta$ : learning rate<li>$g_{t}$ : gradient<li>일반적으로 알고있는 gradient descent</ul>\[\begin{aligned} &amp; a_{t+1} \leftarrow \beta a_{t} + g_{t} \\ &amp; W_{t+1} \leftarrow W_{t} - \eta a_{t+1} \end{aligned}\]<ul><li>$a_{t+1}$ : accumulation<li>$\beta$ : momentum<li>momentum의 역할은 진행한 gradient의 방향을 유지해주는 역할<li>수식을 보면 이전의 학습결과에 momentum을 처리해서 반영<li>학습을 하다보면 batch에 따라 oscillation의 형태로 gradient가 움직이는 경우가 있는데, 이때 momentum이 적용되면 이를 어느정도 보완해줄 수 있음</ul><h3 id="nestrov-accelerated-gradient-nag">Nestrov Accelerated Gradient (NAG) <a href="#nestrov-accelerated-gradient-nag" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3>\[\begin{aligned} &amp; a_{t+1} \leftarrow \beta a_{t} + \nabla\mathcal{L}(W_{t} - \eta\beta a_{t}) \\ &amp; W_{t+1} \leftarrow W_{t} - \eta a_{t+1} \end{aligned}\]<ul><li>$\nabla\mathcal{L}(W_{t} - \eta\beta a_{t})$ : Lookahead gradient<li>NAG는 momentum의 최적값을 지나치는 문제를 방지하는 목적을 갖고 있음<ul><li>lookahead gradient는 momentum에의해 이동하는 곳에서 gradient를 계산하여 적용한 것</ul></ul><h3 id="adagrad">Adagrad <a href="#adagrad" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3>\[\begin{aligned} W_{t+1} = W_t - \frac{\eta}{\sqrt{G_{t} + \epsilon}} g_{t} \end{aligned}\]<ul><li>$G_{t}$ : Sum of gradient squares<li>$\epsilon$ : for numerical stability<li><strong>Adagrad</strong>의 핵심 아이디어는 learning rate를 건드린다는 것<ul><li>$G_{t}$에 이전 gradient들의 제곱이 누적되기 때문에 많이 변화한 parameter의 학습률을 낮추고 적게 변화한 parameter의 학습률을 높이는 방식을 선택</ul><li>$\epsilon$은 0으로 나누는 것을 방지하는 상수<li>Adagrad는 <strong>$G_{t}$가 너무 커지면 더 이상 학습이 일어나지 않는 문제가 발생</strong></ul><h3 id="adadelta">Adadelta <a href="#adadelta" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3>\[\begin{aligned} G_t = \gamma G_{t-1} + (1-\gamma)g_{t}^2 \\ W_{t+1} = W_{t} - \frac{\sqrt{H_{t-1} + \epsilon}}{\sqrt{G_{t}+\epsilon}}g_t \\ H_t = \gamma H_{t-1} + (1-\gamma)(\Delta W_t)^2 \end{aligned}\]<ul><li>$G_t$ : EMA of gradient squares<li>$H_t$ : EMA of differentce squares<li>Adadelta에서 $G_t$와 $H_t$는 $\gamma$가 recursive한 값으로 영향을 주므로 time의 영향을 받게 적용함 (Exponential Moving Average)</ul><h3 id="rmsprop">RMSprop <a href="#rmsprop" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3>\[\begin{aligned} G_t = \gamma G_{t-1} + (1-\gamma)g_{t}^2 \\ W_{t+1} = W_{t} - \frac{\eta}{\sqrt{G_{t}+\epsilon}}g_t \end{aligned}\]<ul><li>RMSprop은 공식적으로 출판한 내용은 없고 그냥 이런식으로 하니 잘 되었다~ 라는 식으로 등장<li>이전에 Adagrad는 stepsize가 없었는데, 이를 수정하여 stepsize를 적용함</ul><h3 id="adam">Adam <a href="#adam" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p>\(m_t = \beta_1 m_{t=1} + (1-\beta_1)g_t \\ v_t = \beta_@ v_{t-1} + (1-\beta_2)g_t^2 \\ W_{t+1} = W_t - \frac{\eta}{\sqrt{v_t + \epsilon}}\frac{\sqrt{1-\beta_2^t}}{1-\beta_1^t}m_t\)</p><ul><li>$m_t$ : momentum<li>$v_t$ : EMA of gradient squares<li>$\frac{\sqrt{1-\beta_2^t}}{1-\beta_1^t}$ : unbiased estimator<li>Adam은 momentum에 RMSprop 개념이 합쳐진 방식<li>단, 연산에서 지수평균을 사용</ul><hr /><h2 id="regularization">Regularization <a href="#regularization" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><h3 id="early-stopping">Early Stopping <a href="#early-stopping" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><ul><li>test set을 활용해 학습을 하는 것은 cheating이므로 valid set의 결과를 활용해 특정 metric 조건에 맞춰 학습을 <strong>조기멈춤</strong>하는 것</ul><h3 id="parameter-norm-penalty">Parameter Norm Penalty <a href="#parameter-norm-penalty" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3>\[\text{total cost} = \text{loss}(\mathcal{D}; W) + \frac{\alpha}{2}\lVert W \rVert_2^2\]<ul><li><strong>weight decay</strong>라고도 함<li>parameter 폭발을 방지하는 것이 목적임</ul><h3 id="data-augmentation">Data Augmentation <a href="#data-augmentation" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><ul><li>주어진 데이터의 방향 혹은 거울상과 같은 다양한 변형을 취하는 것을 의미<li>더 많은 데이터셋을 위해 자주 사용<li>이미지 분류 문제에서 많이 사용</ul><h3 id="noise-robustness">Noise Robustness <a href="#noise-robustness" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><ul><li>input이나 weight에 노이즈를 추가하는 것</ul><h3 id="label-smoothing">Label smoothing <a href="#label-smoothing" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><ul><li><strong>Mix-up</strong>, <strong>CutMix</strong>같은 방식을 통새 label의 decision boundary 처리를 도와주는 것<li>이미지 처리를 할 때 두 개의 label을 섞은 이미지를 사용하는 Mixup이나 일부를 잘라서 하비는 cutmix 데이터를 활용하면 더 좋은 학습 성능을 보이기도 함</ul><h3 id="dropout">Dropout <a href="#dropout" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><ul><li>forward 과정에서 일정 확률로 랜덤하게 neuron의 연결 가중치를 0으로 설정하는 것<li>1개의 모델 구조에서 뉴런 구성을 변경하기 때문에 마치 여러개의 모델을 학습하는 것과 같은 효과를 가짐</ul><h3 id="batch-normalization">Batch Normalization <a href="#batch-normalization" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3>\[\begin{aligned} &amp; \mu_{B} = \frac{1}{m}\sum_{i=1}^m x_i \\ &amp; \sigma^2_{B} = \frac{1}{m}\sum_{i=1}^m (x_i - \mu_{B})^2 \\ &amp; \hat{x}_i = \frac{x_i - \mu_{B}}{\sqrt{\sigma^2_{B}+\epsilon}} \end{aligned}\]<ul><li>Batch 정규화는 Gradient Vanishing/Exploding 문제를 해결하고자 등장<li>각 배치별로 데이터를 정규화하여 0 ~ 1의 수치로 조정</ul></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/naver-boostcamp-ai-tech/'>NAVER BoostCamp AI Tech</a>, <a href='/categories/level-1-dl-basic/'>Level 1 - DL Basic</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/deep-learning/" class="post-tag no-text-decoration" >Deep Learning</a> <a href="/tags/naver/" class="post-tag no-text-decoration" >NAVER</a> <a href="/tags/boostcamp/" class="post-tag no-text-decoration" >BoostCamp</a> <a href="/tags/ai-tech/" class="post-tag no-text-decoration" >AI Tech</a> <a href="/tags/dl-basic/" class="post-tag no-text-decoration" >DL Basic</a> <a href="/tags/dl/" class="post-tag no-text-decoration" >DL</a> <a href="/tags/ml/" class="post-tag no-text-decoration" >ML</a> <a href="/tags/optimization/" class="post-tag no-text-decoration" >Optimization</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=[BoostCamp AI Tech / Level 1 - DL Basic] Day13 - Optimization - Coding Gallery&url=https://cow-coding.github.io/posts/day13_3_optimization/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=[BoostCamp AI Tech / Level 1 - DL Basic] Day13 - Optimization - Coding Gallery&u=https://cow-coding.github.io/posts/day13_3_optimization/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=[BoostCamp AI Tech / Level 1 - DL Basic] Day13 - Optimization - Coding Gallery&url=https://cow-coding.github.io/posts/day13_3_optimization/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/coursera2_3/">[MLOps Specialization / Step2] Labeling Data</a><li><a href="/posts/module/">[BoostCamp AI Tech / 심화포스팅] torch.nn.Module 뜯어먹기</a><li><a href="/posts/list/">[Deep Dive Python] 2. List</a><li><a href="/posts/variable/">[Deep Dive Python] 1. Python의 객체와 변수 개념</a><li><a href="/posts/final7/">[BoostCamp AI Tech / Final] Day91 - Airflow setting 및 배치 파이프라인 설계</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/ai-tech/">AI Tech</a> <a class="post-tag" href="/tags/boostcamp/">BoostCamp</a> <a class="post-tag" href="/tags/naver/">NAVER</a> <a class="post-tag" href="/tags/deep-learning/">Deep Learning</a> <a class="post-tag" href="/tags/mlops/">MLOps</a> <a class="post-tag" href="/tags/data-engineering/">Data Engineering</a> <a class="post-tag" href="/tags/project/">Project</a> <a class="post-tag" href="/tags/basic/">Basic</a> <a class="post-tag" href="/tags/python/">Python</a> <a class="post-tag" href="/tags/recommender-system/">Recommender System</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/day13_1_history/"><div class="card-body"> <em class="timeago small" date="2022-02-07 10:00:00 +0900" >Feb 7, 2022</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[BoostCamp AI Tech / Level 1 - DL Basic] Day13 - Historical Review</h3><div class="text-muted small"><p> DL Basic : Introcution &amp;amp; Historical Review 딥러닝의 기본요소 Data 데이터는 “어떤 문제를 해결할 것인가?”에 따라 형식이 달라짐 Model 모델의 성질에 따라 동일한 데이터라도 다른 성능을 보인다. 심지어 같은 task를 해결하기...</p></div></div></a></div><div class="card"> <a href="/posts/day13_2_mlp/"><div class="card-body"> <em class="timeago small" date="2022-02-07 13:00:00 +0900" >Feb 7, 2022</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[BoostCamp AI Tech / Level 1 - DL Basic] Day13 - MLP (Multi-Layer Perceptron)</h3><div class="text-muted small"><p> DL Basic : MLP (Multi-Layer Perceptron) Neural Networks 일반적으로 Neural Netwrok라고 하면 인간의 뇌 구조를 모방해서 만든 시스템이라고 생각함 하지만 우리의 비행기가 새를 모방했다고 하지만 새와는 다른 것처럼 Neural Net이 정확하게 인간의 뇌를 구현한 것이라 보긴 어려움 ...</p></div></div></a></div><div class="card"> <a href="/posts/day14_1_cnn/"><div class="card-body"> <em class="timeago small" date="2022-02-08 13:00:00 +0900" >Feb 8, 2022</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[BoostCamp AI Tech / Level 1 - DL Basic] Day14 - Convolutional Neural Networks</h3><div class="text-muted small"><p> DL Basic : Convolutional Neural Networks Convolution convolution 연산은 기본적으로 Input에 대해 커널단위로 연산을 하는 것을 의미 특징 추출을 목적으로 할 때 많이 사용함 필터(kernel)의 종류에 따라 다른 결과가 나타남 ex) convolution ...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/day13_2_mlp/" class="btn btn-outline-primary" prompt="Older"><p>[BoostCamp AI Tech / Level 1 - DL Basic] Day13 - MLP (Multi-Layer Perceptron)</p></a> <a href="/posts/day13/" class="btn btn-outline-primary" prompt="Newer"><p>[BoostCamp AI Tech] Day13</p></a></div><div id="disqus_thread" class="pt-2 pb-2"><p class="text-center text-muted small"> Comments powered by <a href="https://disqus.com/">Disqus</a>.</p></div><script type="text/javascript"> var disqus_config = function () { this.page.url = 'https://cow-coding.github.io/posts/day13_3_optimization/'; this.page.identifier = '/posts/day13_3_optimization/'; }; /* Lazy loading */ var disqus_observer = new IntersectionObserver(function (entries) { if(entries[0].isIntersecting) { (function () { var d = document, s = d.createElement('script'); s.src = 'https://cow-coding.disqus.com/embed.js'; s.setAttribute('data-timestamp', +new Date()); (d.head || d.body).appendChild(s); })(); disqus_observer.disconnect(); } }, { threshold: [0] }); disqus_observer.observe(document.querySelector('#disqus_thread')); /* Auto switch theme */ function reloadDisqus() { /* Disqus hasn't been loaded */ if (typeof DISQUS === "undefined") { return; } if (document.readyState == 'complete') { DISQUS.reset({ reload: true, config: disqus_config }); } } const modeToggle = document.querySelector(".mode-toggle"); if (typeof modeToggle !== "undefined") { /* modeToggle.addEventListener('click', reloadDisqus); // not pretty for 'color-scheme' */ window.matchMedia('(prefers-color-scheme: dark)').addEventListener('change', reloadDisqus); } </script></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center text-muted"><div class="footer-left"><p class="mb-0"> © 2023 <a href="https://github.com/cow-coding">Park Kibum</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/ai-tech/">AI Tech</a> <a class="post-tag" href="/tags/boostcamp/">BoostCamp</a> <a class="post-tag" href="/tags/naver/">NAVER</a> <a class="post-tag" href="/tags/deep-learning/">Deep Learning</a> <a class="post-tag" href="/tags/mlops/">MLOps</a> <a class="post-tag" href="/tags/data-engineering/">Data Engineering</a> <a class="post-tag" href="/tags/project/">Project</a> <a class="post-tag" href="/tags/basic/">Basic</a> <a class="post-tag" href="/tags/python/">Python</a> <a class="post-tag" href="/tags/recommender-system/">Recommender System</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { loader: {load: ['[tex]/color']}, chtml: { scale: 1.2 }, tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ], packages: {'[+]':['color']} } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=UA-163727422-1"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-163727422-1'); }); </script>
